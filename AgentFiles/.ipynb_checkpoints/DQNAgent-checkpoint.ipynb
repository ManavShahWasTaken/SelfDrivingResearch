{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z6tk-uX9bwY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import socket\n",
    "# import shutil\n",
    "# from skimage import io\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state_data):\n",
    "        self.state_data = np.asarray(state_data)\n",
    "  \n",
    "    def process_state(self):\n",
    "        pass\n",
    "  \n",
    "    def get_batch_tensor(self):\n",
    "        holder = np.asarray(self.state_data)\n",
    "        holder.reshape((1, ) + holder.shape)\n",
    "        return holder\n",
    "  \n",
    "    def get_individual_tensor(self):\n",
    "        return np.asarray(self.state_data)\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.state_data.shape\n",
    "  \n",
    "    def display(self):\n",
    "        print(self.state_data)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class Frame(State):\n",
    "    def __init__(self, state_data, crop_factor=None, destination_size=None, vert_cent=0.5):\n",
    "        State.__init__(self, state_data)\n",
    "#         self.state_data = self.process_state(crop_factor, vert_cent, destination_shape)\n",
    "        self.state_data = self.process_state(crop_factor, vert_cent, (32,32))\n",
    "  \n",
    "    def process_state(self, crop_factor, vert_cent, destination_shape):\n",
    "        \"\"\"\n",
    "        Does all the processing of the frame using the helper functions\n",
    "        \"\"\"\n",
    "        frame = self.crop_frame(self.state_data, crop_factor, vert_cent)\n",
    "        frame = self.normalise_frame(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        assert len(frame.shape) == 2\n",
    "        frame = self.downsample_frame(frame, destination_shape)\n",
    "        return frame\n",
    "\n",
    "  \n",
    "    def gray_scale(self, frame, gray_scale_factor=[0.3, 0.3, 0.3]):\n",
    "        frame = np.dot(frame, np.asarray(gray_scale_factor))\n",
    "        return frame\n",
    "\n",
    "    def normalise_frame(self, frame):\n",
    "        frame = frame.astype('float32') / 255.0\n",
    "        return frame\n",
    "  \n",
    "    def downsample_frame(self, frame, destination_shape):\n",
    "        \"\"\"\n",
    "        downsamples the frame. decreases the resolution\n",
    "        \"\"\"\n",
    "        frame = cv2.resize(np.asarray(frame), dsize=destination_shape, interpolation=cv2.INTER_CUBIC)\n",
    "        return frame\n",
    "  \n",
    "    def crop_frame(self, frame, crop_factor, vert_cent=0.5):\n",
    "        \"\"\"\n",
    "        input is the frame\n",
    "        output is the cropped frame\n",
    "        crop_factor is the ratio at which you want to crop the height and width\n",
    "        cent is the ratio at which the centre of the cropped frame should be\n",
    "        \"\"\"\n",
    "        if crop_factor is None:\n",
    "          return frame\n",
    "    \n",
    "        height_factor = int((crop_factor[0]*frame.shape[0]) // 2)\n",
    "        width_factor = int((crop_factor[1]*frame.shape[1]) // 2)\n",
    "        vert_cent = int(frame.shape[0]*vert_cent)\n",
    "        width_cent = int(frame.shape[1]*0.5)\n",
    "\n",
    "        frame = frame[vert_cent - height_factor: vert_cent + height_factor, \n",
    "                      width_cent - width_factor: width_cent + width_factor]\n",
    "        return frame\n",
    "\n",
    "# ------------------------------------\n",
    "class DataBuffer:\n",
    "    \"\"\"\n",
    "    Keeps track of n latest states \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, size=1):\n",
    "        self.buffer = []\n",
    "        self.size = size\n",
    "\n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        arr = np.array(self.buffer)\n",
    "        if self.size == 1 or in_batch:\n",
    "            return arr\n",
    "        else:\n",
    "            return arr.reshape((1, ) + arr.shape)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return np.asarray(self.buffer).shape\n",
    "\n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(state)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class FrameBuffer(DataBuffer):\n",
    "    def __init__(self, size = 4):\n",
    "        DataBuffer.__init__(self, size=size)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return (32, 32, 4)\n",
    "  \n",
    "    def get_input_tesnor(self, in_batch=True):\n",
    "        temp = np.array(self.buffer)\n",
    "        temp.transpose((1, 2, 0))\n",
    "        return temp\n",
    "    \n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        # if buffer not initialised\n",
    "        if len(self.buffer) == 0:\n",
    "            self.buffer = [state]\n",
    "            return\n",
    "\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.buffer.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff = FrameBuffer(3)\n",
    "arr = np.array([[255, 255, 255], [255, 255, 255], [255, 255, 255]])\n",
    "buff.assign_to_buffer(arr)\n",
    "arr2 = np.array([[255, 255, 255], [255, 255, 255], [255, 255, 255]])\n",
    "buff.assign_to_buffer(arr2)\n",
    "buff.get_input_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh-452ClVFpT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # initialise comms with the simulator here\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # initialise the socket\n",
    "        # connect to localhost, port 2345 \n",
    "        self.sock.bind((\"127.0.0.1\", 4444))\n",
    "        self.sock.listen(1)\n",
    "        print(\"Waiting to connect to Simulator...\")\n",
    "        self.clientsocket, _ = self.sock.accept() # connect to simulator [BLOCKING]\n",
    "        print(\"Connected to simulator!\")\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # initialising frame buffer\n",
    "        self.buffer_size = 4. # could change this \n",
    "        \n",
    "         # this is the FrameBuffer that keeps track of the latest frames.\n",
    "         #initialising the frame buffer by just giving it multiple copies of the same starting frame\n",
    "        # =========================================\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        \n",
    "        self.time_steps = 0\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        self.max_time_steps_per_episode = 500 #change this based on the enviorment\n",
    "        \n",
    "        self.reset()\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # Create target directory if it doesn't exist\n",
    "       \n",
    "        parent_path = os.path.abspath(os.path.join(\"\", os.pardir))\n",
    "        final_path = parent_path + \"\\simulation\\Screenshots\"\n",
    "        if not os.path.exists(final_path):\n",
    "            os.mkdir(final_path)\n",
    "            print(\"Directory \" , final_path,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , final_path,  \" already exists\")\n",
    "        \n",
    "        self.action_space = ['as', 'ar', 'al', 'ds', 'dr', 'dl', 'bs', 'br', 'bl']\n",
    "        \n",
    "    def get_input_shape(self):\n",
    "        \"\"\"\n",
    "        returns the input shape for the input layer of the network\n",
    "        \"\"\"\n",
    "        return self.buffer.get_input_shape()\n",
    "    \n",
    "    def get_state_shape(self):\n",
    "        \"\"\"\n",
    "        not to be confused with the input shape. This is the shape of individual state (the shape of an individual processed shape of the environment)\n",
    "        \"\"\"\n",
    "        return self.current_state.get_shape()\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        returns a random action to be taken. [steering, acceleration, brake]\n",
    "        \"\"\"\n",
    "        return random.choice(self.action_space())\n",
    "    \n",
    "    def get_action_at_index(self, index):\n",
    "        return self.action_space[index]\n",
    "    \n",
    "    def get_num_action_space(self):\n",
    "        \"\"\"\n",
    "        returns the number of permuations of valide actions. For Eg. [steer left, accelerate and no brake] is ONE action\n",
    "        [steer right, accelerate and brake] is invalid as we cannot accelerate and brake at the same time.\n",
    "        there are 9 possible actions I think?\n",
    "        \"\"\"\n",
    "        return len(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the environment. self.done denotes whether the episode is done i.e. the car has crashed or we have stopped it\n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "        self.time_steps = 0\n",
    "        \n",
    "        tup = self.step('reset') # initial step. Says don't do anything but send the first 4 frames and game info\n",
    "        self.current_state, _, self.done = tup\n",
    "        \n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        does the action and returns the reward for that action along with the next state\n",
    "\n",
    "        This function may get complicated as it has to interact with the car simulator throught sockets.\n",
    "        \"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if not self.is_done():\n",
    "            # if the episode has not ended\n",
    "            #=======================\n",
    "            # in this section of the code, we wait for the simulator for to give us an action request\n",
    "            # then we give it an action through the TCP/IP protocol\n",
    "            msg = self.receive_message()\n",
    "            print(\"1:{0}\".format(msg))\n",
    "            if msg == 'requesting action':\n",
    "                # send the action\n",
    "                self.send_message(action)\n",
    "                print(\"2: sent action\")\n",
    "                # now wait for the action to be completed\n",
    "                action_done = self.receive_message()\n",
    "                print(\"3:{0}\".format(action_done))\n",
    "                # the 4 images will be stored as scr{i}.png\n",
    "                \n",
    "                buff = FrameBuffer(size = self.buffer_size)\n",
    "                \n",
    "                for i in range(1, 4):\n",
    "                    # each Frame object is then assigned to the FrameBuffer class in chronological order\n",
    "                    path = Path(os.getcwd()).parent / 'scr{0}.png'.format(i)\n",
    "                    buff.assign_to_buffer(self.get_frame(path))\n",
    "                \n",
    "                print(\"4: requesting info\")\n",
    "                angle, distance, speed, self.done = self.get_game_stats()\n",
    "                print(\"5: info:{0}, {1}, {2}, {3}\".format(angle, distance, speed, self.done))\n",
    "\n",
    "                dist_delta = self.prev_dist - distance\n",
    "                \n",
    "                self.prev_dist = distance\n",
    "\n",
    "                if abs(dist_delta) > 30:\n",
    "                    dist_delta = 5 # if there's too big a negative jump in the distance, the car has passed a checkpoint.\n",
    "                    # so, don't penalise it for that.\n",
    "\n",
    "                # calculate reward based on the game stats \n",
    "                reward = (dist_delta * 0.7) + (speed * 0.3) - (abs(angle) * 0.1)\n",
    "                \n",
    "                buffer_tensor = buff.get_input_tensor()\n",
    "\n",
    "            # this buffer tensor is what we will input to the NN in the next time step\n",
    "            # we record this as the next observation.\n",
    "            \n",
    "            # A buffer is a collection of consecutive frames that we feed to the NN. These frames are already processed.\n",
    "            \n",
    "            self.current_state = (buffer_tensor, angle, distance, speed)\n",
    "\n",
    "            # this returns the state of the environment after the action has been completed, the reward for the action and if the episode ended.\n",
    "            return self.current_state , reward, self.done\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def send_message(self, string):\n",
    "        try:\n",
    "            self.clientsocket.sendall(string.encode())\n",
    "        except:\n",
    "            print(\"Socket Exception while sending data to simulator\")\n",
    "    \n",
    "    def receive_message(self):\n",
    "        data = None\n",
    "        try:\n",
    "            data  = self.clientsocket.recv(256).decode()\n",
    "        except:\n",
    "            print(\"Socket Exception while recieving data from simulator\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        returns if the episode is finished\n",
    "        \"\"\"\n",
    "        return self.done\n",
    "        \n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        in case we need to 'close' the environment\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def get_frame(self, path: str) -> Frame:\n",
    "        \"\"\"\n",
    "        communicates with the sim to get the latest state/frame. \n",
    "        returns a Frame object\n",
    "        Get image from path then convert to np array then make a frame object\n",
    "        \"\"\"\n",
    "        image = Image.open(path, 'r')\n",
    "        image.load()\n",
    "        np_data = np.asarray(image, dtype=\"float32\" )\n",
    "        return Frame(np_data)\n",
    "    \n",
    "    \n",
    "    # def delete_screenshots(self, folder_path: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This method deletes the four screenshots saved in folder_path, along with the entire folder.\n",
    "    #     Method must be called after all four screenshots are converted to Frame objects.\n",
    "    #     \"\"\"\n",
    "    #     shutil.rmtree(folder_path)\n",
    "    \n",
    "    \n",
    "    def get_current_state():\n",
    "        \"\"\"\n",
    "        get the last n frames from the simulator (they might be stored in a folder by the simulator)\n",
    "        and store them in a buffer and return them\n",
    "        \"\"\"\n",
    "        return self.current_buff\n",
    "\n",
    "    \n",
    "    def get_game_stats(self):\n",
    "        \"\"\"\n",
    "        returns a tuple of angle, distance from checkpoint and speed from the sim. Again requires comms with simulator.\n",
    "        \"\"\"\n",
    "        self.send_message(\"requesting info\")\n",
    "        string = self.receive_message()\n",
    "        self.send_message(\"got it\")\n",
    "        \n",
    "        value_list = string.split(\", \")\n",
    "        angle = float(value_list[0])\n",
    "        distance = float(value_list[1])\n",
    "        speed = float(value_list[2])\n",
    "\n",
    "        crashed_state = False\n",
    "        if value_list[3] == '1':\n",
    "            crashed_state = True\n",
    "        return (angle, distance, speed, crashed_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u36eHVMV285q"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment, network, run_only=False, eps_decay_rate=0.9975,max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "        self.env = environment # this should be the environment wrapper class\n",
    "        \n",
    "        if not run_only:\n",
    "            self.exp_rate = max_exp_rate     # our network starts off with this exploration rate\n",
    "        else:\n",
    "            self.exp_rate = 0.0\n",
    "        \n",
    "        self.min_exp_rate = min_exp_rate  # have at least 0.01 exploration rate at all times\n",
    "        \n",
    "        self.decay_rate = eps_decay_rate   # decay the exploration rate at this rate\n",
    "        \n",
    "        self.time_step = 0     # keeps track of time steps\n",
    "        \n",
    "        self.network = network\n",
    "    \n",
    "    def take_action(self, current_state):\n",
    "        # Implement the epsilon greedy strategy \n",
    "        result = random.random()                      # get a random number from 0 to 1 with linear distribution\n",
    "        if result > self.get_exp_rate():              # if it falls over the explore rate, exploit\n",
    "            action = self.network.get_max_q_value_index(current_state)  # exploit\n",
    "            \n",
    "        else:                                         # if it falls under the explore rate, explore\n",
    "            action = self.env.get_random_action()          # explore (generate a random action from the environment class)\n",
    "            \n",
    "        self.increment_time_step()                    # increment time step as well as update the decay rate\n",
    "        next_state, reward, done = self.env.step(action)                     # finally, take the action and record the reward\n",
    "        return current_state, action, reward, next_state, done  # return an experience Tuple\n",
    "        \n",
    "    \n",
    "    def reset_time_steps(self, i=0):\n",
    "        self.timesteps = i\n",
    "    \n",
    "    def increment_time_step(self):\n",
    "        self.time_step += 1\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.exp_rate > self.min_exp_rate:\n",
    "            self.exp_rate = self.exp_rate * self.decay_rate\n",
    "        else:\n",
    "            self.exp_rate = self.min_exp_rate\n",
    "    \n",
    "    def get_exp_rate(self):\n",
    "        return self.exp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MVpqLFJ83IXp",
    "outputId": "27566a6e-9187-4053-ce13-02fa2abb6cbe"
   },
   "outputs": [],
   "source": [
    "network_name = 'DrivePolicy.h5'\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "class NetworkTracker:\n",
    "    \n",
    "    def __init__(self, environment, source=False): # pass in the environment which has input shape of the frame\n",
    "        if source:\n",
    "            self.model = models.load_model(network_name)\n",
    "        else:\n",
    "            self.model = self.define_model(environment)\n",
    "            self.model.save(network_name)\n",
    "        self.target_model = self.model\n",
    "              \n",
    "    def define_model(self, env):\n",
    "        model = models.Sequential() \n",
    "        model.add(layers.Conv2D(filters=10, kernel_size=(3,3), activation = 'relu', input_shape=env.get_input_shape()))\n",
    "        \n",
    "        model.add(layers.MaxPool2D((3, 3)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=20, kernal_size = (2, 2), activation='relu'))\n",
    "\n",
    "        model.add(layers.MaxPool2D(3, 3))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='softmax'))\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        \n",
    "        model.add(layers.Dense(env.get_num_action_space(), activation='linear'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                           loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def get_q_values_for_one(self, state):\n",
    "        \n",
    "        output_tensor = self.model.predict(state.reshape((1, ) + state.shape )) # the State class handles turning the state \n",
    "                                                                    # into an appropriate input tensor for a NN\n",
    "                                                                    # so you don't have to change it everywhere\n",
    "        return output_tensor[0]  # you want to convert the 2 dimensional output to 1 dimension to call argmax\n",
    "    \n",
    "    def get_max_q_value_index(self, state):\n",
    "        return np.argmax(self.get_q_values_for_one(state))\n",
    "    \n",
    "    def get_q_values_for_batch(self, states):\n",
    "        if states[0] is State:\n",
    "            states = np.asarray(states)\n",
    "        \n",
    "        f = self.model.predict(states)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def get_target_tensor(self, next_states):\n",
    "        if next_states[0] is State:\n",
    "            next_states = np.asarray([i.to_input_tensor(individual=False) for i in next_states])\n",
    "        output_tensor = self.target_model.predict(next_states)\n",
    "        \n",
    "        return output_tensor\n",
    "                  \n",
    "    def fit(self, states_batch, targets_batch):\n",
    "        self.model.fit(states_batch, targets_batch, verbose=1)\n",
    "        \n",
    "    def clone_policy(self):\n",
    "        self.model.save(network_name)\n",
    "        self.target_model = models.load_model(network_name)\n",
    "                  \n",
    "    def get_model_summary(self):\n",
    "        return self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.replay = []\n",
    "        self.limit = size\n",
    "        self.exp_count = 0\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.exp_count += 1\n",
    "        \n",
    "        if self.exp_count < self.limit:\n",
    "            self.replay.append(experience)  #append to experiences\n",
    "        else:\n",
    "            self.replay[self.exp_count%len(self.replay)] = experience  #wrap around if the memory capacity is reached\n",
    "        assert len(self.replay) <= self.limit\n",
    "        \n",
    "    def is_usable(self, batch_size):\n",
    "        return len(self.replay) >= batch_size\n",
    "    \n",
    "    def reset_replay_memory(self):\n",
    "        self.exp_count = 0\n",
    "        self.replay = []\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.replay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(sample):\n",
    "    states = np.asarray([i[0] for i in sample])\n",
    "    actions = np.asarray([i[1] for i in sample])\n",
    "    rewards = np.asarray([i[2] for i in sample])\n",
    "    next_states = np.asarray([i[3] for i in sample])\n",
    "    done_tensor = np.asarray([i[4] for i in sample])\n",
    "    return states, actions, rewards, next_states, done_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(states, actions, rewards, next_states, dones, net, gamma):\n",
    "    assert actions.ndim == 1\n",
    "    assert rewards.ndim == 1\n",
    "    assert dones.ndim == 1\n",
    "    assert len(actions) == len(rewards) == len(dones) == len(states) == len(next_states)\n",
    "    target_q_values = net.get_q_values_for_batch(states)\n",
    "    targets = rewards + gamma * (np.max(net.get_target_tensor(next_states), axis=1))\n",
    "    for i in range(len(targets)):\n",
    "        if dones[i]:\n",
    "            targets[i] = rewards[i]\n",
    "    for index in range(len(target_q_values)):\n",
    "        target_q_values[index][actions[index]] = targets[index]\n",
    "\n",
    "    return target_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_agent(render=False, verbose=False, num_episodes=1500,\n",
    "                discount = 0.95, batch_size = 64, N = 40, memory_size = 1024, \n",
    "                eps_decay_rate=0.9975, max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "    # get all the hyperparameters in one place!\n",
    "    \n",
    "    # stores all the total reward per episode\n",
    "    training_stats = []    \n",
    "    \n",
    "    # initialise your environment\n",
    "    env = EnvironmentWrapper()\n",
    "    \n",
    "    # initialise your policy and target networks\n",
    "    net = NetworkTracker(env)\n",
    "    \n",
    "    # initialise your agent that will follow the epsilon greedy strategy\n",
    "    agent = Agent(env, net, eps_decay_rate=eps_decay_rate, max_exp_rate=max_exp_rate,min_exp_rate=min_exp_rate )    \n",
    "    \n",
    "    # initialise experience replay memory\n",
    "    memory = Memory(memory_size)\n",
    "    \n",
    "    for episode_count in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        # uncomment if you want to start the environmet with a random move\n",
    "        # state = env.step(env.get_random_action())[0]\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        # check if the environment is available to run\n",
    "        while not env.is_done(): \n",
    "            \n",
    "            current_state, action, reward, next_state, done = agent.take_action(state)\n",
    "            Cumulative_reward += reward\n",
    "            experience = current_state, action, reward, next_state, done\n",
    "            state = next_state\n",
    "            memory.push(experience)\n",
    "            if not explored:\n",
    "                exploit_times += 1\n",
    "\n",
    "        agent.update_epsilon()\n",
    "        if memory.is_usable(batch_size):\n",
    "\n",
    "                experience_batch = memory.sample(batch_size)\n",
    "                states, actions, rewards, next_states, done_tensor = extract_tensors(experience_batch)\n",
    "\n",
    "                target_batch = get_target_batch(states, actions, rewards, next_states, done_tensor, net, discount)\n",
    "\n",
    "\n",
    "                net.fit(states, target_batch)\n",
    "\n",
    "        if (episode_count + 1) % N == 0:\n",
    "            net.clone_policy()\n",
    "\n",
    "        training_stats.append(cumulative_reward)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Episode Count: \", episode_count, \"\\t Cumulative Reward: \", Cumulative_reward, \"\\t eps: \", exploit_times/Cumulative_reward )\n",
    "\n",
    "\n",
    "    epochs = list(range(len(training_stats)))\n",
    "\n",
    "    plt.clf\n",
    "    plt.plot(epochs, training_stats, 'b', label='f')\n",
    "    env.close()\n",
    "    \n",
    "    return epochs, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
