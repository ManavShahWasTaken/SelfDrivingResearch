{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z6tk-uX9bwY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import socket\n",
    "# import shutil\n",
    "# from skimage import io\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state_data):\n",
    "        self.state_data = np.asarray(state_data)\n",
    "  \n",
    "    def process_state(self):\n",
    "        pass\n",
    "  \n",
    "    def get_batch_tensor(self):\n",
    "        holder = np.asarray(self.state_data)\n",
    "        holder.reshape((1, ) + holder.shape)\n",
    "        return holder\n",
    "  \n",
    "    def get_individual_tensor(self):\n",
    "        return np.asarray(self.state_data)\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.state_data.shape\n",
    "  \n",
    "    def display(self):\n",
    "        print(self.state_data)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class Frame(State):\n",
    "    def __init__(self, state_data, crop_factor=None, destination_size=None, vert_cent=0.5):\n",
    "        State.__init__(self, state_data)\n",
    "#         self.state_data = self.process_state(crop_factor, vert_cent, destination_shape)\n",
    "        self.state_data = self.process_state([0.7, 1.0], 0.7, (128,64))\n",
    "  \n",
    "    def process_state(self, crop_factor, vert_cent, destination_shape):\n",
    "        \"\"\"\n",
    "        Does all the processing of the frame using the helper functions\n",
    "        \"\"\"\n",
    "        frame = self.crop_frame(self.state_data, crop_factor, vert_cent)\n",
    "        frame = self.normalise_frame(frame)\n",
    "        frame = self.gray_scale(frame) # cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        assert len(frame.shape) == 2\n",
    "        frame = self.downsample_frame(frame, destination_shape)\n",
    "        return frame\n",
    "\n",
    "  \n",
    "    def gray_scale(self, frame, gray_scale_factor=[0.3, 0.3, 0.3]):\n",
    "        frame = np.dot(frame, np.asarray(gray_scale_factor))\n",
    "        return frame\n",
    "\n",
    "    def normalise_frame(self, frame):\n",
    "        frame = frame.astype('float32') / 255.0\n",
    "        return frame\n",
    "  \n",
    "    def downsample_frame(self, frame, destination_shape):\n",
    "        \"\"\"\n",
    "        downsamples the frame. decreases the resolution\n",
    "        \"\"\"\n",
    "        frame = cv2.resize(np.asarray(frame), dsize=destination_shape, interpolation=cv2.INTER_CUBIC)\n",
    "        return frame\n",
    "  \n",
    "    def crop_frame(self, frame, crop_factor, vert_cent):\n",
    "        \"\"\"\n",
    "        input is the frame\n",
    "        output is the cropped frame\n",
    "        crop_factor is the ratio at which you want to crop the height and width(0.8, 0.8)\n",
    "        cent is the ratio at which the centre of the cropped frame should be\n",
    "        \"\"\"\n",
    "        if crop_factor is None:\n",
    "          return frame\n",
    "    \n",
    "        height_factor = int((crop_factor[0]*frame.shape[0]) // 2)\n",
    "        width_factor = int((crop_factor[1]*frame.shape[1]) // 2)\n",
    "        vert_cent = int(frame.shape[0]*vert_cent)\n",
    "        width_cent = int(frame.shape[1]*0.5)\n",
    "\n",
    "        frame = frame[vert_cent - height_factor: vert_cent + height_factor, \n",
    "                      width_cent - width_factor: width_cent + width_factor]\n",
    "        return frame\n",
    "\n",
    "# ------------------------------------\n",
    "class DataBuffer:\n",
    "    \"\"\"\n",
    "    Keeps track of n latest states \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, size=1):\n",
    "        self.buffer = []\n",
    "        self.size = size\n",
    "\n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        arr = np.array(self.buffer)\n",
    "        if self.size == 1 or in_batch:\n",
    "            return arr\n",
    "        else:\n",
    "            return arr.reshape((1, ) + arr.shape)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return np.asarray(self.current_state[0]).shape\n",
    "\n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(state)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class FrameBuffer(DataBuffer):\n",
    "    def __init__(self, size = 4):\n",
    "        DataBuffer.__init__(self, size=size)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.get_input_tensor().shape\n",
    "  \n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        temp = np.array(self.buffer)\n",
    "        return  temp.transpose((1, 2, 0))\n",
    "    \n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        # if buffer not initialised\n",
    "        if len(self.buffer) == 0:\n",
    "            self.buffer = [state]\n",
    "            return\n",
    "\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.buffer.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh-452ClVFpT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # initialise comms with the simulator here\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # initialise the socket\n",
    "        # connect to localhost, port 2345 \n",
    "        self.sock.bind((\"127.0.0.1\", 4444))\n",
    "        self.sock.listen(1)\n",
    "        print(\"Waiting to connect to Simulator...\")\n",
    "        self.clientsocket, _ = self.sock.accept() # connect to simulator [BLOCKING]\n",
    "        print(\"Connected to simulator!\")\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # initialising frame buffer\n",
    "        self.buffer_size = 4 # could change this \n",
    "        \n",
    "         # this is the FrameBuffer that keeps track of the latest frames.\n",
    "         #initialising the frame buffer by just giving it multiple copies of the same starting frame\n",
    "        # =========================================\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        self.current_buffer = None\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        \n",
    "        self.time_steps = 0\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        self.max_time_steps_per_episode = 500 #change this based on the enviorment\n",
    "        \n",
    "        # initialise buffer\n",
    "        self.current_buffer = FrameBuffer(size = self.buffer_size)\n",
    "        \n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # Create target directory if it doesn't exist\n",
    "       \n",
    "        parent_path = os.path.abspath(os.path.join(\"\", os.pardir))\n",
    "        self.final_path = parent_path + \"/simulation/Screenshots\"\n",
    "        if not os.path.exists(self.final_path):\n",
    "            os.mkdir(self.final_path)\n",
    "            print(\"Directory \" , self.final_path,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , self.final_path,  \" already exists\")\n",
    "            \n",
    "        \n",
    "        # Save scr1 to Screenshots if it doesn't already exist\n",
    "        if not os.path.exists(self.final_path + '/scr1.png'):\n",
    "            scr1 = Image.open(self.parent_path + '/simulation/scr1.png', 'r')\n",
    "            scr1.save(self.final_path + \"/scr1.png\", \"PNG\")\n",
    "        \n",
    "        # reset actually initializes self.current_state, self.current_buffer etc.\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.action_space = ['as', 'ar', 'al', 'ds', 'dr', 'dl', 'bs', 'br', 'bl']\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        \"\"\"\n",
    "        returns the input shape for the input layer of the network\n",
    "        \"\"\"\n",
    "        return self.current_buffer.get_input_shape()\n",
    "    \n",
    "#     def get_state_shape(self):\n",
    "#         \"\"\"\n",
    "#         not to be confused with the input shape. This is the shape of individual state (the shape of an individual processed shape of the environment)\n",
    "#         \"\"\"\n",
    "#         return self.current_state\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        returns a random action to be taken. [steering, acceleration, brake]\n",
    "        \"\"\"\n",
    "        return random.choice(self.action_space)\n",
    "    \n",
    "    def get_action_at_index(self, index):\n",
    "        return self.action_space[index]\n",
    "    \n",
    "    def get_num_action_space(self):\n",
    "        \"\"\"\n",
    "        returns the number of permuations of valide actions. For Eg. [steer left, accelerate and no brake] is ONE action\n",
    "        [steer right, accelerate and brake] is invalid as we cannot accelerate and brake at the same time.\n",
    "        there are 9 possible actions I think?\n",
    "        \"\"\"\n",
    "        return len(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the environment. self.done denotes whether the episode is done i.e. the car has crashed or we have stopped it\n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "        self.time_steps = 0\n",
    "        \n",
    "        tup = self.step('reset') # initial step. Says don't do anything but send the first 4 frames and game info\n",
    "        self.current_state, _, self.done = tup\n",
    "        return self.current_state[0] # send only the frames\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        does the action and returns the reward for that action along with the next state\n",
    "\n",
    "        This function may get complicated as it has to interact with the car simulator throught sockets.\n",
    "        \"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if not self.is_done():\n",
    "            # if the episode has not ended\n",
    "            #=======================\n",
    "            \n",
    "            # send the action\n",
    "            self.send_message(action)\n",
    "            \n",
    "            # wait for results from that action\n",
    "            angle, distance, speed, self.done, frames_captured = self.get_game_stats() # blocking line\n",
    "            # print(\"5: info:{0}, {1}, {2}, {3}, {4}\".format(angle, distance, speed, self.done, frames_captured))\n",
    "            \n",
    "            \n",
    "            # add images from path to current_buffer\n",
    "            for i in range(1, frames_captured + 1):\n",
    "                # each Frame object is then assigned to the FrameBuffer class in chronological order\n",
    "                path = self.final_path + '/scr{0}.png'.format(i)\n",
    "                self.current_buffer.assign_to_buffer(self.get_frame(path))\n",
    "            \n",
    "            buffer_tensor = self.current_buffer.get_input_tensor()\n",
    "            # ========================================\n",
    "            \n",
    "            # calculate reward\n",
    "            dist_delta = self.prev_dist - distance\n",
    "            self.prev_dist = distance\n",
    "            if abs(dist_delta) > 10:\n",
    "                dist_delta = 5 # if there's too big a negative jump in the distance, the car has passed a checkpoint.\n",
    "                # so, don't penalise it for that.\n",
    " \n",
    "            reward = (dist_delta * 0.9) - (abs(angle) * 0.1)\n",
    "            #=================\n",
    "            \n",
    "            # A buffer is a collection of consecutive frames that we feed to the NN. These frames are already processed.\n",
    "            \n",
    "            # the current state consists of all the information from the environment\n",
    "            self.current_state = (buffer_tensor, angle, distance, speed)\n",
    "            \n",
    "            # this returns the state of the environment after the action has been completed, the reward for the action and if the episode ended.\n",
    "            return self.current_state , reward, self.done\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def send_message(self, string):\n",
    "        self.clientsocket.sendall(string.encode())\n",
    "    \n",
    "    def receive_message(self):\n",
    "        data  = self.clientsocket.recv(256).decode()\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        returns if the episode is finished\n",
    "        \"\"\"\n",
    "        return self.done\n",
    "        \n",
    "    \n",
    "    def get_frame(self, path: str) -> Frame:\n",
    "        \"\"\"\n",
    "        communicates with the sim to get the latest state/frame. \n",
    "        returns a Frame object\n",
    "        Get image from path then convert to np array then make a frame object\n",
    "        \"\"\"\n",
    "        image = Image.open(path, 'r')\n",
    "        image.load()\n",
    "        np_data = np.asarray(image, dtype=\"float32\" )\n",
    "        return Frame(np_data)\n",
    "    \n",
    "    \n",
    "    # def delete_screenshots(self, folder_path: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This method deletes the four screenshots saved in folder_path, along with the entire folder.\n",
    "    #     Method must be called after all four screenshots are converted to Frame objects.\n",
    "    #     \"\"\"\n",
    "    #     shutil.rmtree(folder_path)\n",
    "    \n",
    "    \n",
    "    def get_current_state():\n",
    "        \"\"\"\n",
    "        get the last n frames from the simulator (they might be stored in a folder by the simulator)\n",
    "        and store them in a buffer and return them\n",
    "        \"\"\"\n",
    "        return self.current_buff\n",
    "\n",
    "    \n",
    "    def get_game_stats(self):\n",
    "        \"\"\"\n",
    "        returns a tuple of angle, distance from checkpoint and speed from the sim. Again requires comms with simulator.\n",
    "        \"\"\"\n",
    "        # wait for info to arrive\n",
    "        string = self.receive_message()\n",
    "        \n",
    "        # process string\n",
    "        value_list = string.split(\", \")\n",
    "        angle = float(value_list[0])\n",
    "        distance = float(value_list[1])\n",
    "        speed = float(value_list[2])\n",
    "        crashed_state = False\n",
    "        if value_list[3] == '1':\n",
    "            crashed_state = True\n",
    "        frames_captured = int(value_list[4])\n",
    "        # return tuple of values\n",
    "        return angle, distance, speed, crashed_state, frames_captured\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        in case we need to 'close' the environment\n",
    "        \"\"\"\n",
    "        self.sock.close()\n",
    "        self.clientsocket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u36eHVMV285q"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment, network, run_only=False, eps_decay_rate=0.9975,max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "        self.env = environment # this should be the environment wrapper class\n",
    "        \n",
    "        if not run_only:\n",
    "            self.exp_rate = max_exp_rate     # our network starts off with this exploration rate\n",
    "        else:\n",
    "            self.exp_rate = 0.0\n",
    "        \n",
    "        self.min_exp_rate = min_exp_rate  # have at least 0.01 exploration rate at all times\n",
    "        \n",
    "        self.decay_rate = eps_decay_rate   # decay the exploration rate at this rate\n",
    "        \n",
    "        self.time_step = 0     # keeps track of time steps\n",
    "        \n",
    "        self.network = network\n",
    "    \n",
    "    def take_action(self, current_state):\n",
    "        # Implement the epsilon greedy strategy \n",
    "        result = random.random()                      # get a random number from 0 to 1 with linear distribution\n",
    "        if result > self.get_exp_rate():              # if it falls over the explore rate, exploit\n",
    "            # Get the action with the maximum q-value\n",
    "            action = self.env.get_action_at_index(self.network.get_max_q_value_index(current_state))  # exploit\n",
    "        else:                                         # if it falls under the explore rate, explore\n",
    "            action = self.env.get_random_action()          # explore (generate a random action from the environment class)\n",
    "            \n",
    "        self.increment_time_step()                    # increment time step as well as update the decay rate\n",
    "        next_state, reward, done = self.env.step(action)# finally, take the action and record the reward\n",
    "        \n",
    "        return current_state, self.env.action_space.index(action), reward, next_state[0], done  # return an experience Tuple\n",
    "        \n",
    "    \n",
    "    def reset_time_steps(self, i=0):\n",
    "        self.timesteps = i\n",
    "    \n",
    "    def increment_time_step(self):\n",
    "        self.time_step += 1\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.exp_rate > self.min_exp_rate:\n",
    "            self.exp_rate = self.exp_rate * self.decay_rate\n",
    "        else:\n",
    "            self.exp_rate = self.min_exp_rate\n",
    "    \n",
    "    def get_exp_rate(self):\n",
    "        return self.exp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MVpqLFJ83IXp",
    "outputId": "27566a6e-9187-4053-ce13-02fa2abb6cbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "network_name = 'DrivePolicy.h5'\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "class NetworkTracker:\n",
    "    \n",
    "    def __init__(self, environment, source=True): # pass in the environment which has input shape of the frame\n",
    "        if source:\n",
    "            self.model = models.load_model(network_name)\n",
    "        else:\n",
    "            self.model = self.define_model(environment)\n",
    "        \n",
    "        self.target_model = None\n",
    "        self.clone_policy()\n",
    "              \n",
    "    def define_model(self, env):\n",
    "        model = models.Sequential() \n",
    "        model.add(layers.Conv2D(filters=10, \n",
    "                                kernel_size=(3,3), \n",
    "                                activation='relu', \n",
    "                                input_shape=env.get_input_shape())) # first layer takes input shape from the environment\n",
    "        \n",
    "        model.add(layers.MaxPool2D((3, 3)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=20, kernel_size = (3, 3), strides=2, activation='relu'))\n",
    "\n",
    "        model.add(layers.MaxPool2D(3, 3))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='sigmoid'))\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        \n",
    "        model.add(layers.Dense(env.get_num_action_space(), activation='linear'))\n",
    "        \n",
    "        model.compile(optimizer=Adam(lr=0.0001),\n",
    "                           loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def get_q_values_for_one(self, state):\n",
    "        \n",
    "        output_tensor = self.model.predict(state.reshape((1, ) + state.shape )) # the State class handles turning the state \n",
    "                                                                    # into an appropriate input tensor for a NN\n",
    "                                                                    # so you don't have to change it everywhere\n",
    "        return output_tensor[0]  # you want to convert the 2 dimensional output to 1 dimension to call argmax\n",
    "    \n",
    "    def get_max_q_value_index(self, state):\n",
    "        return np.argmax(self.get_q_values_for_one(state))\n",
    "    \n",
    "    def get_q_values_for_batch(self, states):\n",
    "        if states[0] is State:\n",
    "            states = np.asarray(states)\n",
    "        \n",
    "        f = self.model.predict(states)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def get_target_tensor(self, next_states):\n",
    "        \"\"\"\n",
    "        Return the output from the next states in the experience tuple.\n",
    "        \"\"\"\n",
    "        if isinstance(next_states[0], DataBuffer): # if you have a list of buffers, convert them to numpy tensors\n",
    "            next_states = np.asarray([i.get_input_tensor(in_batch=True) for i in next_states])\n",
    "        \n",
    "        output_tensor = self.target_model.predict(next_states)\n",
    "        \n",
    "        return output_tensor\n",
    "                  \n",
    "    def fit(self, states_batch, targets_batch):\n",
    "        \"\"\"\n",
    "        Fit the states with the target batch\n",
    "        \"\"\"\n",
    "        self.model.fit(states_batch, targets_batch, verbose=1)\n",
    "        \n",
    "    def clone_policy(self):\n",
    "        \"\"\"\n",
    "        Clone the target policy\n",
    "        \"\"\"\n",
    "        self.model.save(network_name)\n",
    "        self.target_model = models.load_model(network_name)\n",
    "                  \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        Return a summary of the defined model\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def save_policy(self):\n",
    "        self.model.save(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.replay = []\n",
    "        self.limit = size\n",
    "        self.exp_count = 0\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.exp_count += 1\n",
    "        \n",
    "        if self.exp_count < self.limit:\n",
    "            self.replay.append(experience)  #append to experiences\n",
    "        else:\n",
    "            self.replay[self.exp_count%len(self.replay)] = experience  #wrap around if the memory capacity is reached\n",
    "        assert len(self.replay) <= self.limit\n",
    "        \n",
    "    def is_usable(self, batch_size):\n",
    "        return len(self.replay) >= batch_size\n",
    "    \n",
    "    def reset_replay_memory(self):\n",
    "        self.exp_count = 0\n",
    "        self.replay = []\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.replay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(sample):\n",
    "    states = np.asarray([i[0] for i in sample])\n",
    "    actions = np.asarray([i[1] for i in sample])\n",
    "    rewards = np.asarray([i[2] for i in sample])\n",
    "    next_states = np.asarray([i[3] for i in sample])\n",
    "    done_tensor = np.asarray([i[4] for i in sample])\n",
    "    return states, actions, rewards, next_states, done_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(states, actions, rewards, next_states, dones, net, gamma):\n",
    "    assert actions.ndim == 1\n",
    "    assert rewards.ndim == 1\n",
    "    assert dones.ndim == 1\n",
    "    assert len(actions) == len(rewards) == len(dones) == len(states) == len(next_states)\n",
    "    target_q_values = net.get_q_values_for_batch(states) # get the q values from the current states\n",
    "    targets = rewards + gamma * (np.max(net.get_target_tensor(next_states), axis=1)) # get the target values for the state action pairs.\n",
    "    for i in range(len(targets)): # change the targets for state which ended an episode\n",
    "        if dones[i]:\n",
    "            targets[i] = rewards[i]\n",
    "    \n",
    "    for index in range(len(target_q_values)):\n",
    "        target_q_values[index][actions[index]] = targets[index] # assign the targets to the corresponding state action pairs\n",
    "\n",
    "    return target_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_agent(contd=True, verbose=False, num_episodes=1500,\n",
    "                discount = 0.95, batch_size = 64, N = 40, memory_size = 1024, \n",
    "                eps_decay_rate=0.9975, max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "    # get all the hyperparameters in one place!\n",
    "    \n",
    "    # stores all the total reward per episode\n",
    "    training_stats = []    \n",
    "    \n",
    "    # initialise your environment\n",
    "    env = EnvironmentWrapper()\n",
    "    \n",
    "    # initialise your policy and target networks\n",
    "    net = NetworkTracker(env, source = contd)\n",
    "    print(net.get_model_summary())\n",
    "    \n",
    "    # initialise your agent that will follow the epsilon greedy strategy\n",
    "    agent = Agent(env, net, eps_decay_rate=eps_decay_rate, max_exp_rate=max_exp_rate,min_exp_rate=min_exp_rate )    \n",
    "    \n",
    "    # initialise experience replay memory\n",
    "    memory = Memory(memory_size)\n",
    "    \n",
    "    # graph display init code\n",
    "    epochs = []\n",
    "    %matplotlib notebook\n",
    "    plt.rcParams['animation.html'] = 'jshtml'\n",
    "    fig = plt.figure()\n",
    "    subplot = fig.add_subplot(111)\n",
    "    \n",
    "    for episode_count in range(num_episodes):\n",
    "        # uncomment if you want to start the environmet with a random move\n",
    "        # state = env.step(env.get_random_action())[0]\n",
    "        \n",
    "        # keeps track of the total reward that we got for this episode\n",
    "        \n",
    "        valid_episode = False\n",
    "        # check if the environment is available to run\n",
    "        while not valid_episode:\n",
    "            stuck_counter = 0\n",
    "            cumulative_reward = 0\n",
    "            counter = 0\n",
    "            state = env.reset()\n",
    "            while not env.is_done(): # run the environment for one episode\n",
    "                counter += 1\n",
    "                current_state, action, reward, next_state, done = agent.take_action(state) # let the agent take an action for one time step\n",
    "                cumulative_reward += reward \n",
    "                experience = current_state, action, reward, next_state, done # experience tuple \n",
    "                state = next_state # update the current state\n",
    "                memory.push(experience) # push the experience in memory\n",
    "                \n",
    "                # check if the car is stuck when the reward isn't changing by much\n",
    "                if abs(reward) < 0.5:\n",
    "                    stuck_counter += 1\n",
    "                    if stuck_counter > 5:\n",
    "                        break\n",
    "                else:\n",
    "                    stuck_counter = 0\n",
    "            \n",
    "            if counter > 3:\n",
    "                valid_episode = True\n",
    "        \n",
    "        agent.update_epsilon() # update the exploration rate of the agent after each episode\n",
    "        \n",
    "        if memory.is_usable(batch_size * 2):\n",
    "                experience_batch = memory.sample(batch_size) # sample randomly from memory\n",
    "                states, actions, rewards, next_states, done_tensor = extract_tensors(experience_batch) # unzips the tensors\n",
    "\n",
    "                target_batch = get_target_batch(states, actions, rewards, next_states, done_tensor, net, discount) # get a batch of target values to fit against\n",
    "                \n",
    "                net.fit(states, target_batch) # fit the network\n",
    "        \n",
    "        training_stats.append(cumulative_reward)\n",
    "        epochs.append(episode_count)\n",
    "        \n",
    "        if (episode_count + 1) % N == 0:\n",
    "            net.clone_policy()  # clone the target policy every N episodes.\n",
    "            \n",
    "        if (episode_count + 1) % 10 == 0:\n",
    "            subplot.plot(epochs, training_stats, color='b')\n",
    "            fig.canvas.draw()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Episode Count: \", episode_count, \"\\t Cumulative Reward: \", cumulative_reward, \"\\t eps: \", agent.exp_rate )\n",
    "\n",
    "        \n",
    "    \n",
    "    return epochs, training_stats, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = EnvironmentWrapper()\n",
    "# lmao = []\n",
    "# for i in range(10):\n",
    "    \n",
    "#     valid_episode = False\n",
    "#     while not valid_episode:\n",
    "#         counter = 0\n",
    "#         env.reset()\n",
    "#         while not env.is_done():\n",
    "#             counter += 1\n",
    "#             exp = env.step('as')\n",
    "            \n",
    "#             lmao.append(exp)\n",
    "        \n",
    "#         if counter > 3:\n",
    "#             valid_episode = True\n",
    "# env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to connect to Simulator...\n",
      "Connected to simulator!\n",
      "Directory  /Users/manavshah/GitHub/SelfDrivingResearch/simulation/Screenshots  already exists\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 126, 10)       370       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 42, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 20, 20)         1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 6, 20)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 360)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5776      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 8,391\n",
      "Trainable params: 8,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Count:  0 \t Cumulative Reward:  -0.23900000000000055 \t eps:  0.98901\n",
      "Episode Count:  1 \t Cumulative Reward:  13.173 \t eps:  0.98802099\n",
      "Episode Count:  2 \t Cumulative Reward:  2.8759999999999994 \t eps:  0.98703296901\n",
      "Episode Count:  3 \t Cumulative Reward:  7.936000000000002 \t eps:  0.98604593604099\n",
      "Episode Count:  4 \t Cumulative Reward:  42.99 \t eps:  0.985059890104949\n",
      "Episode Count:  5 \t Cumulative Reward:  18.188000000000002 \t eps:  0.984074830214844\n",
      "Episode Count:  6 \t Cumulative Reward:  7.361 \t eps:  0.9830907553846291\n",
      "Episode Count:  7 \t Cumulative Reward:  23.512999999999998 \t eps:  0.9821076646292445\n",
      "Episode Count:  8 \t Cumulative Reward:  11.088 \t eps:  0.9811255569646152\n",
      "Episode Count:  9 \t Cumulative Reward:  10.642 \t eps:  0.9801444314076506\n",
      "Episode Count:  10 \t Cumulative Reward:  3.366000000000003 \t eps:  0.9791642869762429\n",
      "Episode Count:  11 \t Cumulative Reward:  0.27599999999999775 \t eps:  0.9781851226892667\n",
      "Episode Count:  12 \t Cumulative Reward:  23.984000000000012 \t eps:  0.9772069375665775\n",
      "Episode Count:  13 \t Cumulative Reward:  1.9989999999999983 \t eps:  0.976229730629011\n",
      "Episode Count:  14 \t Cumulative Reward:  9.217 \t eps:  0.9752535008983819\n",
      "Episode Count:  15 \t Cumulative Reward:  26.711000000000002 \t eps:  0.9742782473974836\n",
      "Episode Count:  16 \t Cumulative Reward:  6.5980000000000025 \t eps:  0.9733039691500861\n",
      "Episode Count:  17 \t Cumulative Reward:  15.086000000000002 \t eps:  0.972330665180936\n",
      "Episode Count:  18 \t Cumulative Reward:  2.9900000000000007 \t eps:  0.971358334515755\n",
      "Episode Count:  19 \t Cumulative Reward:  23.967000000000002 \t eps:  0.9703869761812393\n",
      "Episode Count:  20 \t Cumulative Reward:  19.574 \t eps:  0.969416589205058\n",
      "Episode Count:  21 \t Cumulative Reward:  18.74 \t eps:  0.968447172615853\n",
      "Episode Count:  22 \t Cumulative Reward:  32.339 \t eps:  0.9674787254432371\n",
      "Episode Count:  23 \t Cumulative Reward:  20.881 \t eps:  0.9665112467177939\n",
      "Episode Count:  24 \t Cumulative Reward:  32.827000000000005 \t eps:  0.9655447354710761\n",
      "Episode Count:  25 \t Cumulative Reward:  6.651999999999997 \t eps:  0.964579190735605\n",
      "Episode Count:  26 \t Cumulative Reward:  8.604000000000001 \t eps:  0.9636146115448695\n",
      "Episode Count:  27 \t Cumulative Reward:  11.163 \t eps:  0.9626509969333247\n",
      "Episode Count:  28 \t Cumulative Reward:  22.787 \t eps:  0.9616883459363913\n",
      "Episode Count:  29 \t Cumulative Reward:  12.493000000000004 \t eps:  0.960726657590455\n",
      "Episode Count:  30 \t Cumulative Reward:  20.089 \t eps:  0.9597659309328644\n",
      "Episode Count:  31 \t Cumulative Reward:  7.895000000000001 \t eps:  0.9588061650019316\n",
      "Episode Count:  32 \t Cumulative Reward:  21.015000000000004 \t eps:  0.9578473588369297\n",
      "Episode Count:  33 \t Cumulative Reward:  25.336000000000002 \t eps:  0.9568895114780928\n",
      "Episode Count:  34 \t Cumulative Reward:  19.666 \t eps:  0.9559326219666147\n",
      "Episode Count:  35 \t Cumulative Reward:  12.768000000000002 \t eps:  0.9549766893446481\n",
      "Episode Count:  36 \t Cumulative Reward:  24.149999999999995 \t eps:  0.9540217126553034\n",
      "Episode Count:  37 \t Cumulative Reward:  3.4349999999999987 \t eps:  0.9530676909426481\n",
      "Episode Count:  38 \t Cumulative Reward:  12.124000000000002 \t eps:  0.9521146232517055\n",
      "Episode Count:  39 \t Cumulative Reward:  18.813 \t eps:  0.9511625086284538\n",
      "Episode Count:  40 \t Cumulative Reward:  11.188000000000002 \t eps:  0.9502113461198253\n",
      "Episode Count:  41 \t Cumulative Reward:  17.566000000000006 \t eps:  0.9492611347737054\n",
      "Episode Count:  42 \t Cumulative Reward:  5.3069999999999995 \t eps:  0.9483118736389318\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 1.4719\n",
      "Episode Count:  43 \t Cumulative Reward:  14.963000000000003 \t eps:  0.9473635617652928\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4988\n",
      "Episode Count:  44 \t Cumulative Reward:  35.613 \t eps:  0.9464161982035275\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3951\n",
      "Episode Count:  45 \t Cumulative Reward:  18.159000000000006 \t eps:  0.9454697820053239\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4086\n",
      "Episode Count:  46 \t Cumulative Reward:  14.284999999999997 \t eps:  0.9445243122233187\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5064\n",
      "Episode Count:  47 \t Cumulative Reward:  18.941999999999997 \t eps:  0.9435797879110953\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4607\n",
      "Episode Count:  48 \t Cumulative Reward:  13.600999999999999 \t eps:  0.9426362081231843\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4110\n",
      "Episode Count:  49 \t Cumulative Reward:  8.254000000000001 \t eps:  0.9416935719150611\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4720\n",
      "Episode Count:  50 \t Cumulative Reward:  14.482999999999993 \t eps:  0.940751878343146\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4305\n",
      "Episode Count:  51 \t Cumulative Reward:  26.148999999999997 \t eps:  0.9398111264648028\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4686\n",
      "Episode Count:  52 \t Cumulative Reward:  8.623000000000006 \t eps:  0.938871315338338\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5005\n",
      "Episode Count:  53 \t Cumulative Reward:  15.310000000000002 \t eps:  0.9379324440229997\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4770\n",
      "Episode Count:  54 \t Cumulative Reward:  35.765 \t eps:  0.9369945115789767\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4422\n",
      "Episode Count:  55 \t Cumulative Reward:  0.29400000000000226 \t eps:  0.9360575170673977\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5120\n",
      "Episode Count:  56 \t Cumulative Reward:  59.888 \t eps:  0.9351214595503303\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3804\n",
      "Episode Count:  57 \t Cumulative Reward:  60.105000000000004 \t eps:  0.93418633809078\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3854\n",
      "Episode Count:  58 \t Cumulative Reward:  0.19499999999999643 \t eps:  0.9332521517526893\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4282\n",
      "Episode Count:  59 \t Cumulative Reward:  29.219 \t eps:  0.9323188996009366\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4210\n",
      "Episode Count:  60 \t Cumulative Reward:  5.767999999999995 \t eps:  0.9313865807013356\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3230\n",
      "Episode Count:  61 \t Cumulative Reward:  14.223000000000003 \t eps:  0.9304551941206343\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1666\n",
      "Episode Count:  62 \t Cumulative Reward:  15.62 \t eps:  0.9295247389265137\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2268\n",
      "Episode Count:  63 \t Cumulative Reward:  8.962 \t eps:  0.9285952141875872\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2075\n",
      "Episode Count:  64 \t Cumulative Reward:  16.117 \t eps:  0.9276666189733996\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2018\n",
      "Episode Count:  65 \t Cumulative Reward:  12.920000000000002 \t eps:  0.9267389523544263\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2501\n",
      "Episode Count:  66 \t Cumulative Reward:  12.716000000000001 \t eps:  0.9258122134020719\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2666\n",
      "Episode Count:  67 \t Cumulative Reward:  6.708999999999999 \t eps:  0.9248864011886698\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2539\n",
      "Episode Count:  68 \t Cumulative Reward:  11.017 \t eps:  0.9239615147874811\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2256\n",
      "Episode Count:  69 \t Cumulative Reward:  7.860000000000001 \t eps:  0.9230375532726937\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2034\n",
      "Episode Count:  70 \t Cumulative Reward:  12.572000000000001 \t eps:  0.9221145157194209\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1914\n",
      "Episode Count:  71 \t Cumulative Reward:  20.928 \t eps:  0.9211924012037015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3511\n",
      "Episode Count:  72 \t Cumulative Reward:  10.382 \t eps:  0.9202712088024978\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3407\n",
      "Episode Count:  73 \t Cumulative Reward:  4.571000000000003 \t eps:  0.9193509375936953\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3941\n",
      "Episode Count:  74 \t Cumulative Reward:  -0.0930000000000018 \t eps:  0.9184315866561017\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2483\n",
      "Episode Count:  75 \t Cumulative Reward:  -0.21599999999999886 \t eps:  0.9175131550694456\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3701\n",
      "Episode Count:  76 \t Cumulative Reward:  8.13 \t eps:  0.9165956419143761\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2191\n",
      "Episode Count:  77 \t Cumulative Reward:  0.5699999999999997 \t eps:  0.9156790462724618\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2530\n",
      "Episode Count:  78 \t Cumulative Reward:  24.181999999999995 \t eps:  0.9147633672261893\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2046\n",
      "Episode Count:  79 \t Cumulative Reward:  8.657 \t eps:  0.9138486038589632\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3946\n",
      "Episode Count:  80 \t Cumulative Reward:  15.995000000000003 \t eps:  0.9129347552551041\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 1.3328\n",
      "Episode Count:  81 \t Cumulative Reward:  14.560999999999998 \t eps:  0.912021820499849\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2545\n",
      "Episode Count:  82 \t Cumulative Reward:  10.277999999999999 \t eps:  0.9111097986793492\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2071\n",
      "Episode Count:  83 \t Cumulative Reward:  18.656000000000006 \t eps:  0.9101986888806698\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3890\n",
      "Episode Count:  84 \t Cumulative Reward:  12.746000000000004 \t eps:  0.9092884901917891\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2126\n",
      "Episode Count:  85 \t Cumulative Reward:  6.144000000000008 \t eps:  0.9083792017015974\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4228\n",
      "Episode Count:  86 \t Cumulative Reward:  9.514 \t eps:  0.9074708224998957\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0493\n",
      "Episode Count:  87 \t Cumulative Reward:  38.602999999999994 \t eps:  0.9065633516773959\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3205\n",
      "Episode Count:  88 \t Cumulative Reward:  24.044 \t eps:  0.9056567883257185\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0702\n",
      "Episode Count:  89 \t Cumulative Reward:  8.054 \t eps:  0.9047511315373927\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2156\n",
      "Episode Count:  90 \t Cumulative Reward:  0.5089999999999977 \t eps:  0.9038463804058553\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2953\n",
      "Episode Count:  91 \t Cumulative Reward:  65.019 \t eps:  0.9029425340254494\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1815\n",
      "Episode Count:  92 \t Cumulative Reward:  21.688000000000002 \t eps:  0.902039591491424\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2086\n",
      "Episode Count:  93 \t Cumulative Reward:  34.287000000000006 \t eps:  0.9011375518999326\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1941\n",
      "Episode Count:  94 \t Cumulative Reward:  35.53999999999999 \t eps:  0.9002364143480327\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3241\n",
      "Episode Count:  95 \t Cumulative Reward:  10.176000000000002 \t eps:  0.8993361779336847\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1748\n",
      "Episode Count:  96 \t Cumulative Reward:  5.690999999999998 \t eps:  0.898436841755751\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1351\n",
      "Episode Count:  97 \t Cumulative Reward:  0.49299999999999944 \t eps:  0.8975384049139953\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3045\n",
      "Episode Count:  98 \t Cumulative Reward:  8.095 \t eps:  0.8966408665090813\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1120\n",
      "Episode Count:  99 \t Cumulative Reward:  10.752 \t eps:  0.8957442256425722\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3572\n",
      "Episode Count:  100 \t Cumulative Reward:  13.176000000000002 \t eps:  0.8948484814169296\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7163\n",
      "Episode Count:  101 \t Cumulative Reward:  13.163000000000002 \t eps:  0.8939536329355127\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5180\n",
      "Episode Count:  102 \t Cumulative Reward:  5.384 \t eps:  0.8930596793025771\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5212\n",
      "Episode Count:  103 \t Cumulative Reward:  0.628000000000002 \t eps:  0.8921666196232746\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4248\n",
      "Episode Count:  104 \t Cumulative Reward:  4.001000000000005 \t eps:  0.8912744530036513\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6499\n",
      "Episode Count:  105 \t Cumulative Reward:  0.5300000000000015 \t eps:  0.8903831785506476\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4253\n",
      "Episode Count:  106 \t Cumulative Reward:  9.777000000000003 \t eps:  0.889492795372097\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3073\n",
      "Episode Count:  107 \t Cumulative Reward:  6.435 \t eps:  0.8886033025767249\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4926\n",
      "Episode Count:  108 \t Cumulative Reward:  5.798999999999996 \t eps:  0.8877146992741483\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4616\n",
      "Episode Count:  109 \t Cumulative Reward:  19.153 \t eps:  0.8868269845748741\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2954\n",
      "Episode Count:  110 \t Cumulative Reward:  56.84300000000002 \t eps:  0.8859401575902992\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2683\n",
      "Episode Count:  111 \t Cumulative Reward:  18.130000000000003 \t eps:  0.885054217432709\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1192\n",
      "Episode Count:  112 \t Cumulative Reward:  10.763000000000002 \t eps:  0.8841691632152763\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4039\n",
      "Episode Count:  113 \t Cumulative Reward:  14.497999999999998 \t eps:  0.883284994052061\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5162\n",
      "Episode Count:  114 \t Cumulative Reward:  15.075000000000001 \t eps:  0.8824017090580089\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4843\n",
      "Episode Count:  115 \t Cumulative Reward:  10.583999999999998 \t eps:  0.8815193073489509\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4187\n",
      "Episode Count:  116 \t Cumulative Reward:  16.283 \t eps:  0.880637788041602\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3721\n",
      "Episode Count:  117 \t Cumulative Reward:  19.982999999999997 \t eps:  0.8797571502535604\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4037\n",
      "Episode Count:  118 \t Cumulative Reward:  32.717999999999996 \t eps:  0.8788773931033068\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1399\n",
      "Episode Count:  119 \t Cumulative Reward:  78.59000000000002 \t eps:  0.8779985157102035\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2662\n",
      "Episode Count:  120 \t Cumulative Reward:  8.174000000000003 \t eps:  0.8771205171944932\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2876\n",
      "Episode Count:  121 \t Cumulative Reward:  8.951 \t eps:  0.8762433966772988\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6657\n",
      "Episode Count:  122 \t Cumulative Reward:  10.323000000000002 \t eps:  0.8753671532806215\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2775\n",
      "Episode Count:  123 \t Cumulative Reward:  5.030000000000001 \t eps:  0.8744917861273409\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4704\n",
      "Episode Count:  124 \t Cumulative Reward:  5.274 \t eps:  0.8736172943412136\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3284\n",
      "Episode Count:  125 \t Cumulative Reward:  19.776000000000003 \t eps:  0.8727436770468724\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5974\n",
      "Episode Count:  126 \t Cumulative Reward:  5.843000000000001 \t eps:  0.8718709333698255\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7207\n",
      "Episode Count:  127 \t Cumulative Reward:  -3.317000000000002 \t eps:  0.8709990624364556\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5705\n",
      "Episode Count:  128 \t Cumulative Reward:  13.846 \t eps:  0.8701280633740192\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6460\n",
      "Episode Count:  129 \t Cumulative Reward:  11.001000000000001 \t eps:  0.8692579353106451\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5679\n",
      "Episode Count:  130 \t Cumulative Reward:  0.2979999999999998 \t eps:  0.8683886773753344\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6311\n",
      "Episode Count:  131 \t Cumulative Reward:  61.38599999999999 \t eps:  0.867520288697959\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0387\n",
      "Episode Count:  132 \t Cumulative Reward:  15.431 \t eps:  0.8666527684092611\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0752\n",
      "Episode Count:  133 \t Cumulative Reward:  13.723000000000006 \t eps:  0.8657861156408518\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2497\n",
      "Episode Count:  134 \t Cumulative Reward:  21.714 \t eps:  0.864920329525211\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3195\n",
      "Episode Count:  135 \t Cumulative Reward:  17.590999999999998 \t eps:  0.8640554091956858\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6062\n",
      "Episode Count:  136 \t Cumulative Reward:  0.08000000000000174 \t eps:  0.8631913537864901\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4383\n",
      "Episode Count:  137 \t Cumulative Reward:  0.4410000000000008 \t eps:  0.8623281624327036\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4655\n",
      "Episode Count:  138 \t Cumulative Reward:  -25.249 \t eps:  0.8614658342702709\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4993\n",
      "Episode Count:  139 \t Cumulative Reward:  10.975999999999999 \t eps:  0.8606043684360006\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4789\n",
      "Episode Count:  140 \t Cumulative Reward:  17.040000000000003 \t eps:  0.8597437640675646\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4898\n",
      "Episode Count:  141 \t Cumulative Reward:  7.403000000000001 \t eps:  0.858884020303497\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4676\n",
      "Episode Count:  142 \t Cumulative Reward:  39.399000000000015 \t eps:  0.8580251362831934\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1811\n",
      "Episode Count:  143 \t Cumulative Reward:  7.368999999999996 \t eps:  0.8571671111469102\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3919\n",
      "Episode Count:  144 \t Cumulative Reward:  14.665000000000003 \t eps:  0.8563099440357633\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2132\n",
      "Episode Count:  145 \t Cumulative Reward:  5.302000000000002 \t eps:  0.8554536340917276\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5120\n",
      "Episode Count:  146 \t Cumulative Reward:  19.665 \t eps:  0.8545981804576359\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2627\n",
      "Episode Count:  147 \t Cumulative Reward:  19.202 \t eps:  0.8537435822771783\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1537\n",
      "Episode Count:  148 \t Cumulative Reward:  31.907000000000004 \t eps:  0.8528898386949011\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4040\n",
      "Episode Count:  149 \t Cumulative Reward:  4.043999999999999 \t eps:  0.8520369488562062\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3932\n",
      "Episode Count:  150 \t Cumulative Reward:  10.112999999999998 \t eps:  0.85118491190735\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5304\n",
      "Episode Count:  151 \t Cumulative Reward:  19.583000000000006 \t eps:  0.8503337269954426\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3628\n",
      "Episode Count:  152 \t Cumulative Reward:  15.478000000000002 \t eps:  0.8494833932684472\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3706\n",
      "Episode Count:  153 \t Cumulative Reward:  -0.027000000000001793 \t eps:  0.8486339098751788\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4659\n",
      "Episode Count:  154 \t Cumulative Reward:  21.706 \t eps:  0.8477852759653036\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5137\n",
      "Episode Count:  155 \t Cumulative Reward:  9.765000000000002 \t eps:  0.8469374906893383\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5571\n",
      "Episode Count:  156 \t Cumulative Reward:  8.829000000000002 \t eps:  0.846090553198649\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3459\n",
      "Episode Count:  157 \t Cumulative Reward:  21.075 \t eps:  0.8452444626454504\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2383\n",
      "Episode Count:  158 \t Cumulative Reward:  22.395 \t eps:  0.844399218182805\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3864\n",
      "Episode Count:  159 \t Cumulative Reward:  0.0030000000000022925 \t eps:  0.8435548189646221\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2506\n",
      "Episode Count:  160 \t Cumulative Reward:  0.3580000000000038 \t eps:  0.8427112641456576\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2350\n",
      "Episode Count:  161 \t Cumulative Reward:  12.141000000000002 \t eps:  0.8418685528815119\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6976\n",
      "Episode Count:  162 \t Cumulative Reward:  6.23 \t eps:  0.8410266843286304\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1911\n",
      "Episode Count:  163 \t Cumulative Reward:  6.579 \t eps:  0.8401856576443018\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6082\n",
      "Episode Count:  164 \t Cumulative Reward:  8.232000000000005 \t eps:  0.8393454719866574\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3644\n",
      "Episode Count:  165 \t Cumulative Reward:  94.70400000000001 \t eps:  0.8385061265146708\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3662\n",
      "Episode Count:  166 \t Cumulative Reward:  21.161000000000005 \t eps:  0.8376676203881561\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4440\n",
      "Episode Count:  167 \t Cumulative Reward:  16.916 \t eps:  0.836829952767768\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4972\n",
      "Episode Count:  168 \t Cumulative Reward:  32.452999999999996 \t eps:  0.8359931228150002\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4557\n",
      "Episode Count:  169 \t Cumulative Reward:  9.397 \t eps:  0.8351571296921851\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6200\n",
      "Episode Count:  170 \t Cumulative Reward:  8.635 \t eps:  0.834321972562493\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2529\n",
      "Episode Count:  171 \t Cumulative Reward:  5.929000000000002 \t eps:  0.8334876505899305\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4135\n",
      "Episode Count:  172 \t Cumulative Reward:  13.711999999999996 \t eps:  0.8326541629393406\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5044\n",
      "Episode Count:  173 \t Cumulative Reward:  6.731000000000001 \t eps:  0.8318215087764013\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3670\n",
      "Episode Count:  174 \t Cumulative Reward:  35.888 \t eps:  0.8309896872676249\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4070\n",
      "Episode Count:  175 \t Cumulative Reward:  8.271 \t eps:  0.8301586975803573\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4978\n",
      "Episode Count:  176 \t Cumulative Reward:  4.899000000000003 \t eps:  0.8293285388827769\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4619\n",
      "Episode Count:  177 \t Cumulative Reward:  7.518999999999988 \t eps:  0.8284992103438942\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2450\n",
      "Episode Count:  178 \t Cumulative Reward:  19.109 \t eps:  0.8276707111335503\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5944\n",
      "Episode Count:  179 \t Cumulative Reward:  6.434 \t eps:  0.8268430404224167\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7496\n",
      "Episode Count:  180 \t Cumulative Reward:  5.699000000000002 \t eps:  0.8260161973819943\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3545\n",
      "Episode Count:  181 \t Cumulative Reward:  29.545000000000005 \t eps:  0.8251901811846123\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1978\n",
      "Episode Count:  182 \t Cumulative Reward:  4.846999999999998 \t eps:  0.8243649910034276\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3103\n",
      "Episode Count:  183 \t Cumulative Reward:  6.837999999999999 \t eps:  0.8235406260124242\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4128\n",
      "Episode Count:  184 \t Cumulative Reward:  45.509 \t eps:  0.8227170853864118\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4307\n",
      "Episode Count:  185 \t Cumulative Reward:  16.083000000000002 \t eps:  0.8218943683010254\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4600\n",
      "Episode Count:  186 \t Cumulative Reward:  0.14800000000000152 \t eps:  0.8210724739327243\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2041\n",
      "Episode Count:  187 \t Cumulative Reward:  4.822000000000001 \t eps:  0.8202514014587916\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3257\n",
      "Episode Count:  188 \t Cumulative Reward:  32.434 \t eps:  0.8194311500573328\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4530\n",
      "Episode Count:  189 \t Cumulative Reward:  6.862999999999999 \t eps:  0.8186117189072755\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2622\n",
      "Episode Count:  190 \t Cumulative Reward:  4.551 \t eps:  0.8177931071883682\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.0898\n",
      "Episode Count:  191 \t Cumulative Reward:  9.483 \t eps:  0.8169753140811798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6143\n",
      "Episode Count:  192 \t Cumulative Reward:  17.680000000000003 \t eps:  0.8161583387670986\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4392\n",
      "Episode Count:  193 \t Cumulative Reward:  42.308 \t eps:  0.8153421804283315\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5411\n",
      "Episode Count:  194 \t Cumulative Reward:  25.358 \t eps:  0.8145268382479032\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1343\n",
      "Episode Count:  195 \t Cumulative Reward:  7.796999999999998 \t eps:  0.8137123114096553\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1845\n",
      "Episode Count:  196 \t Cumulative Reward:  18.149 \t eps:  0.8128985990982456\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8040\n",
      "Episode Count:  197 \t Cumulative Reward:  16.246 \t eps:  0.8120857004991473\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6716\n",
      "Episode Count:  198 \t Cumulative Reward:  12.367999999999999 \t eps:  0.8112736147986481\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6688\n",
      "Episode Count:  199 \t Cumulative Reward:  3.9799999999999995 \t eps:  0.8104623411838495\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6519\n",
      "Episode Count:  200 \t Cumulative Reward:  25.467000000000002 \t eps:  0.8096518788426657\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6420\n",
      "Episode Count:  201 \t Cumulative Reward:  52.07 \t eps:  0.808842226963823\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6298\n",
      "Episode Count:  202 \t Cumulative Reward:  13.388000000000003 \t eps:  0.8080333847368592\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5393\n",
      "Episode Count:  203 \t Cumulative Reward:  28.518 \t eps:  0.8072253513521223\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2467\n",
      "Episode Count:  204 \t Cumulative Reward:  -5.394000000000002 \t eps:  0.8064181260007701\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3868\n",
      "Episode Count:  205 \t Cumulative Reward:  6.291000000000002 \t eps:  0.8056117078747693\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5148\n",
      "Episode Count:  206 \t Cumulative Reward:  32.119 \t eps:  0.8048060961668946\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4394\n",
      "Episode Count:  207 \t Cumulative Reward:  6.7020000000000035 \t eps:  0.8040012900707277\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5305\n",
      "Episode Count:  208 \t Cumulative Reward:  13.565 \t eps:  0.803197288780657\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1523\n",
      "Episode Count:  209 \t Cumulative Reward:  11.234000000000004 \t eps:  0.8023940914918763\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4257\n",
      "Episode Count:  210 \t Cumulative Reward:  7.773999999999998 \t eps:  0.8015916974003845\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7200\n",
      "Episode Count:  211 \t Cumulative Reward:  14.839999999999996 \t eps:  0.8007901057029841\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4041\n",
      "Episode Count:  212 \t Cumulative Reward:  19.663 \t eps:  0.7999893155972811\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0600\n",
      "Episode Count:  213 \t Cumulative Reward:  17.502000000000002 \t eps:  0.7991893262816838\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3306\n",
      "Episode Count:  214 \t Cumulative Reward:  -0.18499999999999964 \t eps:  0.7983901369554022\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2327\n",
      "Episode Count:  215 \t Cumulative Reward:  39.38000000000001 \t eps:  0.7975917468184468\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3831\n",
      "Episode Count:  216 \t Cumulative Reward:  6.489999999999999 \t eps:  0.7967941550716283\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1505\n",
      "Episode Count:  217 \t Cumulative Reward:  7.467 \t eps:  0.7959973609165567\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2233\n",
      "Episode Count:  218 \t Cumulative Reward:  7.392000000000003 \t eps:  0.7952013635556402\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3928\n",
      "Episode Count:  219 \t Cumulative Reward:  3.272000000000001 \t eps:  0.7944061621920846\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7323\n",
      "Episode Count:  220 \t Cumulative Reward:  16.668000000000003 \t eps:  0.7936117560298925\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2120\n",
      "Episode Count:  221 \t Cumulative Reward:  15.970999999999998 \t eps:  0.7928181442738627\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3720\n",
      "Episode Count:  222 \t Cumulative Reward:  62.181 \t eps:  0.7920253261295888\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4756\n",
      "Episode Count:  223 \t Cumulative Reward:  15.754000000000001 \t eps:  0.7912333008034592\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1191\n",
      "Episode Count:  224 \t Cumulative Reward:  12.049 \t eps:  0.7904420675026558\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4943\n",
      "Episode Count:  225 \t Cumulative Reward:  7.253999999999999 \t eps:  0.7896516254351531\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3379\n",
      "Episode Count:  226 \t Cumulative Reward:  9.155 \t eps:  0.7888619738097179\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6662\n",
      "Episode Count:  227 \t Cumulative Reward:  20.817 \t eps:  0.7880731118359082\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6153\n",
      "Episode Count:  228 \t Cumulative Reward:  32.961000000000006 \t eps:  0.7872850387240723\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1560\n",
      "Episode Count:  229 \t Cumulative Reward:  19.441 \t eps:  0.7864977536853482\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4775\n",
      "Episode Count:  230 \t Cumulative Reward:  11.207000000000006 \t eps:  0.7857112559316629\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4563\n",
      "Episode Count:  231 \t Cumulative Reward:  5.049999999999999 \t eps:  0.7849255446757313\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7424\n",
      "Episode Count:  232 \t Cumulative Reward:  7.553000000000002 \t eps:  0.7841406191310556\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2831\n",
      "Episode Count:  233 \t Cumulative Reward:  11.226000000000003 \t eps:  0.7833564785119246\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2398\n",
      "Episode Count:  234 \t Cumulative Reward:  20.533 \t eps:  0.7825731220334127\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3040\n",
      "Episode Count:  235 \t Cumulative Reward:  21.387 \t eps:  0.7817905489113792\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3209\n",
      "Episode Count:  236 \t Cumulative Reward:  8.607999999999997 \t eps:  0.7810087583624679\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4694\n",
      "Episode Count:  237 \t Cumulative Reward:  7.0390000000000015 \t eps:  0.7802277496041053\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6263\n",
      "Episode Count:  238 \t Cumulative Reward:  17.874 \t eps:  0.7794475218545013\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6730\n",
      "Episode Count:  239 \t Cumulative Reward:  19.573999999999998 \t eps:  0.7786680743326467\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2229\n",
      "Episode Count:  240 \t Cumulative Reward:  13.737 \t eps:  0.777889406258314\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4112\n",
      "Episode Count:  241 \t Cumulative Reward:  12.770999999999999 \t eps:  0.7771115168520557\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6672\n",
      "Episode Count:  242 \t Cumulative Reward:  14.055999999999997 \t eps:  0.7763344053352037\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5944\n",
      "Episode Count:  243 \t Cumulative Reward:  20.17 \t eps:  0.7755580709298685\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5878\n",
      "Episode Count:  244 \t Cumulative Reward:  18.896 \t eps:  0.7747825128589386\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3537\n",
      "Episode Count:  245 \t Cumulative Reward:  13.641000000000002 \t eps:  0.7740077303460797\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5024\n",
      "Episode Count:  246 \t Cumulative Reward:  17.352 \t eps:  0.7732337226157336\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5136\n",
      "Episode Count:  247 \t Cumulative Reward:  14.972000000000003 \t eps:  0.7724604888931179\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7218\n",
      "Episode Count:  248 \t Cumulative Reward:  18.618000000000002 \t eps:  0.7716880284042248\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3172\n",
      "Episode Count:  249 \t Cumulative Reward:  35.33400000000001 \t eps:  0.7709163403758206\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3085\n",
      "Episode Count:  250 \t Cumulative Reward:  18.196 \t eps:  0.7701454240354447\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4463\n",
      "Episode Count:  251 \t Cumulative Reward:  20.946999999999996 \t eps:  0.7693752786114093\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2705\n",
      "Episode Count:  252 \t Cumulative Reward:  8.218 \t eps:  0.7686059033327979\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3286\n",
      "Episode Count:  253 \t Cumulative Reward:  5.032000000000001 \t eps:  0.767837297429465\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4207\n",
      "Episode Count:  254 \t Cumulative Reward:  12.256999999999998 \t eps:  0.7670694601320356\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5037\n",
      "Episode Count:  255 \t Cumulative Reward:  6.778999999999998 \t eps:  0.7663023906719035\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4813\n",
      "Episode Count:  256 \t Cumulative Reward:  14.279999999999998 \t eps:  0.7655360882812317\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2048\n",
      "Episode Count:  257 \t Cumulative Reward:  17.363 \t eps:  0.7647705521929504\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4680\n",
      "Episode Count:  258 \t Cumulative Reward:  47.17400000000001 \t eps:  0.7640057816407575\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5526\n",
      "Episode Count:  259 \t Cumulative Reward:  22.529999999999998 \t eps:  0.7632417758591168\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7574\n",
      "Episode Count:  260 \t Cumulative Reward:  15.175999999999997 \t eps:  0.7624785340832577\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5927\n",
      "Episode Count:  261 \t Cumulative Reward:  6.874999999999999 \t eps:  0.7617160555491744\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5739\n",
      "Episode Count:  262 \t Cumulative Reward:  23.171 \t eps:  0.7609543394936252\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5605\n",
      "Episode Count:  263 \t Cumulative Reward:  17.884999999999994 \t eps:  0.7601933851541316\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4497\n",
      "Episode Count:  264 \t Cumulative Reward:  7.996 \t eps:  0.7594331917689775\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3867\n",
      "Episode Count:  265 \t Cumulative Reward:  14.987 \t eps:  0.7586737585772085\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7046\n",
      "Episode Count:  266 \t Cumulative Reward:  23.639999999999997 \t eps:  0.7579150848186313\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7009\n",
      "Episode Count:  267 \t Cumulative Reward:  24.279 \t eps:  0.7571571697338128\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5067\n",
      "Episode Count:  268 \t Cumulative Reward:  16.108 \t eps:  0.756400012564079\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7168\n",
      "Episode Count:  269 \t Cumulative Reward:  19.573999999999998 \t eps:  0.7556436125515149\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6143\n",
      "Episode Count:  270 \t Cumulative Reward:  57.713000000000015 \t eps:  0.7548879689389634\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6241\n",
      "Episode Count:  271 \t Cumulative Reward:  34.3 \t eps:  0.7541330809700244\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6084\n",
      "Episode Count:  272 \t Cumulative Reward:  12.870000000000001 \t eps:  0.7533789478890544\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4483\n",
      "Episode Count:  273 \t Cumulative Reward:  15.563999999999997 \t eps:  0.7526255689411653\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7280\n",
      "Episode Count:  274 \t Cumulative Reward:  17.346000000000004 \t eps:  0.7518729433722241\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8732\n",
      "Episode Count:  275 \t Cumulative Reward:  20.732 \t eps:  0.7511210704288519\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5560\n",
      "Episode Count:  276 \t Cumulative Reward:  29.261 \t eps:  0.750369949358423\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6317\n",
      "Episode Count:  277 \t Cumulative Reward:  5.862 \t eps:  0.7496195794090647\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4041\n",
      "Episode Count:  278 \t Cumulative Reward:  -0.010999999999999503 \t eps:  0.7488699598296555\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7278\n",
      "Episode Count:  279 \t Cumulative Reward:  17.583000000000002 \t eps:  0.7481210898698258\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4312\n",
      "Episode Count:  280 \t Cumulative Reward:  14.668000000000001 \t eps:  0.747372968779956\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5107\n",
      "Episode Count:  281 \t Cumulative Reward:  12.292999999999997 \t eps:  0.746625595811176\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5379\n",
      "Episode Count:  282 \t Cumulative Reward:  47.633 \t eps:  0.7458789702153649\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6578\n",
      "Episode Count:  283 \t Cumulative Reward:  23.505 \t eps:  0.7451330912451495\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5567\n",
      "Episode Count:  284 \t Cumulative Reward:  9.223 \t eps:  0.7443879581539043\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4662\n",
      "Episode Count:  285 \t Cumulative Reward:  9.454999999999998 \t eps:  0.7436435701957504\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5208\n",
      "Episode Count:  286 \t Cumulative Reward:  15.672999999999996 \t eps:  0.7428999266255547\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4932\n",
      "Episode Count:  287 \t Cumulative Reward:  36.159 \t eps:  0.7421570266989291\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4438\n",
      "Episode Count:  288 \t Cumulative Reward:  13.466999999999997 \t eps:  0.7414148696722302\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6457\n",
      "Episode Count:  289 \t Cumulative Reward:  18.47 \t eps:  0.740673454802558\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6229\n",
      "Episode Count:  290 \t Cumulative Reward:  6.824000000000001 \t eps:  0.7399327813477554\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7008\n",
      "Episode Count:  291 \t Cumulative Reward:  49.395 \t eps:  0.7391928485664077\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6754\n",
      "Episode Count:  292 \t Cumulative Reward:  26.933000000000003 \t eps:  0.7384536557178413\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6475\n",
      "Episode Count:  293 \t Cumulative Reward:  0.9459999999999964 \t eps:  0.7377152020621234\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5113\n",
      "Episode Count:  294 \t Cumulative Reward:  34.24500000000001 \t eps:  0.7369774868600613\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7411\n",
      "Episode Count:  295 \t Cumulative Reward:  8.873 \t eps:  0.7362405093732012\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8064\n",
      "Episode Count:  296 \t Cumulative Reward:  12.337 \t eps:  0.735504268863828\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6174\n",
      "Episode Count:  297 \t Cumulative Reward:  9.228999999999996 \t eps:  0.7347687645949642\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5452\n",
      "Episode Count:  298 \t Cumulative Reward:  31.757 \t eps:  0.7340339958303692\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6991\n",
      "Episode Count:  299 \t Cumulative Reward:  6.255 \t eps:  0.7332999618345388\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8688\n",
      "Episode Count:  300 \t Cumulative Reward:  25.468 \t eps:  0.7325666618727043\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0539\n",
      "Episode Count:  301 \t Cumulative Reward:  7.042999999999999 \t eps:  0.7318340952108315\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8779\n",
      "Episode Count:  302 \t Cumulative Reward:  5.969999999999997 \t eps:  0.7311022611156207\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0747\n",
      "Episode Count:  303 \t Cumulative Reward:  7.464 \t eps:  0.7303711588545051\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9290\n",
      "Episode Count:  304 \t Cumulative Reward:  8.056 \t eps:  0.7296407876956506\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9151\n",
      "Episode Count:  305 \t Cumulative Reward:  32.41900000000002 \t eps:  0.7289111469079549\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8097\n",
      "Episode Count:  306 \t Cumulative Reward:  62.052 \t eps:  0.7281822357610469\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0706\n",
      "Episode Count:  307 \t Cumulative Reward:  37.129 \t eps:  0.7274540535252858\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1412\n",
      "Episode Count:  308 \t Cumulative Reward:  3.282000000000001 \t eps:  0.7267265994717605\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0445\n",
      "Episode Count:  309 \t Cumulative Reward:  33.76900000000002 \t eps:  0.7259998728722887\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6953\n",
      "Episode Count:  310 \t Cumulative Reward:  18.372000000000003 \t eps:  0.7252738729994165\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8658\n",
      "Episode Count:  311 \t Cumulative Reward:  14.940000000000005 \t eps:  0.7245485991264171\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8461\n",
      "Episode Count:  312 \t Cumulative Reward:  22.591 \t eps:  0.7238240505272907\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9690\n",
      "Episode Count:  313 \t Cumulative Reward:  21.900000000000006 \t eps:  0.7231002264767634\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1569\n",
      "Episode Count:  314 \t Cumulative Reward:  21.589 \t eps:  0.7223771262502866\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7856\n",
      "Episode Count:  315 \t Cumulative Reward:  17.05 \t eps:  0.7216547491240363\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0511\n",
      "Episode Count:  316 \t Cumulative Reward:  14.591000000000003 \t eps:  0.7209330943749123\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8799\n",
      "Episode Count:  317 \t Cumulative Reward:  9.64100000000001 \t eps:  0.7202121612805373\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8243\n",
      "Episode Count:  318 \t Cumulative Reward:  9.638 \t eps:  0.7194919491192568\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9391\n",
      "Episode Count:  319 \t Cumulative Reward:  7.390999999999998 \t eps:  0.7187724571701376\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0016\n",
      "Episode Count:  320 \t Cumulative Reward:  21.463 \t eps:  0.7180536847129675\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9344\n",
      "Episode Count:  321 \t Cumulative Reward:  13.481999999999996 \t eps:  0.7173356310282545\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5148\n",
      "Episode Count:  322 \t Cumulative Reward:  26.962999999999997 \t eps:  0.7166182953972262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8833\n",
      "Episode Count:  323 \t Cumulative Reward:  36.309000000000005 \t eps:  0.715901677101829\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7377\n",
      "Episode Count:  324 \t Cumulative Reward:  4.491000000000001 \t eps:  0.7151857754247272\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7985\n",
      "Episode Count:  325 \t Cumulative Reward:  17.431 \t eps:  0.7144705896493024\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9239\n",
      "Episode Count:  326 \t Cumulative Reward:  7.865 \t eps:  0.7137561190596531\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8404\n",
      "Episode Count:  327 \t Cumulative Reward:  12.756000000000002 \t eps:  0.7130423629405934\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0698\n",
      "Episode Count:  328 \t Cumulative Reward:  35.19 \t eps:  0.7123293205776529\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0518\n",
      "Episode Count:  329 \t Cumulative Reward:  38.726000000000006 \t eps:  0.7116169912570752\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4436\n",
      "Episode Count:  330 \t Cumulative Reward:  7.785 \t eps:  0.7109053742658181\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7422\n",
      "Episode Count:  331 \t Cumulative Reward:  21.802999999999997 \t eps:  0.7101944688915524\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8460\n",
      "Episode Count:  332 \t Cumulative Reward:  9.581000000000001 \t eps:  0.7094842744226608\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7070\n",
      "Episode Count:  333 \t Cumulative Reward:  -0.0869999999999982 \t eps:  0.7087747901482382\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6457\n",
      "Episode Count:  334 \t Cumulative Reward:  -0.060000000000002815 \t eps:  0.70806601535809\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8501\n",
      "Episode Count:  335 \t Cumulative Reward:  -16.897 \t eps:  0.7073579493427319\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9353\n",
      "Episode Count:  336 \t Cumulative Reward:  8.561000000000003 \t eps:  0.7066505913933891\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7027\n",
      "Episode Count:  337 \t Cumulative Reward:  -0.10800000000000039 \t eps:  0.7059439408019957\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8108\n",
      "Episode Count:  338 \t Cumulative Reward:  44.33 \t eps:  0.7052379968611937\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7557\n",
      "Episode Count:  339 \t Cumulative Reward:  5.789000000000001 \t eps:  0.7045327588643325\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0771\n",
      "Episode Count:  340 \t Cumulative Reward:  7.562000000000003 \t eps:  0.7038282261054682\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7967\n",
      "Episode Count:  341 \t Cumulative Reward:  18.47 \t eps:  0.7031243978793628\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5711\n",
      "Episode Count:  342 \t Cumulative Reward:  8.438 \t eps:  0.7024212734814834\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5318\n",
      "Episode Count:  343 \t Cumulative Reward:  10.55 \t eps:  0.7017188522080019\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9787\n",
      "Episode Count:  344 \t Cumulative Reward:  -1.5929999999999955 \t eps:  0.7010171333557939\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6041\n",
      "Episode Count:  345 \t Cumulative Reward:  14.299000000000005 \t eps:  0.7003161162224381\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5412\n",
      "Episode Count:  346 \t Cumulative Reward:  14.145 \t eps:  0.6996158001062156\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8807\n",
      "Episode Count:  347 \t Cumulative Reward:  24.926 \t eps:  0.6989161843061094\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0986\n",
      "Episode Count:  348 \t Cumulative Reward:  19.31 \t eps:  0.6982172681218033\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3669\n",
      "Episode Count:  349 \t Cumulative Reward:  29.90400000000001 \t eps:  0.6975190508536815\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8051\n",
      "Episode Count:  350 \t Cumulative Reward:  38.20799999999999 \t eps:  0.6968215318028278\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7481\n",
      "Episode Count:  351 \t Cumulative Reward:  0.611999999999999 \t eps:  0.696124710271025\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6284\n",
      "Episode Count:  352 \t Cumulative Reward:  6.671000000000004 \t eps:  0.6954285855607539\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7989\n",
      "Episode Count:  353 \t Cumulative Reward:  8.654000000000002 \t eps:  0.6947331569751932\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6970\n",
      "Episode Count:  354 \t Cumulative Reward:  22.867 \t eps:  0.694038423818218\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6991\n",
      "Episode Count:  355 \t Cumulative Reward:  36.602 \t eps:  0.6933443853943998\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6416\n",
      "Episode Count:  356 \t Cumulative Reward:  11.874999999999998 \t eps:  0.6926510410090054\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8879\n",
      "Episode Count:  357 \t Cumulative Reward:  6.019 \t eps:  0.6919583899679964\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6347\n",
      "Episode Count:  358 \t Cumulative Reward:  19.354999999999997 \t eps:  0.6912664315780284\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3241\n",
      "Episode Count:  359 \t Cumulative Reward:  7.916000000000001 \t eps:  0.6905751651464503\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7142\n",
      "Episode Count:  360 \t Cumulative Reward:  14.218000000000002 \t eps:  0.6898845899813039\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1409\n",
      "Episode Count:  361 \t Cumulative Reward:  24.442000000000004 \t eps:  0.6891947053913225\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2161\n",
      "Episode Count:  362 \t Cumulative Reward:  8.866 \t eps:  0.6885055106859312\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3067\n",
      "Episode Count:  363 \t Cumulative Reward:  6.679 \t eps:  0.6878170051752454\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9722\n",
      "Episode Count:  364 \t Cumulative Reward:  4.705999999999999 \t eps:  0.6871291881700701\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7971\n",
      "Episode Count:  365 \t Cumulative Reward:  7.558000000000003 \t eps:  0.6864420589819\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7831\n",
      "Episode Count:  366 \t Cumulative Reward:  9.035 \t eps:  0.6857556169229181\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7341\n",
      "Episode Count:  367 \t Cumulative Reward:  21.589 \t eps:  0.6850698613059951\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1452\n",
      "Episode Count:  368 \t Cumulative Reward:  6.881000000000001 \t eps:  0.6843847914446891\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1654\n",
      "Episode Count:  369 \t Cumulative Reward:  7.1190000000000015 \t eps:  0.6837004066532444\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8952\n",
      "Episode Count:  370 \t Cumulative Reward:  8.591 \t eps:  0.6830167062465912\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8764\n",
      "Episode Count:  371 \t Cumulative Reward:  3.7370000000000005 \t eps:  0.6823336895403446\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8725\n",
      "Episode Count:  372 \t Cumulative Reward:  15.217 \t eps:  0.6816513558508043\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9771\n",
      "Episode Count:  373 \t Cumulative Reward:  16.134999999999998 \t eps:  0.6809697044949534\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9189\n",
      "Episode Count:  374 \t Cumulative Reward:  18.929 \t eps:  0.6802887347904585\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6548\n",
      "Episode Count:  375 \t Cumulative Reward:  -0.10500000000000219 \t eps:  0.6796084460556681\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5918\n",
      "Episode Count:  376 \t Cumulative Reward:  12.328000000000003 \t eps:  0.6789288376096124\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1073\n",
      "Episode Count:  377 \t Cumulative Reward:  10.166999999999998 \t eps:  0.6782499087720029\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8173\n",
      "Episode Count:  378 \t Cumulative Reward:  7.382999999999995 \t eps:  0.6775716588632309\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4270\n",
      "Episode Count:  379 \t Cumulative Reward:  22.316 \t eps:  0.6768940872043676\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7485\n",
      "Episode Count:  380 \t Cumulative Reward:  12.210999999999993 \t eps:  0.6762171931171632\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9768\n",
      "Episode Count:  381 \t Cumulative Reward:  58.062 \t eps:  0.675540975924046\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4684\n",
      "Episode Count:  382 \t Cumulative Reward:  16.905 \t eps:  0.674865434948122\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9577\n",
      "Episode Count:  383 \t Cumulative Reward:  11.405 \t eps:  0.6741905695131739\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3219\n",
      "Episode Count:  384 \t Cumulative Reward:  20.014 \t eps:  0.6735163789436608\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6331\n",
      "Episode Count:  385 \t Cumulative Reward:  14.411999999999997 \t eps:  0.6728428625647171\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1203\n",
      "Episode Count:  386 \t Cumulative Reward:  27.008000000000006 \t eps:  0.6721700197021524\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9426\n",
      "Episode Count:  387 \t Cumulative Reward:  18.526000000000003 \t eps:  0.6714978496824503\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8230\n",
      "Episode Count:  388 \t Cumulative Reward:  20.714000000000006 \t eps:  0.6708263518327678\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0235\n",
      "Episode Count:  389 \t Cumulative Reward:  17.899999999999995 \t eps:  0.670155525480935\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0986\n",
      "Episode Count:  390 \t Cumulative Reward:  15.672999999999998 \t eps:  0.6694853699554542\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7588\n",
      "Episode Count:  391 \t Cumulative Reward:  3.928999999999993 \t eps:  0.6688158845854987\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2659\n",
      "Episode Count:  392 \t Cumulative Reward:  36.730999999999995 \t eps:  0.6681470687009132\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1167\n",
      "Episode Count:  393 \t Cumulative Reward:  4.234999999999999 \t eps:  0.6674789216322123\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7536\n",
      "Episode Count:  394 \t Cumulative Reward:  -0.08700000000000105 \t eps:  0.6668114427105801\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8636\n",
      "Episode Count:  395 \t Cumulative Reward:  12.533000000000001 \t eps:  0.6661446312678695\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3704\n",
      "Episode Count:  396 \t Cumulative Reward:  7.976999999999999 \t eps:  0.6654784866366016\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5951\n",
      "Episode Count:  397 \t Cumulative Reward:  6.099 \t eps:  0.664813008149965\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0921\n",
      "Episode Count:  398 \t Cumulative Reward:  3.392000000000001 \t eps:  0.6641481951418151\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9686\n",
      "Episode Count:  399 \t Cumulative Reward:  11.740999999999998 \t eps:  0.6634840469466733\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0186\n",
      "Episode Count:  400 \t Cumulative Reward:  23.756000000000004 \t eps:  0.6628205628997266\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2015\n",
      "Episode Count:  401 \t Cumulative Reward:  35.538 \t eps:  0.6621577423368269\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9278\n",
      "Episode Count:  402 \t Cumulative Reward:  24.924000000000007 \t eps:  0.6614955845944901\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8497\n",
      "Episode Count:  403 \t Cumulative Reward:  11.052 \t eps:  0.6608340890098956\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0520\n",
      "Episode Count:  404 \t Cumulative Reward:  34.391000000000005 \t eps:  0.6601732549208857\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9644\n",
      "Episode Count:  405 \t Cumulative Reward:  22.860999999999994 \t eps:  0.6595130816659649\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6771\n",
      "Episode Count:  406 \t Cumulative Reward:  7.606000000000001 \t eps:  0.6588535685842989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9703\n",
      "Episode Count:  407 \t Cumulative Reward:  11.160999999999998 \t eps:  0.6581947150157146\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8810\n",
      "Episode Count:  408 \t Cumulative Reward:  35.577000000000005 \t eps:  0.6575365203006989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0899\n",
      "Episode Count:  409 \t Cumulative Reward:  5.960000000000001 \t eps:  0.6568789837803982\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3444\n",
      "Episode Count:  410 \t Cumulative Reward:  11.166999999999996 \t eps:  0.6562221047966178\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5994\n",
      "Episode Count:  411 \t Cumulative Reward:  14.503000000000004 \t eps:  0.6555658826918211\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3369\n",
      "Episode Count:  412 \t Cumulative Reward:  6.562000000000001 \t eps:  0.6549103168091294\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4751\n",
      "Episode Count:  413 \t Cumulative Reward:  17.84 \t eps:  0.6542554064923203\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8309\n",
      "Episode Count:  414 \t Cumulative Reward:  12.904000000000002 \t eps:  0.653601151085828\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0704\n",
      "Episode Count:  415 \t Cumulative Reward:  21.41200000000001 \t eps:  0.6529475499347421\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4157\n",
      "Episode Count:  416 \t Cumulative Reward:  6.472000000000002 \t eps:  0.6522946023848074\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3378\n",
      "Episode Count:  417 \t Cumulative Reward:  14.234000000000002 \t eps:  0.6516423077824226\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0573\n",
      "Episode Count:  418 \t Cumulative Reward:  15.288000000000002 \t eps:  0.6509906654746401\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7355\n",
      "Episode Count:  419 \t Cumulative Reward:  14.126000000000001 \t eps:  0.6503396748091654\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0184\n",
      "Episode Count:  420 \t Cumulative Reward:  14.744999999999997 \t eps:  0.6496893351343562\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9715\n",
      "Episode Count:  421 \t Cumulative Reward:  25.065 \t eps:  0.6490396457992219\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9474\n",
      "Episode Count:  422 \t Cumulative Reward:  13.813000000000004 \t eps:  0.6483906061534227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1341\n",
      "Episode Count:  423 \t Cumulative Reward:  8.537 \t eps:  0.6477422155472693\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3159\n",
      "Episode Count:  424 \t Cumulative Reward:  16.386000000000003 \t eps:  0.647094473331722\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4533\n",
      "Episode Count:  425 \t Cumulative Reward:  36.92500000000001 \t eps:  0.6464473788583902\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8625\n",
      "Episode Count:  426 \t Cumulative Reward:  11.462000000000002 \t eps:  0.6458009314795318\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2901\n",
      "Episode Count:  427 \t Cumulative Reward:  14.782 \t eps:  0.6451551305480523\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2928\n",
      "Episode Count:  428 \t Cumulative Reward:  18.704 \t eps:  0.6445099754175042\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9289\n",
      "Episode Count:  429 \t Cumulative Reward:  7.699000000000002 \t eps:  0.6438654654420867\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8495\n",
      "Episode Count:  430 \t Cumulative Reward:  15.483000000000002 \t eps:  0.6432215999766446\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2964\n",
      "Episode Count:  431 \t Cumulative Reward:  30.761 \t eps:  0.642578378376668\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1708\n",
      "Episode Count:  432 \t Cumulative Reward:  16.622 \t eps:  0.6419357999982913\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2913\n",
      "Episode Count:  433 \t Cumulative Reward:  19.328 \t eps:  0.641293864198293\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3109\n",
      "Episode Count:  434 \t Cumulative Reward:  6.494000000000002 \t eps:  0.6406525703340947\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0904\n",
      "Episode Count:  435 \t Cumulative Reward:  1.5979999999999994 \t eps:  0.6400119177637607\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0490\n",
      "Episode Count:  436 \t Cumulative Reward:  11.626000000000003 \t eps:  0.6393719058459969\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9567\n",
      "Episode Count:  437 \t Cumulative Reward:  7.739999999999999 \t eps:  0.638732533940151\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8775\n",
      "Episode Count:  438 \t Cumulative Reward:  29.585 \t eps:  0.6380938014062107\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9239\n",
      "Episode Count:  439 \t Cumulative Reward:  29.206999999999997 \t eps:  0.6374557076048045\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9600\n",
      "Episode Count:  440 \t Cumulative Reward:  7.698999999999999 \t eps:  0.6368182518971998\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7870\n",
      "Episode Count:  441 \t Cumulative Reward:  8.499999999999998 \t eps:  0.6361814336453026\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0389\n",
      "Episode Count:  442 \t Cumulative Reward:  9.554999999999998 \t eps:  0.6355452522116573\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1816\n",
      "Episode Count:  443 \t Cumulative Reward:  6.076000000000005 \t eps:  0.6349097069594456\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6925\n",
      "Episode Count:  444 \t Cumulative Reward:  11.304000000000002 \t eps:  0.6342747972524861\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0770\n",
      "Episode Count:  445 \t Cumulative Reward:  13.665999999999999 \t eps:  0.6336405224552336\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2568\n",
      "Episode Count:  446 \t Cumulative Reward:  41.46900000000001 \t eps:  0.6330068819327784\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2017\n",
      "Episode Count:  447 \t Cumulative Reward:  -0.00900000000000089 \t eps:  0.6323738750508456\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0787\n",
      "Episode Count:  448 \t Cumulative Reward:  4.437000000000003 \t eps:  0.6317415011757948\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1108\n",
      "Episode Count:  449 \t Cumulative Reward:  1.0850000000000004 \t eps:  0.6311097596746189\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6391\n",
      "Episode Count:  450 \t Cumulative Reward:  1.0609999999999973 \t eps:  0.6304786499149443\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6505\n",
      "Episode Count:  451 \t Cumulative Reward:  0.8130000000000025 \t eps:  0.6298481712650293\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8990\n",
      "Episode Count:  452 \t Cumulative Reward:  -6.910000000000002 \t eps:  0.6292183230937644\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7513\n",
      "Episode Count:  453 \t Cumulative Reward:  -0.1169999999999986 \t eps:  0.6285891047706705\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1247\n",
      "Episode Count:  454 \t Cumulative Reward:  12.427999999999997 \t eps:  0.6279605156658998\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9294\n",
      "Episode Count:  455 \t Cumulative Reward:  3.347999999999999 \t eps:  0.6273325551502339\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1527\n",
      "Episode Count:  456 \t Cumulative Reward:  5.091 \t eps:  0.6267052225950837\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7606\n",
      "Episode Count:  457 \t Cumulative Reward:  6.570000000000004 \t eps:  0.6260785173724885\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6126\n",
      "Episode Count:  458 \t Cumulative Reward:  -0.036 \t eps:  0.625452438855116\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6806\n",
      "Episode Count:  459 \t Cumulative Reward:  3.368000000000004 \t eps:  0.6248269864162609\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5823\n",
      "Episode Count:  460 \t Cumulative Reward:  16.552999999999997 \t eps:  0.6242021594298446\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8798\n",
      "Episode Count:  461 \t Cumulative Reward:  9.629999999999997 \t eps:  0.6235779572704148\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9384\n",
      "Episode Count:  462 \t Cumulative Reward:  16.222 \t eps:  0.6229543793131443\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7742\n",
      "Episode Count:  463 \t Cumulative Reward:  0.20599999999999638 \t eps:  0.6223314249338312\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8466\n",
      "Episode Count:  464 \t Cumulative Reward:  -0.2819999999999977 \t eps:  0.6217090935088974\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5325\n",
      "Episode Count:  465 \t Cumulative Reward:  11.092000000000002 \t eps:  0.6210873844153885\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8240\n",
      "Episode Count:  466 \t Cumulative Reward:  19.922 \t eps:  0.6204662970309731\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6693\n",
      "Episode Count:  467 \t Cumulative Reward:  22.339999999999996 \t eps:  0.6198458307339422\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5307\n",
      "Episode Count:  468 \t Cumulative Reward:  -0.19200000000000206 \t eps:  0.6192259849032082\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8416\n",
      "Episode Count:  469 \t Cumulative Reward:  12.092999999999998 \t eps:  0.6186067589183051\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0546\n",
      "Episode Count:  470 \t Cumulative Reward:  27.691000000000003 \t eps:  0.6179881521593867\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0148\n",
      "Episode Count:  471 \t Cumulative Reward:  15.527000000000005 \t eps:  0.6173701640072273\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8449\n",
      "Episode Count:  472 \t Cumulative Reward:  10.143000000000002 \t eps:  0.6167527938432201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6043\n",
      "Episode Count:  473 \t Cumulative Reward:  8.737 \t eps:  0.6161360410493769\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7223\n",
      "Episode Count:  474 \t Cumulative Reward:  7.345000000000002 \t eps:  0.6155199050083275\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6575\n",
      "Episode Count:  475 \t Cumulative Reward:  6.032000000000001 \t eps:  0.6149043851033192\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7237\n",
      "Episode Count:  476 \t Cumulative Reward:  4.390999999999998 \t eps:  0.6142894807182159\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7670\n",
      "Episode Count:  477 \t Cumulative Reward:  6.932000000000002 \t eps:  0.6136751912374977\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7877\n",
      "Episode Count:  478 \t Cumulative Reward:  31.677999999999997 \t eps:  0.6130615160462602\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6704\n",
      "Episode Count:  479 \t Cumulative Reward:  12.605000000000006 \t eps:  0.612448454530214\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3834\n",
      "Episode Count:  480 \t Cumulative Reward:  16.103999999999996 \t eps:  0.6118360060756838\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8045\n",
      "Episode Count:  481 \t Cumulative Reward:  26.698000000000004 \t eps:  0.611224170069608\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7971\n",
      "Episode Count:  482 \t Cumulative Reward:  16.526 \t eps:  0.6106129458995384\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6051\n",
      "Episode Count:  483 \t Cumulative Reward:  23.091 \t eps:  0.6100023329536388\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6321\n",
      "Episode Count:  484 \t Cumulative Reward:  27.889999999999993 \t eps:  0.6093923306206852\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6576\n",
      "Episode Count:  485 \t Cumulative Reward:  19.757999999999996 \t eps:  0.6087829382900645\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8468\n",
      "Episode Count:  486 \t Cumulative Reward:  15.924000000000001 \t eps:  0.6081741553517744\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3839\n",
      "Episode Count:  487 \t Cumulative Reward:  17.796 \t eps:  0.6075659811964227\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7161\n",
      "Episode Count:  488 \t Cumulative Reward:  25.177999999999997 \t eps:  0.6069584152152263\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6949\n",
      "Episode Count:  489 \t Cumulative Reward:  28.334 \t eps:  0.6063514568000111\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5888\n",
      "Episode Count:  490 \t Cumulative Reward:  13.652 \t eps:  0.6057451053432111\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9946\n",
      "Episode Count:  491 \t Cumulative Reward:  34.481 \t eps:  0.605139360237868\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6513\n",
      "Episode Count:  492 \t Cumulative Reward:  24.322 \t eps:  0.6045342208776301\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7014\n",
      "Episode Count:  493 \t Cumulative Reward:  22.410000000000004 \t eps:  0.6039296866567525\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6901\n",
      "Episode Count:  494 \t Cumulative Reward:  21.054000000000006 \t eps:  0.6033257569700957\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2276\n",
      "Episode Count:  495 \t Cumulative Reward:  13.015999999999998 \t eps:  0.6027224312131256\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6845\n",
      "Episode Count:  496 \t Cumulative Reward:  8.173999999999998 \t eps:  0.6021197087819125\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8906\n",
      "Episode Count:  497 \t Cumulative Reward:  33.748000000000005 \t eps:  0.6015175890731306\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8398\n",
      "Episode Count:  498 \t Cumulative Reward:  16.936000000000007 \t eps:  0.6009160714840575\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0980\n",
      "Episode Count:  499 \t Cumulative Reward:  8.058000000000003 \t eps:  0.6003151554125734\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9856\n",
      "Episode Count:  500 \t Cumulative Reward:  0.48400000000000154 \t eps:  0.5997148402571608\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 1.6681\n",
      "Episode Count:  501 \t Cumulative Reward:  8.501 \t eps:  0.5991151254169037\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8600\n",
      "Episode Count:  502 \t Cumulative Reward:  53.224999999999994 \t eps:  0.5985160102914868\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0250\n",
      "Episode Count:  503 \t Cumulative Reward:  23.391 \t eps:  0.5979174942811953\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7376\n",
      "Episode Count:  504 \t Cumulative Reward:  25.563000000000006 \t eps:  0.5973195767869142\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4516\n",
      "Episode Count:  505 \t Cumulative Reward:  22.562000000000005 \t eps:  0.5967222572101273\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9932\n",
      "Episode Count:  506 \t Cumulative Reward:  2.1720000000000006 \t eps:  0.5961255349529172\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0735\n",
      "Episode Count:  507 \t Cumulative Reward:  12.885000000000005 \t eps:  0.5955294094179643\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0534\n",
      "Episode Count:  508 \t Cumulative Reward:  15.493000000000002 \t eps:  0.5949338800085463\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9559\n",
      "Episode Count:  509 \t Cumulative Reward:  35.577999999999996 \t eps:  0.5943389461285378\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8506\n",
      "Episode Count:  510 \t Cumulative Reward:  7.594 \t eps:  0.5937446071824092\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8809\n",
      "Episode Count:  511 \t Cumulative Reward:  17.65 \t eps:  0.5931508625752268\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8172\n",
      "Episode Count:  512 \t Cumulative Reward:  20.175 \t eps:  0.5925577117126515\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9984\n",
      "Episode Count:  513 \t Cumulative Reward:  25.208 \t eps:  0.5919651540009389\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9572\n",
      "Episode Count:  514 \t Cumulative Reward:  26.050000000000004 \t eps:  0.5913731888469379\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0644\n",
      "Episode Count:  515 \t Cumulative Reward:  21.956000000000003 \t eps:  0.590781815658091\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0524\n",
      "Episode Count:  516 \t Cumulative Reward:  10.252999999999998 \t eps:  0.5901910338424329\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9498\n",
      "Episode Count:  517 \t Cumulative Reward:  -0.17400000000000118 \t eps:  0.5896008428085905\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1133\n",
      "Episode Count:  518 \t Cumulative Reward:  16.171 \t eps:  0.5890112419657819\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8180\n",
      "Episode Count:  519 \t Cumulative Reward:  20.067000000000007 \t eps:  0.5884222307238162\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9729\n",
      "Episode Count:  520 \t Cumulative Reward:  6.710000000000001 \t eps:  0.5878338084930923\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1855\n",
      "Episode Count:  521 \t Cumulative Reward:  10.767000000000005 \t eps:  0.5872459746845992\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1057\n",
      "Episode Count:  522 \t Cumulative Reward:  28.493000000000002 \t eps:  0.5866587287099145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1612\n",
      "Episode Count:  523 \t Cumulative Reward:  13.854999999999997 \t eps:  0.5860720699812046\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6639\n",
      "Episode Count:  524 \t Cumulative Reward:  -0.18299999999999744 \t eps:  0.5854859979112235\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7714\n",
      "Episode Count:  525 \t Cumulative Reward:  31.096000000000004 \t eps:  0.5849005119133123\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1250\n",
      "Episode Count:  526 \t Cumulative Reward:  1.1220000000000003 \t eps:  0.584315611401399\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6290\n",
      "Episode Count:  527 \t Cumulative Reward:  22.376000000000005 \t eps:  0.5837312957899976\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8060\n",
      "Episode Count:  528 \t Cumulative Reward:  19.326000000000004 \t eps:  0.5831475644942076\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2987\n",
      "Episode Count:  529 \t Cumulative Reward:  16.342 \t eps:  0.5825644169297134\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1371\n",
      "Episode Count:  530 \t Cumulative Reward:  23.355 \t eps:  0.5819818525127837\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2095\n",
      "Episode Count:  531 \t Cumulative Reward:  36.208 \t eps:  0.5813998706602709\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0686\n",
      "Episode Count:  532 \t Cumulative Reward:  21.643000000000004 \t eps:  0.5808184707896106\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1435\n",
      "Episode Count:  533 \t Cumulative Reward:  25.291999999999998 \t eps:  0.580237652318821\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7248\n",
      "Episode Count:  534 \t Cumulative Reward:  17.994999999999997 \t eps:  0.5796574146665021\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0939\n",
      "Episode Count:  535 \t Cumulative Reward:  12.459000000000003 \t eps:  0.5790777572518356\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3517\n",
      "Episode Count:  536 \t Cumulative Reward:  14.847999999999999 \t eps:  0.5784986794945838\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0863\n",
      "Episode Count:  537 \t Cumulative Reward:  22.513999999999996 \t eps:  0.5779201808150892\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1933\n",
      "Episode Count:  538 \t Cumulative Reward:  19.77 \t eps:  0.5773422606342742\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1533\n",
      "Episode Count:  539 \t Cumulative Reward:  10.237 \t eps:  0.5767649183736399\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8853\n",
      "Episode Count:  540 \t Cumulative Reward:  13.921999999999997 \t eps:  0.5761881534552662\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2100\n",
      "Episode Count:  541 \t Cumulative Reward:  24.849000000000004 \t eps:  0.5756119653018109\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2253\n",
      "Episode Count:  542 \t Cumulative Reward:  61.139 \t eps:  0.5750363533365092\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1089\n",
      "Episode Count:  543 \t Cumulative Reward:  24.323000000000008 \t eps:  0.5744613169831726\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2804\n",
      "Episode Count:  544 \t Cumulative Reward:  24.16 \t eps:  0.5738868556661895\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0539\n",
      "Episode Count:  545 \t Cumulative Reward:  13.473000000000003 \t eps:  0.5733129688105233\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3947\n",
      "Episode Count:  546 \t Cumulative Reward:  20.543999999999993 \t eps:  0.5727396558417128\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9708\n",
      "Episode Count:  547 \t Cumulative Reward:  11.709000000000001 \t eps:  0.5721669161858711\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9971\n",
      "Episode Count:  548 \t Cumulative Reward:  11.463 \t eps:  0.5715947492696852\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6013\n",
      "Episode Count:  549 \t Cumulative Reward:  10.850000000000001 \t eps:  0.5710231545204155\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1616\n",
      "Episode Count:  550 \t Cumulative Reward:  36.29299999999999 \t eps:  0.5704521313658951\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8487\n",
      "Episode Count:  551 \t Cumulative Reward:  21.368 \t eps:  0.5698816792345293\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9703\n",
      "Episode Count:  552 \t Cumulative Reward:  28.868000000000002 \t eps:  0.5693117975552947\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3503\n",
      "Episode Count:  553 \t Cumulative Reward:  25.639 \t eps:  0.5687424857577394\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3293\n",
      "Episode Count:  554 \t Cumulative Reward:  18.320999999999994 \t eps:  0.5681737432719817\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4241\n",
      "Episode Count:  555 \t Cumulative Reward:  29.998 \t eps:  0.5676055695287097\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4459\n",
      "Episode Count:  556 \t Cumulative Reward:  14.258000000000003 \t eps:  0.5670379639591809\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1214\n",
      "Episode Count:  557 \t Cumulative Reward:  30.582 \t eps:  0.5664709259952218\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4625\n",
      "Episode Count:  558 \t Cumulative Reward:  9.767000000000001 \t eps:  0.5659044550692265\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.6490\n",
      "Episode Count:  559 \t Cumulative Reward:  30.553999999999995 \t eps:  0.5653385506141573\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3271\n",
      "Episode Count:  560 \t Cumulative Reward:  21.019000000000002 \t eps:  0.5647732120635431\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5364\n",
      "Episode Count:  561 \t Cumulative Reward:  17.617 \t eps:  0.5642084388514796\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4721\n",
      "Episode Count:  562 \t Cumulative Reward:  6.894000000000002 \t eps:  0.5636442304126281\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6795\n",
      "Episode Count:  563 \t Cumulative Reward:  9.806000000000001 \t eps:  0.5630805861822155\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4650\n",
      "Episode Count:  564 \t Cumulative Reward:  9.515 \t eps:  0.5625175055960332\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2971\n",
      "Episode Count:  565 \t Cumulative Reward:  5.916000000000002 \t eps:  0.5619549880904372\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3332\n",
      "Episode Count:  566 \t Cumulative Reward:  21.849999999999998 \t eps:  0.5613930331023468\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9159\n",
      "Episode Count:  567 \t Cumulative Reward:  20.699999999999996 \t eps:  0.5608316400692445\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5776\n",
      "Episode Count:  568 \t Cumulative Reward:  9.219999999999997 \t eps:  0.5602708084291752\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5296\n",
      "Episode Count:  569 \t Cumulative Reward:  10.691 \t eps:  0.5597105376207461\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3214\n",
      "Episode Count:  570 \t Cumulative Reward:  12.463000000000003 \t eps:  0.5591508270831254\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1473\n",
      "Episode Count:  571 \t Cumulative Reward:  13.631000000000004 \t eps:  0.5585916762560422\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4914\n",
      "Episode Count:  572 \t Cumulative Reward:  14.596 \t eps:  0.5580330845797862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0903\n",
      "Episode Count:  573 \t Cumulative Reward:  13.134000000000002 \t eps:  0.5574750514952064\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1787\n",
      "Episode Count:  574 \t Cumulative Reward:  10.615000000000006 \t eps:  0.5569175764437112\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7008\n",
      "Episode Count:  575 \t Cumulative Reward:  24.105999999999998 \t eps:  0.5563606588672675\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1221\n",
      "Episode Count:  576 \t Cumulative Reward:  31.39 \t eps:  0.5558042982084003\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4161\n",
      "Episode Count:  577 \t Cumulative Reward:  50.525000000000006 \t eps:  0.5552484939101918\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2203\n",
      "Episode Count:  578 \t Cumulative Reward:  20.008000000000003 \t eps:  0.5546932454162816\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3503\n",
      "Episode Count:  579 \t Cumulative Reward:  40.227999999999994 \t eps:  0.5541385521708654\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9458\n",
      "Episode Count:  580 \t Cumulative Reward:  22.928 \t eps:  0.5535844136186945\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9495\n",
      "Episode Count:  581 \t Cumulative Reward:  53.145 \t eps:  0.5530308292050757\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3120\n",
      "Episode Count:  582 \t Cumulative Reward:  16.483 \t eps:  0.5524777983758706\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4835\n",
      "Episode Count:  583 \t Cumulative Reward:  6.949000000000001 \t eps:  0.5519253205774948\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4736\n",
      "Episode Count:  584 \t Cumulative Reward:  31.860999999999994 \t eps:  0.5513733952569173\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2119\n",
      "Episode Count:  585 \t Cumulative Reward:  17.174999999999997 \t eps:  0.5508220218616604\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6168\n",
      "Episode Count:  586 \t Cumulative Reward:  43.359 \t eps:  0.5502711998397988\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.7437\n",
      "Episode Count:  587 \t Cumulative Reward:  50.687999999999995 \t eps:  0.549720928639959\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0655\n",
      "Episode Count:  588 \t Cumulative Reward:  9.776 \t eps:  0.5491712077113191\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0276\n",
      "Episode Count:  589 \t Cumulative Reward:  18.929000000000002 \t eps:  0.5486220365036077\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3785\n",
      "Episode Count:  590 \t Cumulative Reward:  8.020999999999999 \t eps:  0.5480734144671041\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3519\n",
      "Episode Count:  591 \t Cumulative Reward:  35.583 \t eps:  0.5475253410526371\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8041\n",
      "Episode Count:  592 \t Cumulative Reward:  13.014000000000001 \t eps:  0.5469778157115844\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9028\n",
      "Episode Count:  593 \t Cumulative Reward:  10.812 \t eps:  0.5464308378958729\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3512\n",
      "Episode Count:  594 \t Cumulative Reward:  10.586 \t eps:  0.545884407057977\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2245\n",
      "Episode Count:  595 \t Cumulative Reward:  29.84 \t eps:  0.5453385226509191\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.3721\n",
      "Episode Count:  596 \t Cumulative Reward:  40.332000000000015 \t eps:  0.5447931841282682\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2519\n",
      "Episode Count:  597 \t Cumulative Reward:  23.087000000000007 \t eps:  0.5442483909441399\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2022\n",
      "Episode Count:  598 \t Cumulative Reward:  33.734 \t eps:  0.5437041425531958\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.9942\n",
      "Episode Count:  599 \t Cumulative Reward:  20.098999999999997 \t eps:  0.5431604384106425\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3770\n",
      "Episode Count:  600 \t Cumulative Reward:  25.57 \t eps:  0.5426172779722319\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6423\n",
      "Episode Count:  601 \t Cumulative Reward:  22.28 \t eps:  0.5420746606942597\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8899\n",
      "Episode Count:  602 \t Cumulative Reward:  18.659 \t eps:  0.5415325860335655\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2466\n",
      "Episode Count:  603 \t Cumulative Reward:  16.665 \t eps:  0.540991053447532\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6446\n",
      "Episode Count:  604 \t Cumulative Reward:  12.262999999999998 \t eps:  0.5404500623940844\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6979\n",
      "Episode Count:  605 \t Cumulative Reward:  20.945000000000004 \t eps:  0.5399096123316903\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5448\n",
      "Episode Count:  606 \t Cumulative Reward:  10.268999999999997 \t eps:  0.5393697027193586\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6175\n",
      "Episode Count:  607 \t Cumulative Reward:  9.658 \t eps:  0.5388303330166392\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9665\n",
      "Episode Count:  608 \t Cumulative Reward:  11.945999999999998 \t eps:  0.5382915026836226\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3930\n",
      "Episode Count:  609 \t Cumulative Reward:  29.775000000000006 \t eps:  0.537753211180939\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0251\n",
      "Episode Count:  610 \t Cumulative Reward:  19.982 \t eps:  0.537215457969758\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0835\n",
      "Episode Count:  611 \t Cumulative Reward:  5.089999999999997 \t eps:  0.5366782425117882\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8353\n",
      "Episode Count:  612 \t Cumulative Reward:  16.192999999999998 \t eps:  0.5361415642692764\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0725\n",
      "Episode Count:  613 \t Cumulative Reward:  6.509999999999997 \t eps:  0.5356054227050071\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.7312\n",
      "Episode Count:  614 \t Cumulative Reward:  3.6380000000000035 \t eps:  0.5350698172823021\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7290\n",
      "Episode Count:  615 \t Cumulative Reward:  7.261000000000002 \t eps:  0.5345347474650197\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8570\n",
      "Episode Count:  616 \t Cumulative Reward:  6.717999999999999 \t eps:  0.5340002127175547\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0579\n",
      "Episode Count:  617 \t Cumulative Reward:  20.697000000000003 \t eps:  0.5334662125048372\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6698\n",
      "Episode Count:  618 \t Cumulative Reward:  4.5760000000000005 \t eps:  0.5329327462923323\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7912\n",
      "Episode Count:  619 \t Cumulative Reward:  -0.15000000000000283 \t eps:  0.53239981354604\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7015\n",
      "Episode Count:  620 \t Cumulative Reward:  5.166999999999999 \t eps:  0.531867413732494\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9890\n",
      "Episode Count:  621 \t Cumulative Reward:  8.853 \t eps:  0.5313355463187616\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8226\n",
      "Episode Count:  622 \t Cumulative Reward:  16.095 \t eps:  0.5308042107724428\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2990\n",
      "Episode Count:  623 \t Cumulative Reward:  12.173000000000002 \t eps:  0.5302734065616703\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0325\n",
      "Episode Count:  624 \t Cumulative Reward:  7.336999999999999 \t eps:  0.5297431331551087\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1334\n",
      "Episode Count:  625 \t Cumulative Reward:  22.223000000000003 \t eps:  0.5292133900219536\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.6623\n",
      "Episode Count:  626 \t Cumulative Reward:  5.096999999999998 \t eps:  0.5286841766319316\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5982\n",
      "Episode Count:  627 \t Cumulative Reward:  25.452 \t eps:  0.5281554924552997\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8583\n",
      "Episode Count:  628 \t Cumulative Reward:  10.292 \t eps:  0.5276273369628444\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8562\n",
      "Episode Count:  629 \t Cumulative Reward:  13.823000000000004 \t eps:  0.5270997096258815\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0739\n",
      "Episode Count:  630 \t Cumulative Reward:  25.816000000000003 \t eps:  0.5265726099162557\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1013\n",
      "Episode Count:  631 \t Cumulative Reward:  9.591999999999999 \t eps:  0.5260460373063394\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7431\n",
      "Episode Count:  632 \t Cumulative Reward:  9.401 \t eps:  0.525519991269033\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1297\n",
      "Episode Count:  633 \t Cumulative Reward:  19.987000000000002 \t eps:  0.524994471277764\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2216\n",
      "Episode Count:  634 \t Cumulative Reward:  15.476000000000004 \t eps:  0.5244694768064863\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0393\n",
      "Episode Count:  635 \t Cumulative Reward:  13.813 \t eps:  0.5239450073296797\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6456\n",
      "Episode Count:  636 \t Cumulative Reward:  15.896999999999998 \t eps:  0.5234210623223501\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8276\n",
      "Episode Count:  637 \t Cumulative Reward:  -0.04800000000000116 \t eps:  0.5228976412600277\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9208\n",
      "Episode Count:  638 \t Cumulative Reward:  -0.2819999999999989 \t eps:  0.5223747436187677\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7638\n",
      "Episode Count:  639 \t Cumulative Reward:  10.704000000000002 \t eps:  0.521852368875149\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6158\n",
      "Episode Count:  640 \t Cumulative Reward:  12.898999999999997 \t eps:  0.5213305165062738\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6160\n",
      "Episode Count:  641 \t Cumulative Reward:  11.925 \t eps:  0.5208091859897676\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9336\n",
      "Episode Count:  642 \t Cumulative Reward:  21.308 \t eps:  0.5202883768037778\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6276\n",
      "Episode Count:  643 \t Cumulative Reward:  3.0660000000000016 \t eps:  0.5197680884269741\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0699\n",
      "Episode Count:  644 \t Cumulative Reward:  30.464999999999996 \t eps:  0.5192483203385471\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8763\n",
      "Episode Count:  645 \t Cumulative Reward:  21.859000000000005 \t eps:  0.5187290720182085\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9413\n",
      "Episode Count:  646 \t Cumulative Reward:  38.929 \t eps:  0.5182103429461903\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0507\n",
      "Episode Count:  647 \t Cumulative Reward:  9.721 \t eps:  0.5176921326032441\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7750\n",
      "Episode Count:  648 \t Cumulative Reward:  5.718000000000001 \t eps:  0.5171744404706409\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6043\n",
      "Episode Count:  649 \t Cumulative Reward:  -0.07800000000000358 \t eps:  0.5166572660301703\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9329\n",
      "Episode Count:  650 \t Cumulative Reward:  -1.0050000000000003 \t eps:  0.5161406087641401\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5770\n",
      "Episode Count:  651 \t Cumulative Reward:  -0.1200000000000036 \t eps:  0.515624468155376\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2581\n",
      "Episode Count:  652 \t Cumulative Reward:  31.425 \t eps:  0.5151088436872207\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3681\n",
      "Episode Count:  653 \t Cumulative Reward:  33.431000000000004 \t eps:  0.5145937348435334\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9908\n",
      "Episode Count:  654 \t Cumulative Reward:  9.465999999999996 \t eps:  0.5140791411086899\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2489\n",
      "Episode Count:  655 \t Cumulative Reward:  16.592999999999996 \t eps:  0.5135650619675812\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8541\n",
      "Episode Count:  656 \t Cumulative Reward:  7.111 \t eps:  0.5130514969056137\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1206\n",
      "Episode Count:  657 \t Cumulative Reward:  2.2280000000000024 \t eps:  0.5125384454087081\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0235\n",
      "Episode Count:  658 \t Cumulative Reward:  -0.06899999999999859 \t eps:  0.5120259069632994\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6207\n",
      "Episode Count:  659 \t Cumulative Reward:  22.997 \t eps:  0.5115138810563361\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8356\n",
      "Episode Count:  660 \t Cumulative Reward:  16.336 \t eps:  0.5110023671752798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2192\n",
      "Episode Count:  661 \t Cumulative Reward:  5.442000000000001 \t eps:  0.5104913648081045\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9103\n",
      "Episode Count:  662 \t Cumulative Reward:  5.982 \t eps:  0.5099808734432965\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5125\n",
      "Episode Count:  663 \t Cumulative Reward:  13.099999999999998 \t eps:  0.5094708925698531\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4570\n",
      "Episode Count:  664 \t Cumulative Reward:  21.864000000000004 \t eps:  0.5089614216772833\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6111\n",
      "Episode Count:  665 \t Cumulative Reward:  22.252000000000002 \t eps:  0.508452460255606\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6910\n",
      "Episode Count:  666 \t Cumulative Reward:  13.364999999999998 \t eps:  0.5079440077953503\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5155\n",
      "Episode Count:  667 \t Cumulative Reward:  9.221999999999998 \t eps:  0.507436063787555\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0070\n",
      "Episode Count:  668 \t Cumulative Reward:  18.932000000000002 \t eps:  0.5069286277237675\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0152\n",
      "Episode Count:  669 \t Cumulative Reward:  16.439000000000004 \t eps:  0.5064216990960437\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6980\n",
      "Episode Count:  670 \t Cumulative Reward:  7.483 \t eps:  0.5059152773969476\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2735\n",
      "Episode Count:  671 \t Cumulative Reward:  13.852000000000004 \t eps:  0.5054093621195507\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9971\n",
      "Episode Count:  672 \t Cumulative Reward:  14.017999999999997 \t eps:  0.5049039527574312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1802\n",
      "Episode Count:  673 \t Cumulative Reward:  43.69199999999999 \t eps:  0.5043990488046737\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9894\n",
      "Episode Count:  674 \t Cumulative Reward:  16.657000000000004 \t eps:  0.503894649755869\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9212\n",
      "Episode Count:  675 \t Cumulative Reward:  13.871000000000006 \t eps:  0.5033907551061131\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.5878\n",
      "Episode Count:  676 \t Cumulative Reward:  13.992000000000003 \t eps:  0.502887364351007\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.7434\n",
      "Episode Count:  677 \t Cumulative Reward:  39.12500000000001 \t eps:  0.502384476986656\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0359\n",
      "Episode Count:  678 \t Cumulative Reward:  19.516 \t eps:  0.5018820925096693\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2420\n",
      "Episode Count:  679 \t Cumulative Reward:  -0.08000000000000014 \t eps:  0.5013802104171596\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9639\n",
      "Episode Count:  680 \t Cumulative Reward:  8.531 \t eps:  0.5008788302067425\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1258\n",
      "Episode Count:  681 \t Cumulative Reward:  59.33399999999999 \t eps:  0.5003779513765357\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0689\n",
      "Episode Count:  682 \t Cumulative Reward:  19.697000000000003 \t eps:  0.4998775734251592\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0193\n",
      "Episode Count:  683 \t Cumulative Reward:  59.789 \t eps:  0.499377695851734\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9463\n",
      "Episode Count:  684 \t Cumulative Reward:  20.641 \t eps:  0.4988783181558823\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8204\n",
      "Episode Count:  685 \t Cumulative Reward:  6.591999999999999 \t eps:  0.4983794398377264\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8897\n",
      "Episode Count:  686 \t Cumulative Reward:  32.257999999999996 \t eps:  0.4978810603978887\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8183\n",
      "Episode Count:  687 \t Cumulative Reward:  35.125000000000014 \t eps:  0.4973831793374908\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0104\n",
      "Episode Count:  688 \t Cumulative Reward:  11.059999999999999 \t eps:  0.4968857961581533\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3039\n",
      "Episode Count:  689 \t Cumulative Reward:  31.905000000000005 \t eps:  0.4963889103619952\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1110\n",
      "Episode Count:  690 \t Cumulative Reward:  14.880999999999998 \t eps:  0.49589252145163315\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0830\n",
      "Episode Count:  691 \t Cumulative Reward:  26.014999999999997 \t eps:  0.4953966289301815\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4595\n",
      "Episode Count:  692 \t Cumulative Reward:  32.282000000000004 \t eps:  0.4949012323012513\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4641\n",
      "Episode Count:  693 \t Cumulative Reward:  17.659 \t eps:  0.4944063310689501\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3273\n",
      "Episode Count:  694 \t Cumulative Reward:  41.85900000000001 \t eps:  0.49391192473788115\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1298\n",
      "Episode Count:  695 \t Cumulative Reward:  21.616999999999997 \t eps:  0.4934180128131433\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3918\n",
      "Episode Count:  696 \t Cumulative Reward:  3.2359999999999993 \t eps:  0.49292459480033013\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2562\n",
      "Episode Count:  697 \t Cumulative Reward:  0.09000000000000183 \t eps:  0.4924316702055298\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3327\n",
      "Episode Count:  698 \t Cumulative Reward:  -0.060000000000002815 \t eps:  0.4919392385353243\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5136\n",
      "Episode Count:  699 \t Cumulative Reward:  -15.165000000000001 \t eps:  0.491447299296789\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3318\n",
      "Episode Count:  700 \t Cumulative Reward:  0.5750000000000015 \t eps:  0.4909558519974922\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4407\n",
      "Episode Count:  701 \t Cumulative Reward:  0.4499999999999938 \t eps:  0.4904648961454947\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5545\n",
      "Episode Count:  702 \t Cumulative Reward:  19.008 \t eps:  0.4899744312493492\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5939\n",
      "Episode Count:  703 \t Cumulative Reward:  25.56800000000001 \t eps:  0.4894844568180998\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7918\n",
      "Episode Count:  704 \t Cumulative Reward:  29.092 \t eps:  0.4889949723612817\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8664\n",
      "Episode Count:  705 \t Cumulative Reward:  23.028 \t eps:  0.48850597738892043\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2491\n",
      "Episode Count:  706 \t Cumulative Reward:  10.985000000000001 \t eps:  0.4880174714115315\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3794\n",
      "Episode Count:  707 \t Cumulative Reward:  31.925 \t eps:  0.48752945394012\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3268\n",
      "Episode Count:  708 \t Cumulative Reward:  5.420999999999999 \t eps:  0.4870419244861799\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4142\n",
      "Episode Count:  709 \t Cumulative Reward:  25.648 \t eps:  0.4865548825616937\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7984\n",
      "Episode Count:  710 \t Cumulative Reward:  15.977999999999994 \t eps:  0.486068327679132\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4447\n",
      "Episode Count:  711 \t Cumulative Reward:  33.77499999999999 \t eps:  0.4855822593514529\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3733\n",
      "Episode Count:  712 \t Cumulative Reward:  40.754 \t eps:  0.4850966770921014\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6743\n",
      "Episode Count:  713 \t Cumulative Reward:  25.105999999999998 \t eps:  0.4846115804150093\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2883\n",
      "Episode Count:  714 \t Cumulative Reward:  8.878000000000004 \t eps:  0.4841269688345943\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8243\n",
      "Episode Count:  715 \t Cumulative Reward:  9.026999999999997 \t eps:  0.4836428418657597\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6272\n",
      "Episode Count:  716 \t Cumulative Reward:  15.728 \t eps:  0.4831591990238939\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3827\n",
      "Episode Count:  717 \t Cumulative Reward:  6.008999999999999 \t eps:  0.48267603982487\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4923\n",
      "Episode Count:  718 \t Cumulative Reward:  -0.1189999999999985 \t eps:  0.4821933637850451\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2003\n",
      "Episode Count:  719 \t Cumulative Reward:  11.524000000000001 \t eps:  0.4817111704212601\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8561\n",
      "Episode Count:  720 \t Cumulative Reward:  -0.20399999999999882 \t eps:  0.4812294592508388\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2073\n",
      "Episode Count:  721 \t Cumulative Reward:  16.768 \t eps:  0.480748229791588\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9100\n",
      "Episode Count:  722 \t Cumulative Reward:  27.247 \t eps:  0.4802674815617964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8029\n",
      "Episode Count:  723 \t Cumulative Reward:  14.044999999999995 \t eps:  0.4797872140802346\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4675\n",
      "Episode Count:  724 \t Cumulative Reward:  25.573 \t eps:  0.4793074268661544\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7534\n",
      "Episode Count:  725 \t Cumulative Reward:  7.6659999999999995 \t eps:  0.47882811943928827\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8644\n",
      "Episode Count:  726 \t Cumulative Reward:  10.004999999999999 \t eps:  0.478349291319849\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3336\n",
      "Episode Count:  727 \t Cumulative Reward:  17.653 \t eps:  0.47787094202852914\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6105\n",
      "Episode Count:  728 \t Cumulative Reward:  24.452 \t eps:  0.47739307108650064\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3407\n",
      "Episode Count:  729 \t Cumulative Reward:  2.6810000000000027 \t eps:  0.4769156780154141\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8479\n",
      "Episode Count:  730 \t Cumulative Reward:  -0.04200000000000001 \t eps:  0.47643876233739874\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0865\n",
      "Episode Count:  731 \t Cumulative Reward:  17.809000000000005 \t eps:  0.47596232357506135\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2585\n",
      "Episode Count:  732 \t Cumulative Reward:  21.697000000000003 \t eps:  0.4754863612514863\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3852\n",
      "Episode Count:  733 \t Cumulative Reward:  15.443999999999999 \t eps:  0.4750108748902348\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1088\n",
      "Episode Count:  734 \t Cumulative Reward:  14.020999999999999 \t eps:  0.4745358640153446\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3871\n",
      "Episode Count:  735 \t Cumulative Reward:  30.875999999999998 \t eps:  0.47406132815132923\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1468\n",
      "Episode Count:  736 \t Cumulative Reward:  0.969 \t eps:  0.4735872668231779\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6607\n",
      "Episode Count:  737 \t Cumulative Reward:  -0.26399999999999924 \t eps:  0.47311367955635475\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1638\n",
      "Episode Count:  738 \t Cumulative Reward:  0.30800000000000055 \t eps:  0.47264056587679837\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3408\n",
      "Episode Count:  739 \t Cumulative Reward:  7.983000000000001 \t eps:  0.47216792531092155\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4148\n",
      "Episode Count:  740 \t Cumulative Reward:  10.339999999999998 \t eps:  0.47169575738561065\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6766\n",
      "Episode Count:  741 \t Cumulative Reward:  -0.17999999999999924 \t eps:  0.47122406162822505\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4795\n",
      "Episode Count:  742 \t Cumulative Reward:  -0.0389999999999954 \t eps:  0.4707528375665968\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9442\n",
      "Episode Count:  743 \t Cumulative Reward:  8.908000000000003 \t eps:  0.4702820847290302\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8431\n",
      "Episode Count:  744 \t Cumulative Reward:  5.006 \t eps:  0.46981180264430117\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6187\n",
      "Episode Count:  745 \t Cumulative Reward:  7.170000000000001 \t eps:  0.46934199084165684\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5536\n",
      "Episode Count:  746 \t Cumulative Reward:  10.499000000000004 \t eps:  0.4688726488508152\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8437\n",
      "Episode Count:  747 \t Cumulative Reward:  -0.024000000000003588 \t eps:  0.4684037762019644\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3000\n",
      "Episode Count:  748 \t Cumulative Reward:  -12.201000000000002 \t eps:  0.46793537242576244\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3271\n",
      "Episode Count:  749 \t Cumulative Reward:  8.496 \t eps:  0.46746743705333665\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1662\n",
      "Episode Count:  750 \t Cumulative Reward:  -0.29300000000000054 \t eps:  0.4669999696162833\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7150\n",
      "Episode Count:  751 \t Cumulative Reward:  -0.011000000000000003 \t eps:  0.466532969646667\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8572\n",
      "Episode Count:  752 \t Cumulative Reward:  -0.07999999999999897 \t eps:  0.46606643667702036\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3965\n",
      "Episode Count:  753 \t Cumulative Reward:  -0.1140000000000036 \t eps:  0.46560037024034334\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7719\n",
      "Episode Count:  754 \t Cumulative Reward:  14.553999999999998 \t eps:  0.465134769870103\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7882\n",
      "Episode Count:  755 \t Cumulative Reward:  17.188 \t eps:  0.4646696351002329\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8400\n",
      "Episode Count:  756 \t Cumulative Reward:  3.9729999999999994 \t eps:  0.4642049654651327\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7949\n",
      "Episode Count:  757 \t Cumulative Reward:  17.046999999999997 \t eps:  0.46374076049966756\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1009\n",
      "Episode Count:  758 \t Cumulative Reward:  15.29 \t eps:  0.4632770197391679\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2195\n",
      "Episode Count:  759 \t Cumulative Reward:  20.811999999999998 \t eps:  0.4628137427194287\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5403\n",
      "Episode Count:  760 \t Cumulative Reward:  22.463 \t eps:  0.46235092897670926\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8246\n",
      "Episode Count:  761 \t Cumulative Reward:  41.012 \t eps:  0.46188857804773253\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1076\n",
      "Episode Count:  762 \t Cumulative Reward:  23.029 \t eps:  0.4614266894696848\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0705\n",
      "Episode Count:  763 \t Cumulative Reward:  49.632000000000005 \t eps:  0.4609652627802151\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4792\n",
      "Episode Count:  764 \t Cumulative Reward:  17.964 \t eps:  0.4605042975174349\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9164\n",
      "Episode Count:  765 \t Cumulative Reward:  14.354 \t eps:  0.46004379321991745\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2596\n",
      "Episode Count:  766 \t Cumulative Reward:  41.949 \t eps:  0.45958374942669755\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3127\n",
      "Episode Count:  767 \t Cumulative Reward:  34.211000000000006 \t eps:  0.4591241656772709\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7302\n",
      "Episode Count:  768 \t Cumulative Reward:  21.79 \t eps:  0.4586650415115936\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3212\n",
      "Episode Count:  769 \t Cumulative Reward:  19.061 \t eps:  0.458206376470082\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8793\n",
      "Episode Count:  770 \t Cumulative Reward:  18.248000000000008 \t eps:  0.4577481700936119\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0219\n",
      "Episode Count:  771 \t Cumulative Reward:  8.110000000000001 \t eps:  0.4572904219235183\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2396\n",
      "Episode Count:  772 \t Cumulative Reward:  37.52 \t eps:  0.4568331315015948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2253\n",
      "Episode Count:  773 \t Cumulative Reward:  6.627999999999999 \t eps:  0.45637629837009325\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5345\n",
      "Episode Count:  774 \t Cumulative Reward:  10.15 \t eps:  0.45591992207172316\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1540\n",
      "Episode Count:  775 \t Cumulative Reward:  16.55 \t eps:  0.45546400214965144\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1800\n",
      "Episode Count:  776 \t Cumulative Reward:  26.714999999999996 \t eps:  0.4550085381475018\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9973\n",
      "Episode Count:  777 \t Cumulative Reward:  7.732000000000003 \t eps:  0.4545535296093543\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8905\n",
      "Episode Count:  778 \t Cumulative Reward:  18.488000000000003 \t eps:  0.4540989760797449\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0813\n",
      "Episode Count:  779 \t Cumulative Reward:  11.169 \t eps:  0.4536448771036652\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2792\n",
      "Episode Count:  780 \t Cumulative Reward:  30.73200000000001 \t eps:  0.45319123222656155\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9549\n",
      "Episode Count:  781 \t Cumulative Reward:  20.887 \t eps:  0.452738040994335\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2501\n",
      "Episode Count:  782 \t Cumulative Reward:  16.674999999999997 \t eps:  0.4522853029533407\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7941\n",
      "Episode Count:  783 \t Cumulative Reward:  12.944 \t eps:  0.45183301765038736\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2311\n",
      "Episode Count:  784 \t Cumulative Reward:  14.392000000000001 \t eps:  0.45138118463273696\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0324\n",
      "Episode Count:  785 \t Cumulative Reward:  20.873 \t eps:  0.45092980344810424\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6879\n",
      "Episode Count:  786 \t Cumulative Reward:  27.684 \t eps:  0.4504788736446561\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8910\n",
      "Episode Count:  787 \t Cumulative Reward:  13.570000000000002 \t eps:  0.4500283947710115\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5023\n",
      "Episode Count:  788 \t Cumulative Reward:  22.064000000000004 \t eps:  0.4495783663762405\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8646\n",
      "Episode Count:  789 \t Cumulative Reward:  17.008 \t eps:  0.4491287880098643\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3383\n",
      "Episode Count:  790 \t Cumulative Reward:  10.387 \t eps:  0.44867965922185443\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9734\n",
      "Episode Count:  791 \t Cumulative Reward:  8.996999999999998 \t eps:  0.4482309795626326\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5050\n",
      "Episode Count:  792 \t Cumulative Reward:  4.889999999999999 \t eps:  0.44778274858306993\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3755\n",
      "Episode Count:  793 \t Cumulative Reward:  7.612000000000001 \t eps:  0.44733496583448684\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7056\n",
      "Episode Count:  794 \t Cumulative Reward:  8.532 \t eps:  0.44688763086865235\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0856\n",
      "Episode Count:  795 \t Cumulative Reward:  8.186000000000002 \t eps:  0.4464407432377837\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6087\n",
      "Episode Count:  796 \t Cumulative Reward:  30.03400000000001 \t eps:  0.4459943024945459\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1811\n",
      "Episode Count:  797 \t Cumulative Reward:  25.158 \t eps:  0.4455483081920514\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1652\n",
      "Episode Count:  798 \t Cumulative Reward:  21.354000000000003 \t eps:  0.44510275988385933\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9334\n",
      "Episode Count:  799 \t Cumulative Reward:  45.891999999999996 \t eps:  0.4446576571239755\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4309\n",
      "Episode Count:  800 \t Cumulative Reward:  27.697000000000003 \t eps:  0.4442129994668515\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7930\n",
      "Episode Count:  801 \t Cumulative Reward:  16.392 \t eps:  0.44376878646738466\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2440\n",
      "Episode Count:  802 \t Cumulative Reward:  15.006 \t eps:  0.44332501768091725\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1407\n",
      "Episode Count:  803 \t Cumulative Reward:  40.495 \t eps:  0.44288169266323635\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3533\n",
      "Episode Count:  804 \t Cumulative Reward:  14.407000000000002 \t eps:  0.44243881097057314\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4393\n",
      "Episode Count:  805 \t Cumulative Reward:  17.584 \t eps:  0.4419963721596026\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4108\n",
      "Episode Count:  806 \t Cumulative Reward:  11.068000000000001 \t eps:  0.441554375787443\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1262\n",
      "Episode Count:  807 \t Cumulative Reward:  45.55200000000001 \t eps:  0.44111282141165553\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0014\n",
      "Episode Count:  808 \t Cumulative Reward:  9.621999999999996 \t eps:  0.44067170859024385\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3890\n",
      "Episode Count:  809 \t Cumulative Reward:  11.954999999999998 \t eps:  0.4402310368816536\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4450\n",
      "Episode Count:  810 \t Cumulative Reward:  14.276999999999997 \t eps:  0.4397908058447719\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4167\n",
      "Episode Count:  811 \t Cumulative Reward:  11.240999999999998 \t eps:  0.43935101503892715\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7938\n",
      "Episode Count:  812 \t Cumulative Reward:  16.692999999999998 \t eps:  0.4389116640238882\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6717\n",
      "Episode Count:  813 \t Cumulative Reward:  29.506000000000007 \t eps:  0.4384727523598643\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5367\n",
      "Episode Count:  814 \t Cumulative Reward:  19.048000000000002 \t eps:  0.43803427960750446\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1355\n",
      "Episode Count:  815 \t Cumulative Reward:  36.282 \t eps:  0.437596245327897\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2570\n",
      "Episode Count:  816 \t Cumulative Reward:  22.281000000000002 \t eps:  0.4371586490825691\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4562\n",
      "Episode Count:  817 \t Cumulative Reward:  25.164 \t eps:  0.4367214904334865\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5521\n",
      "Episode Count:  818 \t Cumulative Reward:  15.557000000000006 \t eps:  0.436284768943053\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8320\n",
      "Episode Count:  819 \t Cumulative Reward:  25.200999999999997 \t eps:  0.43584848417411\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2386\n",
      "Episode Count:  820 \t Cumulative Reward:  15.136999999999997 \t eps:  0.4354126356899359\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3608\n",
      "Episode Count:  821 \t Cumulative Reward:  47.87600000000001 \t eps:  0.43497722305424597\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5073\n",
      "Episode Count:  822 \t Cumulative Reward:  34.57300000000001 \t eps:  0.4345422458311917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1229\n",
      "Episode Count:  823 \t Cumulative Reward:  18.504999999999995 \t eps:  0.43410770358536055\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0489\n",
      "Episode Count:  824 \t Cumulative Reward:  20.590000000000007 \t eps:  0.4336735958817752\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2898\n",
      "Episode Count:  825 \t Cumulative Reward:  39.04600000000001 \t eps:  0.4332399222858934\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4488\n",
      "Episode Count:  826 \t Cumulative Reward:  22.202 \t eps:  0.43280668236360753\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4245\n",
      "Episode Count:  827 \t Cumulative Reward:  15.366999999999999 \t eps:  0.4323738756812439\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6301\n",
      "Episode Count:  828 \t Cumulative Reward:  26.138 \t eps:  0.4319415018055627\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1787\n",
      "Episode Count:  829 \t Cumulative Reward:  32.524 \t eps:  0.4315095603037571\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7038\n",
      "Episode Count:  830 \t Cumulative Reward:  37.232 \t eps:  0.43107805074345334\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9375\n",
      "Episode Count:  831 \t Cumulative Reward:  0.2670000000000189 \t eps:  0.4306469726927099\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0925\n",
      "Episode Count:  832 \t Cumulative Reward:  19.604999999999997 \t eps:  0.4302163257200172\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4499\n",
      "Episode Count:  833 \t Cumulative Reward:  15.547000000000002 \t eps:  0.4297861093942972\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4953\n",
      "Episode Count:  834 \t Cumulative Reward:  22.159 \t eps:  0.42935632328490286\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0075\n",
      "Episode Count:  835 \t Cumulative Reward:  11.101 \t eps:  0.428926966961618\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5822\n",
      "Episode Count:  836 \t Cumulative Reward:  16.236 \t eps:  0.42849803999465635\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9353\n",
      "Episode Count:  837 \t Cumulative Reward:  18.096999999999994 \t eps:  0.4280695419546617\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8989\n",
      "Episode Count:  838 \t Cumulative Reward:  20.683999999999997 \t eps:  0.427641472412707\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3021\n",
      "Episode Count:  839 \t Cumulative Reward:  9.756999999999998 \t eps:  0.4272138309402943\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1291\n",
      "Episode Count:  840 \t Cumulative Reward:  12.05 \t eps:  0.426786617109354\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6760\n",
      "Episode Count:  841 \t Cumulative Reward:  17.857 \t eps:  0.42635983049224463\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7454\n",
      "Episode Count:  842 \t Cumulative Reward:  5.250000000000002 \t eps:  0.4259334706617524\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8069\n",
      "Episode Count:  843 \t Cumulative Reward:  1.952999999999997 \t eps:  0.42550753719109063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 7ms/step - loss: 3.4313\n",
      "Episode Count:  844 \t Cumulative Reward:  16.462 \t eps:  0.4250820296538995\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6139\n",
      "Episode Count:  845 \t Cumulative Reward:  13.393999999999998 \t eps:  0.4246569476242456\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1121\n",
      "Episode Count:  846 \t Cumulative Reward:  10.579999999999997 \t eps:  0.4242322906766214\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7510\n",
      "Episode Count:  847 \t Cumulative Reward:  14.446 \t eps:  0.42380805838594476\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8672\n",
      "Episode Count:  848 \t Cumulative Reward:  19.542 \t eps:  0.42338425032755883\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4582\n",
      "Episode Count:  849 \t Cumulative Reward:  12.200000000000003 \t eps:  0.42296086607723127\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1117\n",
      "Episode Count:  850 \t Cumulative Reward:  9.517 \t eps:  0.42253790521115403\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9432\n",
      "Episode Count:  851 \t Cumulative Reward:  21.251999999999995 \t eps:  0.4221153673059429\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2526\n",
      "Episode Count:  852 \t Cumulative Reward:  1.7890000000000001 \t eps:  0.42169325193863694\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9058\n",
      "Episode Count:  853 \t Cumulative Reward:  5.5329999999999995 \t eps:  0.4212715586866983\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9409\n",
      "Episode Count:  854 \t Cumulative Reward:  10.656 \t eps:  0.4208502871280116\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0908\n",
      "Episode Count:  855 \t Cumulative Reward:  7.362000000000001 \t eps:  0.4204294368408836\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2335\n",
      "Episode Count:  856 \t Cumulative Reward:  49.947 \t eps:  0.4200090074040427\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5059\n",
      "Episode Count:  857 \t Cumulative Reward:  16.837999999999997 \t eps:  0.41958899839663866\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2030\n",
      "Episode Count:  858 \t Cumulative Reward:  15.852 \t eps:  0.41916940939824204\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4028\n",
      "Episode Count:  859 \t Cumulative Reward:  5.394999999999999 \t eps:  0.4187502399888438\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8662\n",
      "Episode Count:  860 \t Cumulative Reward:  6.908 \t eps:  0.418331489748855\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0005\n",
      "Episode Count:  861 \t Cumulative Reward:  13.864 \t eps:  0.41791315825910613\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3743\n",
      "Episode Count:  862 \t Cumulative Reward:  11.949000000000002 \t eps:  0.417495245100847\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2431\n",
      "Episode Count:  863 \t Cumulative Reward:  7.966000000000002 \t eps:  0.41707774985574614\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3472\n",
      "Episode Count:  864 \t Cumulative Reward:  55.684 \t eps:  0.4166606721058904\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5095\n",
      "Episode Count:  865 \t Cumulative Reward:  6.563000000000001 \t eps:  0.41624401143378453\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5086\n",
      "Episode Count:  866 \t Cumulative Reward:  6.7559999999999985 \t eps:  0.4158277674223507\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3159\n",
      "Episode Count:  867 \t Cumulative Reward:  14.161999999999999 \t eps:  0.4154119396549284\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7280\n",
      "Episode Count:  868 \t Cumulative Reward:  22.761 \t eps:  0.41499652771527346\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2359\n",
      "Episode Count:  869 \t Cumulative Reward:  10.690000000000001 \t eps:  0.4145815311875582\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6006\n",
      "Episode Count:  870 \t Cumulative Reward:  28.458 \t eps:  0.41416694965637063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1461\n",
      "Episode Count:  871 \t Cumulative Reward:  8.123000000000001 \t eps:  0.4137527827067143\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5417\n",
      "Episode Count:  872 \t Cumulative Reward:  16.712000000000007 \t eps:  0.41333902992400756\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3069\n",
      "Episode Count:  873 \t Cumulative Reward:  37.136 \t eps:  0.41292569089408354\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2627\n",
      "Episode Count:  874 \t Cumulative Reward:  21.406999999999996 \t eps:  0.41251276520318947\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5596\n",
      "Episode Count:  875 \t Cumulative Reward:  38.08800000000001 \t eps:  0.41210025243798626\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2092\n",
      "Episode Count:  876 \t Cumulative Reward:  21.832 \t eps:  0.4116881521855483\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0945\n",
      "Episode Count:  877 \t Cumulative Reward:  31.421 \t eps:  0.4112764640333627\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8152\n",
      "Episode Count:  878 \t Cumulative Reward:  14.323999999999998 \t eps:  0.4108651875693293\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2215\n",
      "Episode Count:  879 \t Cumulative Reward:  26.699 \t eps:  0.41045432238176\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0173\n",
      "Episode Count:  880 \t Cumulative Reward:  15.716999999999999 \t eps:  0.41004386805937826\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2597\n",
      "Episode Count:  881 \t Cumulative Reward:  7.331999999999999 \t eps:  0.4096338241913189\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2493\n",
      "Episode Count:  882 \t Cumulative Reward:  13.113 \t eps:  0.40922419036712754\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0146\n",
      "Episode Count:  883 \t Cumulative Reward:  39.05499999999999 \t eps:  0.4088149661767604\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2385\n",
      "Episode Count:  884 \t Cumulative Reward:  19.030999999999995 \t eps:  0.4084061512105836\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9645\n",
      "Episode Count:  885 \t Cumulative Reward:  33.703 \t eps:  0.40799774505937303\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5233\n",
      "Episode Count:  886 \t Cumulative Reward:  10.959000000000001 \t eps:  0.40758974731431363\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3787\n",
      "Episode Count:  887 \t Cumulative Reward:  22.135 \t eps:  0.4071821575669993\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5457\n",
      "Episode Count:  888 \t Cumulative Reward:  0.3289999999999994 \t eps:  0.4067749754094323\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0548\n",
      "Episode Count:  889 \t Cumulative Reward:  12.97 \t eps:  0.4063682004340229\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7362\n",
      "Episode Count:  890 \t Cumulative Reward:  28.301000000000002 \t eps:  0.40596183223358884\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6334\n",
      "Episode Count:  891 \t Cumulative Reward:  8.948000000000002 \t eps:  0.40555587040135527\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1625\n",
      "Episode Count:  892 \t Cumulative Reward:  17.758999999999997 \t eps:  0.4051503145309539\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3178\n",
      "Episode Count:  893 \t Cumulative Reward:  39.164 \t eps:  0.4047451642164229\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4979\n",
      "Episode Count:  894 \t Cumulative Reward:  42.449000000000005 \t eps:  0.4043404190522065\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5383\n",
      "Episode Count:  895 \t Cumulative Reward:  55.35500000000001 \t eps:  0.40393607863315434\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1115\n",
      "Episode Count:  896 \t Cumulative Reward:  12.742999999999999 \t eps:  0.4035321425545212\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1461\n",
      "Episode Count:  897 \t Cumulative Reward:  6.219 \t eps:  0.4031286104119667\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1632\n",
      "Episode Count:  898 \t Cumulative Reward:  23.943 \t eps:  0.4027254818015547\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5627\n",
      "Episode Count:  899 \t Cumulative Reward:  10.98 \t eps:  0.40232275631975317\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3197\n",
      "Episode Count:  900 \t Cumulative Reward:  12.968999999999998 \t eps:  0.40192043356343343\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.1644\n",
      "Episode Count:  901 \t Cumulative Reward:  11.292 \t eps:  0.40151851312987\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6081\n",
      "Episode Count:  902 \t Cumulative Reward:  4.4990000000000006 \t eps:  0.40111699461674016\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2486\n",
      "Episode Count:  903 \t Cumulative Reward:  5.479000000000002 \t eps:  0.4007158776221234\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9391\n",
      "Episode Count:  904 \t Cumulative Reward:  10.734999999999998 \t eps:  0.40031516174450127\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4097\n",
      "Episode Count:  905 \t Cumulative Reward:  17.384 \t eps:  0.3999148465827568\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5921\n",
      "Episode Count:  906 \t Cumulative Reward:  8.245000000000003 \t eps:  0.39951493173617403\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3411\n",
      "Episode Count:  907 \t Cumulative Reward:  35.843 \t eps:  0.39911541680443785\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3859\n",
      "Episode Count:  908 \t Cumulative Reward:  8.885000000000005 \t eps:  0.3987163013876334\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7468\n",
      "Episode Count:  909 \t Cumulative Reward:  16.758 \t eps:  0.39831758508624576\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1371\n",
      "Episode Count:  910 \t Cumulative Reward:  15.904 \t eps:  0.3979192675011595\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0053\n",
      "Episode Count:  911 \t Cumulative Reward:  16.31 \t eps:  0.39752134823365837\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7780\n",
      "Episode Count:  912 \t Cumulative Reward:  23.673000000000002 \t eps:  0.3971238268854247\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3531\n",
      "Episode Count:  913 \t Cumulative Reward:  12.410999999999996 \t eps:  0.39672670305853924\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6510\n",
      "Episode Count:  914 \t Cumulative Reward:  9.968 \t eps:  0.39632997635548073\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5390\n",
      "Episode Count:  915 \t Cumulative Reward:  51.08700000000001 \t eps:  0.39593364637912526\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8523\n",
      "Episode Count:  916 \t Cumulative Reward:  13.586 \t eps:  0.3955377127327461\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8509\n",
      "Episode Count:  917 \t Cumulative Reward:  16.070999999999998 \t eps:  0.39514217502001336\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3091\n",
      "Episode Count:  918 \t Cumulative Reward:  6.733000000000001 \t eps:  0.39474703284499335\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0086\n",
      "Episode Count:  919 \t Cumulative Reward:  8.126000000000001 \t eps:  0.3943522858121484\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4979\n",
      "Episode Count:  920 \t Cumulative Reward:  13.096000000000004 \t eps:  0.39395793352633623\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1858\n",
      "Episode Count:  921 \t Cumulative Reward:  14.997000000000003 \t eps:  0.3935639755928099\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4269\n",
      "Episode Count:  922 \t Cumulative Reward:  17.582000000000004 \t eps:  0.3931704116172171\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2343\n",
      "Episode Count:  923 \t Cumulative Reward:  20.085000000000004 \t eps:  0.39277724120559987\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8907\n",
      "Episode Count:  924 \t Cumulative Reward:  15.939000000000004 \t eps:  0.39238446396439425\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7768\n",
      "Episode Count:  925 \t Cumulative Reward:  50.346000000000004 \t eps:  0.39199207950042986\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8853\n",
      "Episode Count:  926 \t Cumulative Reward:  9.330999999999998 \t eps:  0.39160008742092944\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2316\n",
      "Episode Count:  927 \t Cumulative Reward:  19.455 \t eps:  0.3912084873335085\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9126\n",
      "Episode Count:  928 \t Cumulative Reward:  7.897000000000004 \t eps:  0.390817278846175\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8865\n",
      "Episode Count:  929 \t Cumulative Reward:  5.37300000000007 \t eps:  0.39042646156732885\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3306\n",
      "Episode Count:  930 \t Cumulative Reward:  6.369000000000001 \t eps:  0.3900360351057615\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5872\n",
      "Episode Count:  931 \t Cumulative Reward:  10.034 \t eps:  0.3896459990706558\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6417\n",
      "Episode Count:  932 \t Cumulative Reward:  10.95 \t eps:  0.3892563530715851\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2046\n",
      "Episode Count:  933 \t Cumulative Reward:  0.2790000000000029 \t eps:  0.38886709671851355\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4154\n",
      "Episode Count:  934 \t Cumulative Reward:  -0.11699999999999541 \t eps:  0.388478229621795\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9124\n",
      "Episode Count:  935 \t Cumulative Reward:  9.200999999999997 \t eps:  0.3880897513921732\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3607\n",
      "Episode Count:  936 \t Cumulative Reward:  -0.14999999999999641 \t eps:  0.38770166164078107\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5932\n",
      "Episode Count:  937 \t Cumulative Reward:  3.997999999999999 \t eps:  0.3873139599791403\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6394\n",
      "Episode Count:  938 \t Cumulative Reward:  -0.02399999999999719 \t eps:  0.38692664601916116\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8060\n",
      "Episode Count:  939 \t Cumulative Reward:  4.047999999999999 \t eps:  0.386539719373142\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1677\n",
      "Episode Count:  940 \t Cumulative Reward:  -0.045000000000004606 \t eps:  0.3861531796537689\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7995\n",
      "Episode Count:  941 \t Cumulative Reward:  6.123999999999999 \t eps:  0.3857670264741151\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3772\n",
      "Episode Count:  942 \t Cumulative Reward:  9.421999999999999 \t eps:  0.38538125944764096\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8273\n",
      "Episode Count:  943 \t Cumulative Reward:  -0.15599999999999925 \t eps:  0.3849958781881933\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3030\n",
      "Episode Count:  944 \t Cumulative Reward:  11.332 \t eps:  0.3846108823100051\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3288\n",
      "Episode Count:  945 \t Cumulative Reward:  6.497 \t eps:  0.3842262714276951\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4155\n",
      "Episode Count:  946 \t Cumulative Reward:  0.4099999999999984 \t eps:  0.3838420451562674\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9231\n",
      "Episode Count:  947 \t Cumulative Reward:  61.505 \t eps:  0.3834582031111111\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7629\n",
      "Episode Count:  948 \t Cumulative Reward:  4.397999999999997 \t eps:  0.38307474490800003\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1864\n",
      "Episode Count:  949 \t Cumulative Reward:  15.930000000000001 \t eps:  0.382691670163092\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0581\n",
      "Episode Count:  950 \t Cumulative Reward:  20.938 \t eps:  0.38230897849292894\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6945\n",
      "Episode Count:  951 \t Cumulative Reward:  36.900999999999996 \t eps:  0.381926669514436\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7349\n",
      "Episode Count:  952 \t Cumulative Reward:  38.393 \t eps:  0.38154474284492157\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7980\n",
      "Episode Count:  953 \t Cumulative Reward:  5.583 \t eps:  0.38116319810207666\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6901\n",
      "Episode Count:  954 \t Cumulative Reward:  8.727000000000002 \t eps:  0.3807820349039746\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4106\n",
      "Episode Count:  955 \t Cumulative Reward:  6.894999999999999 \t eps:  0.3804012528690706\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.7976\n",
      "Episode Count:  956 \t Cumulative Reward:  7.850000000000003 \t eps:  0.3800208516162015\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1606\n",
      "Episode Count:  957 \t Cumulative Reward:  45.657 \t eps:  0.3796408307645853\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6395\n",
      "Episode Count:  958 \t Cumulative Reward:  23.767000000000003 \t eps:  0.37926118993382074\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0124\n",
      "Episode Count:  959 \t Cumulative Reward:  14.935000000000002 \t eps:  0.37888192874388693\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9106\n",
      "Episode Count:  960 \t Cumulative Reward:  32.171 \t eps:  0.378503046815143\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8279\n",
      "Episode Count:  961 \t Cumulative Reward:  30.666999999999998 \t eps:  0.3781245437683279\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8327\n",
      "Episode Count:  962 \t Cumulative Reward:  0.8179999999999998 \t eps:  0.37774641922455954\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5458\n",
      "Episode Count:  963 \t Cumulative Reward:  -0.08099999999999898 \t eps:  0.377368672805335\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0356\n",
      "Episode Count:  964 \t Cumulative Reward:  14.847999999999999 \t eps:  0.3769913041325296\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7644\n",
      "Episode Count:  965 \t Cumulative Reward:  29.330999999999996 \t eps:  0.3766143128283971\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8834\n",
      "Episode Count:  966 \t Cumulative Reward:  10.332 \t eps:  0.3762376985155687\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3841\n",
      "Episode Count:  967 \t Cumulative Reward:  18.555000000000003 \t eps:  0.37586146081705313\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1885\n",
      "Episode Count:  968 \t Cumulative Reward:  19.467 \t eps:  0.3754855993562361\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4844\n",
      "Episode Count:  969 \t Cumulative Reward:  31.090000000000003 \t eps:  0.37511011375687986\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7419\n",
      "Episode Count:  970 \t Cumulative Reward:  10.720999999999998 \t eps:  0.37473500364312295\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0471\n",
      "Episode Count:  971 \t Cumulative Reward:  29.657000000000007 \t eps:  0.37436026863947985\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9163\n",
      "Episode Count:  972 \t Cumulative Reward:  19.687 \t eps:  0.3739859083708404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.5081\n",
      "Episode Count:  973 \t Cumulative Reward:  33.323 \t eps:  0.37361192246246955\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0295\n",
      "Episode Count:  974 \t Cumulative Reward:  10.578 \t eps:  0.37323831054000706\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.1355\n",
      "Episode Count:  975 \t Cumulative Reward:  22.558000000000003 \t eps:  0.3728650722294671\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8664\n",
      "Episode Count:  976 \t Cumulative Reward:  26.579 \t eps:  0.37249220715723763\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7310\n",
      "Episode Count:  977 \t Cumulative Reward:  8.777000000000005 \t eps:  0.3721197149500804\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6615\n",
      "Episode Count:  978 \t Cumulative Reward:  9.984000000000002 \t eps:  0.3717475952351303\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.2024\n",
      "Episode Count:  979 \t Cumulative Reward:  48.364000000000004 \t eps:  0.3713758476398952\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9407\n",
      "Episode Count:  980 \t Cumulative Reward:  96.262 \t eps:  0.37100447179225526\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8902\n",
      "Episode Count:  981 \t Cumulative Reward:  22.321 \t eps:  0.370633467320463\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0998\n",
      "Episode Count:  982 \t Cumulative Reward:  35.533 \t eps:  0.3702628338531425\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6524\n",
      "Episode Count:  983 \t Cumulative Reward:  42.998000000000005 \t eps:  0.36989257101928935\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0947\n",
      "Episode Count:  984 \t Cumulative Reward:  12.930000000000005 \t eps:  0.3695226784482701\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5957\n",
      "Episode Count:  985 \t Cumulative Reward:  61.315 \t eps:  0.36915315576982183\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.2146\n",
      "Episode Count:  986 \t Cumulative Reward:  16.474 \t eps:  0.36878400261405203\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8532\n",
      "Episode Count:  987 \t Cumulative Reward:  24.120000000000005 \t eps:  0.368415218611438\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7818\n",
      "Episode Count:  988 \t Cumulative Reward:  8.581999999999999 \t eps:  0.3680468033928266\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7567\n",
      "Episode Count:  989 \t Cumulative Reward:  19.525000000000002 \t eps:  0.3676787565894338\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1376\n",
      "Episode Count:  990 \t Cumulative Reward:  17.613999999999997 \t eps:  0.36731107783284433\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0002\n",
      "Episode Count:  991 \t Cumulative Reward:  13.873999999999997 \t eps:  0.3669437667550115\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4842\n",
      "Episode Count:  992 \t Cumulative Reward:  6.205000000000001 \t eps:  0.3665768229882565\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6952\n",
      "Episode Count:  993 \t Cumulative Reward:  7.0329999999999995 \t eps:  0.36621024616526826\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2023\n",
      "Episode Count:  994 \t Cumulative Reward:  16.673999999999996 \t eps:  0.365844035919103\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1711\n",
      "Episode Count:  995 \t Cumulative Reward:  21.878000000000004 \t eps:  0.36547819188318387\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6351\n",
      "Episode Count:  996 \t Cumulative Reward:  10.545000000000002 \t eps:  0.36511271369130066\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9191\n",
      "Episode Count:  997 \t Cumulative Reward:  21.955999999999996 \t eps:  0.3647476009776094\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5888\n",
      "Episode Count:  998 \t Cumulative Reward:  14.594999999999999 \t eps:  0.3643828533766318\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9362\n",
      "Episode Count:  999 \t Cumulative Reward:  30.275999999999993 \t eps:  0.36401847052325514\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.9440\n",
      "Episode Count:  1000 \t Cumulative Reward:  9.668999999999999 \t eps:  0.3636544520527319\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.2091\n",
      "Episode Count:  1001 \t Cumulative Reward:  9.472 \t eps:  0.36329079760067917\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.8745\n",
      "Episode Count:  1002 \t Cumulative Reward:  23.634 \t eps:  0.3629275068030785\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1817\n",
      "Episode Count:  1003 \t Cumulative Reward:  2.4909999999999997 \t eps:  0.36256457929627545\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.7830\n",
      "Episode Count:  1004 \t Cumulative Reward:  17.066000000000003 \t eps:  0.36220201471697916\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8278\n",
      "Episode Count:  1005 \t Cumulative Reward:  1.7740000000000005 \t eps:  0.3618398127022622\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.6223\n",
      "Episode Count:  1006 \t Cumulative Reward:  6.487999999999999 \t eps:  0.3614779728895599\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.8902\n",
      "Episode Count:  1007 \t Cumulative Reward:  8.223999999999998 \t eps:  0.36111649491667036\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.9413\n",
      "Episode Count:  1008 \t Cumulative Reward:  21.535999999999998 \t eps:  0.3607553784217537\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1621\n",
      "Episode Count:  1009 \t Cumulative Reward:  20.328000000000003 \t eps:  0.360394623043332\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3208\n",
      "Episode Count:  1010 \t Cumulative Reward:  0.5170000000000003 \t eps:  0.36003422842028865\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3174\n",
      "Episode Count:  1011 \t Cumulative Reward:  -11.753 \t eps:  0.35967419419186836\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2706\n",
      "Episode Count:  1012 \t Cumulative Reward:  11.487000000000004 \t eps:  0.3593145199976765\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4612\n",
      "Episode Count:  1013 \t Cumulative Reward:  8.249 \t eps:  0.35895520547767884\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.7752\n",
      "Episode Count:  1014 \t Cumulative Reward:  7.253000000000002 \t eps:  0.35859625027220116\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2734\n",
      "Episode Count:  1015 \t Cumulative Reward:  -0.14099999999999963 \t eps:  0.358237654021929\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2173\n",
      "Episode Count:  1016 \t Cumulative Reward:  -0.22500000000000003 \t eps:  0.35787941636790704\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4577\n",
      "Episode Count:  1017 \t Cumulative Reward:  -0.20700000000000066 \t eps:  0.35752153695153915\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1907\n",
      "Episode Count:  1018 \t Cumulative Reward:  6.075000000000001 \t eps:  0.3571640154145876\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.8965\n",
      "Episode Count:  1019 \t Cumulative Reward:  9.952000000000005 \t eps:  0.356806851399173\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9936\n",
      "Episode Count:  1020 \t Cumulative Reward:  -0.15299999999999783 \t eps:  0.35645004454777385\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4010\n",
      "Episode Count:  1021 \t Cumulative Reward:  4.877 \t eps:  0.3560935945032261\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.5567\n",
      "Episode Count:  1022 \t Cumulative Reward:  -0.036000000000000004 \t eps:  0.35573750090872286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3456\n",
      "Episode Count:  1023 \t Cumulative Reward:  2.1580000000000004 \t eps:  0.35538176340781413\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2769\n",
      "Episode Count:  1024 \t Cumulative Reward:  11.027 \t eps:  0.3550263816444063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7768\n",
      "Episode Count:  1025 \t Cumulative Reward:  10.589999999999996 \t eps:  0.3546713552627619\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4397\n",
      "Episode Count:  1026 \t Cumulative Reward:  3.187000000000002 \t eps:  0.35431668390749915\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7811\n",
      "Episode Count:  1027 \t Cumulative Reward:  3.0629999999999993 \t eps:  0.35396236722359165\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4597\n",
      "Episode Count:  1028 \t Cumulative Reward:  6.2429999999999986 \t eps:  0.3536084048563681\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.4581\n",
      "Episode Count:  1029 \t Cumulative Reward:  -0.26399999999999924 \t eps:  0.3532547964515117\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.7951\n",
      "Episode Count:  1030 \t Cumulative Reward:  6.357999999999999 \t eps:  0.3529015416550602\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9795\n",
      "Episode Count:  1031 \t Cumulative Reward:  -0.036 \t eps:  0.35254864011340514\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.5982\n",
      "Episode Count:  1032 \t Cumulative Reward:  -0.06900000000000103 \t eps:  0.35219609147329173\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8762\n",
      "Episode Count:  1033 \t Cumulative Reward:  6.827000000000001 \t eps:  0.35184389538181843\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0876\n",
      "Episode Count:  1034 \t Cumulative Reward:  -0.06000000000000358 \t eps:  0.3514920514864366\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4774\n",
      "Episode Count:  1035 \t Cumulative Reward:  15.126 \t eps:  0.35114055943495015\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2621\n",
      "Episode Count:  1036 \t Cumulative Reward:  23.777000000000005 \t eps:  0.3507894188755152\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7279\n",
      "Episode Count:  1037 \t Cumulative Reward:  16.468999999999998 \t eps:  0.35043862945663967\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.7979\n",
      "Episode Count:  1038 \t Cumulative Reward:  19.549000000000003 \t eps:  0.350088190827183\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.2051\n",
      "Episode Count:  1039 \t Cumulative Reward:  3.030999999999999 \t eps:  0.34973810263635585\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4517\n",
      "Episode Count:  1040 \t Cumulative Reward:  18.636 \t eps:  0.3493883645337195\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9550\n",
      "Episode Count:  1041 \t Cumulative Reward:  8.642999999999995 \t eps:  0.3490389761691858\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2824\n",
      "Episode Count:  1042 \t Cumulative Reward:  -0.12899999999999906 \t eps:  0.3486899371930166\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3919\n",
      "Episode Count:  1043 \t Cumulative Reward:  5.800000000000004 \t eps:  0.3483412472558236\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9947\n",
      "Episode Count:  1044 \t Cumulative Reward:  29.685999999999993 \t eps:  0.3479929060085678\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1679\n",
      "Episode Count:  1045 \t Cumulative Reward:  57.634 \t eps:  0.34764491310255924\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3732\n",
      "Episode Count:  1046 \t Cumulative Reward:  14.813 \t eps:  0.34729726818945666\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5067\n",
      "Episode Count:  1047 \t Cumulative Reward:  1.1670000000000011 \t eps:  0.3469499709212672\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6677\n",
      "Episode Count:  1048 \t Cumulative Reward:  -0.1930000000000005 \t eps:  0.34660302095034595\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0909\n",
      "Episode Count:  1049 \t Cumulative Reward:  -15.899000000000003 \t eps:  0.3462564179293956\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2470\n",
      "Episode Count:  1050 \t Cumulative Reward:  22.616 \t eps:  0.3459101615114662\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0885\n",
      "Episode Count:  1051 \t Cumulative Reward:  18.26 \t eps:  0.3455642513499547\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7308\n",
      "Episode Count:  1052 \t Cumulative Reward:  20.25 \t eps:  0.34521868709860476\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0357\n",
      "Episode Count:  1053 \t Cumulative Reward:  26.533999999999992 \t eps:  0.3448734684115062\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9290\n",
      "Episode Count:  1054 \t Cumulative Reward:  13.125 \t eps:  0.3445285949430947\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7570\n",
      "Episode Count:  1055 \t Cumulative Reward:  9.825000000000003 \t eps:  0.3441840663481516\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.5618\n",
      "Episode Count:  1056 \t Cumulative Reward:  26.564000000000007 \t eps:  0.3438398822818034\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3196\n",
      "Episode Count:  1057 \t Cumulative Reward:  6.436 \t eps:  0.3434960423995216\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.5038\n",
      "Episode Count:  1058 \t Cumulative Reward:  7.8199999999999985 \t eps:  0.34315254635712206\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3397\n",
      "Episode Count:  1059 \t Cumulative Reward:  11.579999999999998 \t eps:  0.3428093938107649\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2898\n",
      "Episode Count:  1060 \t Cumulative Reward:  3.229999999999999 \t eps:  0.34246658441695416\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7480\n",
      "Episode Count:  1061 \t Cumulative Reward:  16.978 \t eps:  0.3421241178325372\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.6371\n",
      "Episode Count:  1062 \t Cumulative Reward:  5.880000000000002 \t eps:  0.3417819937147047\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1950\n",
      "Episode Count:  1063 \t Cumulative Reward:  6.414000000000001 \t eps:  0.34144021172099\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8359\n",
      "Episode Count:  1064 \t Cumulative Reward:  6.007000000000002 \t eps:  0.341098771509269\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.5088\n",
      "Episode Count:  1065 \t Cumulative Reward:  1.6299999999999988 \t eps:  0.3407576727377597\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2188\n",
      "Episode Count:  1066 \t Cumulative Reward:  9.085999999999999 \t eps:  0.3404169150650219\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2076\n",
      "Episode Count:  1067 \t Cumulative Reward:  7.216999999999997 \t eps:  0.3400764981499569\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8284\n",
      "Episode Count:  1068 \t Cumulative Reward:  10.159999999999991 \t eps:  0.33973642165180695\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3817\n",
      "Episode Count:  1069 \t Cumulative Reward:  15.667000000000002 \t eps:  0.33939668523015515\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9022\n",
      "Episode Count:  1070 \t Cumulative Reward:  5.382999999999995 \t eps:  0.339057288544925\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8506\n",
      "Episode Count:  1071 \t Cumulative Reward:  5.1960000000000015 \t eps:  0.33871823125638006\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2776\n",
      "Episode Count:  1072 \t Cumulative Reward:  7.804 \t eps:  0.3383795130251237\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.5186\n",
      "Episode Count:  1073 \t Cumulative Reward:  16.835 \t eps:  0.3380411335120986\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.9822\n",
      "Episode Count:  1074 \t Cumulative Reward:  -4.022999999999998 \t eps:  0.3377030923785865\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.1204\n",
      "Episode Count:  1075 \t Cumulative Reward:  37.489000000000004 \t eps:  0.3373653892862079\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0084\n",
      "Episode Count:  1076 \t Cumulative Reward:  6.017000000000001 \t eps:  0.3370280238969217\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2258\n",
      "Episode Count:  1077 \t Cumulative Reward:  25.605 \t eps:  0.3366909958730248\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4406\n",
      "Episode Count:  1078 \t Cumulative Reward:  6.383000000000004 \t eps:  0.3363543048771518\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.6423\n",
      "Episode Count:  1079 \t Cumulative Reward:  19.243000000000002 \t eps:  0.3360179505722746\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.0137\n",
      "Episode Count:  1080 \t Cumulative Reward:  5.493999999999996 \t eps:  0.3356819326217024\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0368\n",
      "Episode Count:  1081 \t Cumulative Reward:  13.049000000000003 \t eps:  0.3353462506890807\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3477\n",
      "Episode Count:  1082 \t Cumulative Reward:  21.8 \t eps:  0.3350109044383916\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.6175\n",
      "Episode Count:  1083 \t Cumulative Reward:  5.953000000000001 \t eps:  0.3346758935339532\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3837\n",
      "Episode Count:  1084 \t Cumulative Reward:  4.282 \t eps:  0.33434121764041924\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8592\n",
      "Episode Count:  1085 \t Cumulative Reward:  18.946 \t eps:  0.3340068764227788\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8282\n",
      "Episode Count:  1086 \t Cumulative Reward:  7.285000000000004 \t eps:  0.33367286954635605\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4712\n",
      "Episode Count:  1087 \t Cumulative Reward:  13.480999999999998 \t eps:  0.3333391966768097\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.1421\n",
      "Episode Count:  1088 \t Cumulative Reward:  31.516000000000002 \t eps:  0.3330058574801329\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.4362\n",
      "Episode Count:  1089 \t Cumulative Reward:  17.122 \t eps:  0.33267285162265275\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3903\n",
      "Episode Count:  1090 \t Cumulative Reward:  43.873000000000005 \t eps:  0.3323401787710301\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9440\n",
      "Episode Count:  1091 \t Cumulative Reward:  9.502000000000002 \t eps:  0.3320078385922591\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0711\n",
      "Episode Count:  1092 \t Cumulative Reward:  38.511 \t eps:  0.3316758307536668\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3754\n",
      "Episode Count:  1093 \t Cumulative Reward:  22.891000000000005 \t eps:  0.33134415492291314\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.4126\n",
      "Episode Count:  1094 \t Cumulative Reward:  17.834 \t eps:  0.3310128107679902\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.8691\n",
      "Episode Count:  1095 \t Cumulative Reward:  20.012999999999998 \t eps:  0.33068179795722225\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.2301\n",
      "Episode Count:  1096 \t Cumulative Reward:  17.475999999999996 \t eps:  0.33035111615926505\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.7153\n",
      "Episode Count:  1097 \t Cumulative Reward:  12.681999999999997 \t eps:  0.3300207650431058\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.5863\n",
      "Episode Count:  1098 \t Cumulative Reward:  11.868000000000002 \t eps:  0.32969074427806266\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.9034\n",
      "Episode Count:  1099 \t Cumulative Reward:  21.259000000000007 \t eps:  0.3293610535337846\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.4152\n",
      "Episode Count:  1100 \t Cumulative Reward:  21.365 \t eps:  0.3290316924802508\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.8925\n",
      "Episode Count:  1101 \t Cumulative Reward:  7.899000000000003 \t eps:  0.32870266078777055\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3221\n",
      "Episode Count:  1102 \t Cumulative Reward:  7.4759999999999955 \t eps:  0.3283739581269828\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.7757\n",
      "Episode Count:  1103 \t Cumulative Reward:  17.984 \t eps:  0.3280455841688558\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2915\n",
      "Episode Count:  1104 \t Cumulative Reward:  11.041 \t eps:  0.32771753858468694\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.3443\n",
      "Episode Count:  1105 \t Cumulative Reward:  4.619000000000003 \t eps:  0.32738982104610226\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.4363\n",
      "Episode Count:  1106 \t Cumulative Reward:  10.636999999999999 \t eps:  0.3270624312250562\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9401\n",
      "Episode Count:  1107 \t Cumulative Reward:  26.381999999999998 \t eps:  0.32673536879383114\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.8812\n",
      "Episode Count:  1108 \t Cumulative Reward:  1.1880000000000015 \t eps:  0.3264086334250373\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0380\n",
      "Episode Count:  1109 \t Cumulative Reward:  -0.11700000000000181 \t eps:  0.32608222479161225\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.2918\n",
      "Episode Count:  1110 \t Cumulative Reward:  10.162999999999997 \t eps:  0.3257561425668206\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9266\n",
      "Episode Count:  1111 \t Cumulative Reward:  12.237000000000004 \t eps:  0.3254303864242538\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.6155\n",
      "Episode Count:  1112 \t Cumulative Reward:  13.246000000000002 \t eps:  0.3251049560378296\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.1850\n",
      "Episode Count:  1113 \t Cumulative Reward:  0.7799999999999999 \t eps:  0.32477985108179175\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4348\n",
      "Episode Count:  1114 \t Cumulative Reward:  10.057999999999995 \t eps:  0.32445507123070993\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.9933\n",
      "Episode Count:  1115 \t Cumulative Reward:  -0.1139999999999972 \t eps:  0.32413061615947925\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4321\n",
      "Episode Count:  1116 \t Cumulative Reward:  -0.26399999999999924 \t eps:  0.32380648554331976\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3137\n",
      "Episode Count:  1117 \t Cumulative Reward:  -0.14400000000000002 \t eps:  0.32348267905777645\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.6817\n",
      "Episode Count:  1118 \t Cumulative Reward:  -0.16799999999999923 \t eps:  0.32315919637871865\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0208\n",
      "Episode Count:  1119 \t Cumulative Reward:  -0.060000000000000386 \t eps:  0.32283603718233994\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.3729\n",
      "Episode Count:  1120 \t Cumulative Reward:  -0.08699999999999464 \t eps:  0.3225132011451576\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0813\n",
      "Episode Count:  1121 \t Cumulative Reward:  9.04 \t eps:  0.3221906879440124\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8967\n",
      "Episode Count:  1122 \t Cumulative Reward:  -0.0839999999999972 \t eps:  0.32186849725606836\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4852\n",
      "Episode Count:  1123 \t Cumulative Reward:  40.346000000000004 \t eps:  0.3215466287588123\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2355\n",
      "Episode Count:  1124 \t Cumulative Reward:  14.352999999999996 \t eps:  0.32122508213005346\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8320\n",
      "Episode Count:  1125 \t Cumulative Reward:  13.734 \t eps:  0.3209038570479234\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7313\n",
      "Episode Count:  1126 \t Cumulative Reward:  -0.28500000000000236 \t eps:  0.3205829531908755\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2379\n",
      "Episode Count:  1127 \t Cumulative Reward:  20.763 \t eps:  0.3202623702376846\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7811\n",
      "Episode Count:  1128 \t Cumulative Reward:  2.7100000000000013 \t eps:  0.3199421078674469\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5378\n",
      "Episode Count:  1129 \t Cumulative Reward:  -0.16799999999999887 \t eps:  0.31962216575957947\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.4542\n",
      "Episode Count:  1130 \t Cumulative Reward:  14.961 \t eps:  0.3193025435938199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4003\n",
      "Episode Count:  1131 \t Cumulative Reward:  0.29299999999999893 \t eps:  0.3189832410502261\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2775\n",
      "Episode Count:  1132 \t Cumulative Reward:  1.2180000000000013 \t eps:  0.31866425780917584\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4282\n",
      "Episode Count:  1133 \t Cumulative Reward:  34.951000000000015 \t eps:  0.31834559355136666\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7545\n",
      "Episode Count:  1134 \t Cumulative Reward:  -0.06599999999999881 \t eps:  0.3180272479578153\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0942\n",
      "Episode Count:  1135 \t Cumulative Reward:  -0.20700000000000063 \t eps:  0.31770922070985746\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.7577\n",
      "Episode Count:  1136 \t Cumulative Reward:  5.975000000000002 \t eps:  0.3173915114891476\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3579\n",
      "Episode Count:  1137 \t Cumulative Reward:  -0.12000000000000084 \t eps:  0.3170741199776585\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1877\n",
      "Episode Count:  1138 \t Cumulative Reward:  13.613999999999999 \t eps:  0.31675704585768083\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4345\n",
      "Episode Count:  1139 \t Cumulative Reward:  5.899 \t eps:  0.3164402888118231\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 4.2120\n",
      "Episode Count:  1140 \t Cumulative Reward:  24.792 \t eps:  0.3161238485230113\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2989\n",
      "Episode Count:  1141 \t Cumulative Reward:  11.934999999999999 \t eps:  0.31580772467448825\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9715\n",
      "Episode Count:  1142 \t Cumulative Reward:  8.044 \t eps:  0.31549191694981377\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8488\n",
      "Episode Count:  1143 \t Cumulative Reward:  5.767 \t eps:  0.31517642503286397\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5030\n",
      "Episode Count:  1144 \t Cumulative Reward:  -0.08700000000000105 \t eps:  0.3148612486078311\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.8504\n",
      "Episode Count:  1145 \t Cumulative Reward:  -10.268999999999998 \t eps:  0.31454638735922325\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5451\n",
      "Episode Count:  1146 \t Cumulative Reward:  8.844999999999995 \t eps:  0.314231840971864\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.5779\n",
      "Episode Count:  1147 \t Cumulative Reward:  -0.18000000000000282 \t eps:  0.3139176091308922\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3294\n",
      "Episode Count:  1148 \t Cumulative Reward:  -0.10500000000000102 \t eps:  0.31360369152176126\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3272\n",
      "Episode Count:  1149 \t Cumulative Reward:  -0.06900000000000103 \t eps:  0.3132900878302395\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9159\n",
      "Episode Count:  1150 \t Cumulative Reward:  -0.17000000000000065 \t eps:  0.3129767977424092\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.2913\n",
      "Episode Count:  1151 \t Cumulative Reward:  12.193 \t eps:  0.3126638209446668\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.4275\n",
      "Episode Count:  1152 \t Cumulative Reward:  5.908999999999997 \t eps:  0.31235115712372213\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.3757\n",
      "Episode Count:  1153 \t Cumulative Reward:  5.238999999999999 \t eps:  0.3120388059665984\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0023\n",
      "Episode Count:  1154 \t Cumulative Reward:  -0.1079999999999972 \t eps:  0.3117267671606318\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3784\n",
      "Episode Count:  1155 \t Cumulative Reward:  -0.1859999999999985 \t eps:  0.3114150403934712\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3224\n",
      "Episode Count:  1156 \t Cumulative Reward:  0.002999999999991813 \t eps:  0.31110362535307773\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9791\n",
      "Episode Count:  1157 \t Cumulative Reward:  -0.0839999999999972 \t eps:  0.31079252172772465\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.1792\n",
      "Episode Count:  1158 \t Cumulative Reward:  7.766999999999999 \t eps:  0.3104817292059969\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8643\n",
      "Episode Count:  1159 \t Cumulative Reward:  6.155999999999997 \t eps:  0.3101712474767909\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4028\n",
      "Episode Count:  1160 \t Cumulative Reward:  -0.0689999999999954 \t eps:  0.30986107622931414\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0613\n",
      "Episode Count:  1161 \t Cumulative Reward:  -0.00900000000000179 \t eps:  0.3095512151530848\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4208\n",
      "Episode Count:  1162 \t Cumulative Reward:  -0.08900000000000077 \t eps:  0.30924166393793173\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7524\n",
      "Episode Count:  1163 \t Cumulative Reward:  9.239000000000004 \t eps:  0.3089324222739938\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1477\n",
      "Episode Count:  1164 \t Cumulative Reward:  -0.0420000000000004 \t eps:  0.30862348985171983\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0697\n",
      "Episode Count:  1165 \t Cumulative Reward:  25.16700000000001 \t eps:  0.3083148663618681\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0026\n",
      "Episode Count:  1166 \t Cumulative Reward:  -0.20999999999999885 \t eps:  0.30800655149550626\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2435\n",
      "Episode Count:  1167 \t Cumulative Reward:  -0.13500000000000462 \t eps:  0.3076985449440108\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9401\n",
      "Episode Count:  1168 \t Cumulative Reward:  7.0379999999999985 \t eps:  0.3073908463990668\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1144\n",
      "Episode Count:  1169 \t Cumulative Reward:  -0.22500000000000028 \t eps:  0.3070834555526677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9322\n",
      "Episode Count:  1170 \t Cumulative Reward:  12.494000000000002 \t eps:  0.306776372097115\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0180\n",
      "Episode Count:  1171 \t Cumulative Reward:  5.263999999999999 \t eps:  0.30646959572501786\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9173\n",
      "Episode Count:  1172 \t Cumulative Reward:  7.205 \t eps:  0.30616312612929286\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1495\n",
      "Episode Count:  1173 \t Cumulative Reward:  12.072000000000003 \t eps:  0.30585696300316356\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3052\n",
      "Episode Count:  1174 \t Cumulative Reward:  15.227000000000002 \t eps:  0.3055511060401604\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4617\n",
      "Episode Count:  1175 \t Cumulative Reward:  18.215 \t eps:  0.30524555493412026\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4084\n",
      "Episode Count:  1176 \t Cumulative Reward:  7.718999999999999 \t eps:  0.30494030937918615\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.4587\n",
      "Episode Count:  1177 \t Cumulative Reward:  0.33199999999999935 \t eps:  0.304635369069807\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3393\n",
      "Episode Count:  1178 \t Cumulative Reward:  0.035999999999999 \t eps:  0.30433073370073715\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3681\n",
      "Episode Count:  1179 \t Cumulative Reward:  -0.03300000000000819 \t eps:  0.3040264029670364\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8415\n",
      "Episode Count:  1180 \t Cumulative Reward:  -1.161 \t eps:  0.30372237656406936\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1192\n",
      "Episode Count:  1181 \t Cumulative Reward:  0.7140000000000032 \t eps:  0.3034186541875053\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6567\n",
      "Episode Count:  1182 \t Cumulative Reward:  -0.26399999999999807 \t eps:  0.3031152355333178\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0283\n",
      "Episode Count:  1183 \t Cumulative Reward:  0.3129999999999987 \t eps:  0.3028121202977845\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4567\n",
      "Episode Count:  1184 \t Cumulative Reward:  8.805000000000001 \t eps:  0.3025093081774867\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3614\n",
      "Episode Count:  1185 \t Cumulative Reward:  0.002999999999991814 \t eps:  0.30220679886930923\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6920\n",
      "Episode Count:  1186 \t Cumulative Reward:  4.021000000000001 \t eps:  0.3019045920704399\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6775\n",
      "Episode Count:  1187 \t Cumulative Reward:  -0.08700000000000258 \t eps:  0.30160268747836944\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9410\n",
      "Episode Count:  1188 \t Cumulative Reward:  -0.9149999999999999 \t eps:  0.30130108479089107\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5927\n",
      "Episode Count:  1189 \t Cumulative Reward:  -0.019999999999998977 \t eps:  0.3009997837061002\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1777\n",
      "Episode Count:  1190 \t Cumulative Reward:  14.103000000000002 \t eps:  0.3006987839223941\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0433\n",
      "Episode Count:  1191 \t Cumulative Reward:  39.559 \t eps:  0.30039808513847166\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8843\n",
      "Episode Count:  1192 \t Cumulative Reward:  -0.22500000000000028 \t eps:  0.3000976870533332\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8340\n",
      "Episode Count:  1193 \t Cumulative Reward:  5.5530000000000035 \t eps:  0.2997975893662798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8614\n",
      "Episode Count:  1194 \t Cumulative Reward:  -0.2519999999999981 \t eps:  0.29949779177691355\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6138\n",
      "Episode Count:  1195 \t Cumulative Reward:  -0.1079999999999972 \t eps:  0.29919829398513664\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0015\n",
      "Episode Count:  1196 \t Cumulative Reward:  -0.1199999999999972 \t eps:  0.29889909569115153\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5010\n",
      "Episode Count:  1197 \t Cumulative Reward:  -0.1079999999999972 \t eps:  0.2986001965954604\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4209\n",
      "Episode Count:  1198 \t Cumulative Reward:  6.417000000000001 \t eps:  0.29830159639886494\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1818\n",
      "Episode Count:  1199 \t Cumulative Reward:  18.585 \t eps:  0.29800329480246607\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1363\n",
      "Episode Count:  1200 \t Cumulative Reward:  1.2090000000000014 \t eps:  0.2977052915076636\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5607\n",
      "Episode Count:  1201 \t Cumulative Reward:  -0.027000000000001793 \t eps:  0.29740758621615593\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6268\n",
      "Episode Count:  1202 \t Cumulative Reward:  -0.019999999999998977 \t eps:  0.29711017862993977\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.5843\n",
      "Episode Count:  1203 \t Cumulative Reward:  9.620000000000003 \t eps:  0.29681306845130984\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4854\n",
      "Episode Count:  1204 \t Cumulative Reward:  -0.135 \t eps:  0.29651625538285853\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8105\n",
      "Episode Count:  1205 \t Cumulative Reward:  -0.05999999999999718 \t eps:  0.2962197391274757\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6472\n",
      "Episode Count:  1206 \t Cumulative Reward:  -0.13500000000000462 \t eps:  0.2959235193883482\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.6025\n",
      "Episode Count:  1207 \t Cumulative Reward:  10.841000000000005 \t eps:  0.29562759586895987\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2595\n",
      "Episode Count:  1208 \t Cumulative Reward:  6.327 \t eps:  0.2953319682730909\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1561\n",
      "Episode Count:  1209 \t Cumulative Reward:  3.1219999999999994 \t eps:  0.2950366363048178\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9553\n",
      "Episode Count:  1210 \t Cumulative Reward:  -0.09899999999999899 \t eps:  0.294741599668513\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6657\n",
      "Episode Count:  1211 \t Cumulative Reward:  0.3559999999999982 \t eps:  0.29444685806884446\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3469\n",
      "Episode Count:  1212 \t Cumulative Reward:  3.223000000000001 \t eps:  0.2941524112107756\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2642\n",
      "Episode Count:  1213 \t Cumulative Reward:  11.229999999999999 \t eps:  0.29385825879956484\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4825\n",
      "Episode Count:  1214 \t Cumulative Reward:  3.6630000000000003 \t eps:  0.29356440054076527\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6498\n",
      "Episode Count:  1215 \t Cumulative Reward:  -0.027000000000001793 \t eps:  0.2932708361402245\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3614\n",
      "Episode Count:  1216 \t Cumulative Reward:  -1.212 \t eps:  0.29297756530408425\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1755\n",
      "Episode Count:  1217 \t Cumulative Reward:  -0.08700000000000256 \t eps:  0.29268458773878014\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4056\n",
      "Episode Count:  1218 \t Cumulative Reward:  -0.08599999999999758 \t eps:  0.29239190315104135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7727\n",
      "Episode Count:  1219 \t Cumulative Reward:  3.509999999999998 \t eps:  0.2920995112478903\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4574\n",
      "Episode Count:  1220 \t Cumulative Reward:  -0.03300000000000219 \t eps:  0.29180741173664243\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3050\n",
      "Episode Count:  1221 \t Cumulative Reward:  0.3839999999999982 \t eps:  0.2915156043249058\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5264\n",
      "Episode Count:  1222 \t Cumulative Reward:  2.7180000000000026 \t eps:  0.2912240887205809\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2461\n",
      "Episode Count:  1223 \t Cumulative Reward:  4.157 \t eps:  0.29093286463186036\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2795\n",
      "Episode Count:  1224 \t Cumulative Reward:  -0.04500000000000178 \t eps:  0.29064193176722847\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0474\n",
      "Episode Count:  1225 \t Cumulative Reward:  -0.17700000000000063 \t eps:  0.2903512898354612\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1509\n",
      "Episode Count:  1226 \t Cumulative Reward:  -0.06000000000000001 \t eps:  0.29006093854562576\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9487\n",
      "Episode Count:  1227 \t Cumulative Reward:  -0.20999999999999885 \t eps:  0.28977087760708015\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2937\n",
      "Episode Count:  1228 \t Cumulative Reward:  -0.02999999999999719 \t eps:  0.2894811067294731\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5516\n",
      "Episode Count:  1229 \t Cumulative Reward:  5.048999999999999 \t eps:  0.2891916256227436\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0465\n",
      "Episode Count:  1230 \t Cumulative Reward:  -0.08799999999999539 \t eps:  0.28890243399712084\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2561\n",
      "Episode Count:  1231 \t Cumulative Reward:  -0.04199999999999284 \t eps:  0.28861353156312375\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8737\n",
      "Episode Count:  1232 \t Cumulative Reward:  -0.11400000000000207 \t eps:  0.28832491803156063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3651\n",
      "Episode Count:  1233 \t Cumulative Reward:  14.266 \t eps:  0.2880365931135291\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8173\n",
      "Episode Count:  1234 \t Cumulative Reward:  -0.14399999999999644 \t eps:  0.28774855652041553\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7449\n",
      "Episode Count:  1235 \t Cumulative Reward:  4.675000000000001 \t eps:  0.2874608079638951\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2102\n",
      "Episode Count:  1236 \t Cumulative Reward:  5.081000000000002 \t eps:  0.28717334715593124\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8363\n",
      "Episode Count:  1237 \t Cumulative Reward:  19.165 \t eps:  0.2868861738087753\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1726\n",
      "Episode Count:  1238 \t Cumulative Reward:  0.6689999999999996 \t eps:  0.2865992876349665\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0410\n",
      "Episode Count:  1239 \t Cumulative Reward:  -0.059999999999996424 \t eps:  0.28631268834733153\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9566\n",
      "Episode Count:  1240 \t Cumulative Reward:  -0.002999999999997182 \t eps:  0.2860263756589842\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2992\n",
      "Episode Count:  1241 \t Cumulative Reward:  -0.09599999999999925 \t eps:  0.2857403492833252\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8764\n",
      "Episode Count:  1242 \t Cumulative Reward:  7.249000000000002 \t eps:  0.2854546089340419\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8588\n",
      "Episode Count:  1243 \t Cumulative Reward:  -0.14700000000000144 \t eps:  0.28516915432510787\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5197\n",
      "Episode Count:  1244 \t Cumulative Reward:  14.938999999999998 \t eps:  0.2848839851707828\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1047\n",
      "Episode Count:  1245 \t Cumulative Reward:  7.491 \t eps:  0.284599101185612\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7068\n",
      "Episode Count:  1246 \t Cumulative Reward:  -0.13900000000000284 \t eps:  0.2843145020844264\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4492\n",
      "Episode Count:  1247 \t Cumulative Reward:  3.5330000000000017 \t eps:  0.28403018758234194\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2435\n",
      "Episode Count:  1248 \t Cumulative Reward:  -0.1649999999999998 \t eps:  0.2837461573947596\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2868\n",
      "Episode Count:  1249 \t Cumulative Reward:  16.221 \t eps:  0.2834624112373649\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5635\n",
      "Episode Count:  1250 \t Cumulative Reward:  0.09799999999999823 \t eps:  0.2831789488261275\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2636\n",
      "Episode Count:  1251 \t Cumulative Reward:  -0.09599999999999925 \t eps:  0.28289576987730136\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7739\n",
      "Episode Count:  1252 \t Cumulative Reward:  22.008000000000006 \t eps:  0.2826128741074241\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9395\n",
      "Episode Count:  1253 \t Cumulative Reward:  4.099 \t eps:  0.28233026123331667\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1941\n",
      "Episode Count:  1254 \t Cumulative Reward:  6.011999999999998 \t eps:  0.28204793097208336\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0141\n",
      "Episode Count:  1255 \t Cumulative Reward:  3.692000000000004 \t eps:  0.2817658830411113\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.5080\n",
      "Episode Count:  1256 \t Cumulative Reward:  -0.02999999999999719 \t eps:  0.28148411715807015\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9888\n",
      "Episode Count:  1257 \t Cumulative Reward:  -0.08699999999999783 \t eps:  0.2812026330409121\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1356\n",
      "Episode Count:  1258 \t Cumulative Reward:  -0.09599999999999925 \t eps:  0.28092143040787115\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8584\n",
      "Episode Count:  1259 \t Cumulative Reward:  -0.05399999999999641 \t eps:  0.2806405089774633\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.9181\n",
      "Episode Count:  1260 \t Cumulative Reward:  -0.0869999999999982 \t eps:  0.2803598684684858\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4322\n",
      "Episode Count:  1261 \t Cumulative Reward:  -0.9440000000000002 \t eps:  0.28007950860001735\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7554\n",
      "Episode Count:  1262 \t Cumulative Reward:  12.219999999999999 \t eps:  0.2797994290914173\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7476\n",
      "Episode Count:  1263 \t Cumulative Reward:  13.396999999999998 \t eps:  0.2795196296623259\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0021\n",
      "Episode Count:  1264 \t Cumulative Reward:  1.2709999999999992 \t eps:  0.2792401100326636\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0810\n",
      "Episode Count:  1265 \t Cumulative Reward:  -0.18200000000000383 \t eps:  0.2789608699226309\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9666\n",
      "Episode Count:  1266 \t Cumulative Reward:  -0.012000000000001789 \t eps:  0.2786819090527083\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9232\n",
      "Episode Count:  1267 \t Cumulative Reward:  24.875 \t eps:  0.27840322714365556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7101\n",
      "Episode Count:  1268 \t Cumulative Reward:  1.522000000000003 \t eps:  0.2781248239165119\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.6373\n",
      "Episode Count:  1269 \t Cumulative Reward:  11.260000000000002 \t eps:  0.27784669909259535\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8359\n",
      "Episode Count:  1270 \t Cumulative Reward:  6.645000000000001 \t eps:  0.27756885239350276\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7276\n",
      "Episode Count:  1271 \t Cumulative Reward:  5.390000000000002 \t eps:  0.27729128354110927\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0918\n",
      "Episode Count:  1272 \t Cumulative Reward:  0.07499999999999589 \t eps:  0.2770139922575682\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0673\n",
      "Episode Count:  1273 \t Cumulative Reward:  -11.797 \t eps:  0.27673697826531063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.5372\n",
      "Episode Count:  1274 \t Cumulative Reward:  -0.20700000000000066 \t eps:  0.2764602412870453\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8281\n",
      "Episode Count:  1275 \t Cumulative Reward:  -0.0390000000000018 \t eps:  0.2761837810457583\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8143\n",
      "Episode Count:  1276 \t Cumulative Reward:  -0.01400000000000179 \t eps:  0.27590759726471253\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.3599\n",
      "Episode Count:  1277 \t Cumulative Reward:  -0.21699999999999864 \t eps:  0.27563168966744783\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6949\n",
      "Episode Count:  1278 \t Cumulative Reward:  8.781999999999998 \t eps:  0.2753560579777804\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2749\n",
      "Episode Count:  1279 \t Cumulative Reward:  9.350999999999999 \t eps:  0.2750807019198026\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1720\n",
      "Episode Count:  1280 \t Cumulative Reward:  4.477999999999999 \t eps:  0.2748056212178828\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7545\n",
      "Episode Count:  1281 \t Cumulative Reward:  -0.10500000000000102 \t eps:  0.2745308155966649\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3778\n",
      "Episode Count:  1282 \t Cumulative Reward:  11.926999999999998 \t eps:  0.2742562847810683\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9279\n",
      "Episode Count:  1283 \t Cumulative Reward:  14.274999999999999 \t eps:  0.2739820284962872\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6377\n",
      "Episode Count:  1284 \t Cumulative Reward:  -0.0420000000000036 \t eps:  0.27370804646779096\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9130\n",
      "Episode Count:  1285 \t Cumulative Reward:  -0.05399999999999641 \t eps:  0.27343433842132314\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9555\n",
      "Episode Count:  1286 \t Cumulative Reward:  -0.11400000000000207 \t eps:  0.27316090408290183\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2166\n",
      "Episode Count:  1287 \t Cumulative Reward:  6.39 \t eps:  0.2728877431788189\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6665\n",
      "Episode Count:  1288 \t Cumulative Reward:  8.156 \t eps:  0.2726148554356401\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8675\n",
      "Episode Count:  1289 \t Cumulative Reward:  22.887000000000008 \t eps:  0.2723422405802045\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6785\n",
      "Episode Count:  1290 \t Cumulative Reward:  6.526999999999992 \t eps:  0.2720698983396243\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4310\n",
      "Episode Count:  1291 \t Cumulative Reward:  -0.05999999999999718 \t eps:  0.2717978284412847\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2350\n",
      "Episode Count:  1292 \t Cumulative Reward:  -0.1159999999999986 \t eps:  0.2715260306128434\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7064\n",
      "Episode Count:  1293 \t Cumulative Reward:  19.398000000000003 \t eps:  0.27125450458223055\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8937\n",
      "Episode Count:  1294 \t Cumulative Reward:  0.09600000000000092 \t eps:  0.27098325007764834\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6846\n",
      "Episode Count:  1295 \t Cumulative Reward:  -9.097000000000001 \t eps:  0.2707122668275707\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3526\n",
      "Episode Count:  1296 \t Cumulative Reward:  0.22800000000000048 \t eps:  0.27044155456074315\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6274\n",
      "Episode Count:  1297 \t Cumulative Reward:  0.38099999999999895 \t eps:  0.2701711130061824\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3351\n",
      "Episode Count:  1298 \t Cumulative Reward:  0.9370000000000013 \t eps:  0.2699009418931762\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3829\n",
      "Episode Count:  1299 \t Cumulative Reward:  -0.023000000000003587 \t eps:  0.26963104095128304\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5177\n",
      "Episode Count:  1300 \t Cumulative Reward:  7.537000000000001 \t eps:  0.26936140991033175\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1885\n",
      "Episode Count:  1301 \t Cumulative Reward:  -0.08099999999999896 \t eps:  0.2690920485004214\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1759\n",
      "Episode Count:  1302 \t Cumulative Reward:  0.22400000000000386 \t eps:  0.268822956451921\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7087\n",
      "Episode Count:  1303 \t Cumulative Reward:  11.755000000000003 \t eps:  0.2685541334954691\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1468\n",
      "Episode Count:  1304 \t Cumulative Reward:  19.28300000000001 \t eps:  0.2682855793619736\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0247\n",
      "Episode Count:  1305 \t Cumulative Reward:  5.306 \t eps:  0.26801729378261163\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.2254\n",
      "Episode Count:  1306 \t Cumulative Reward:  6.853999999999998 \t eps:  0.267749276488829\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.8130\n",
      "Episode Count:  1307 \t Cumulative Reward:  0.08399999999999971 \t eps:  0.2674815272123402\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.5039\n",
      "Episode Count:  1308 \t Cumulative Reward:  7.4799999999999995 \t eps:  0.26721404568512785\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1815\n",
      "Episode Count:  1309 \t Cumulative Reward:  0.006000000000000484 \t eps:  0.2669468316394427\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.1420\n",
      "Episode Count:  1310 \t Cumulative Reward:  -0.15599999999998926 \t eps:  0.26667988480780325\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3832\n",
      "Episode Count:  1311 \t Cumulative Reward:  -0.15900000000000103 \t eps:  0.2664132049229954\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2396\n",
      "Episode Count:  1312 \t Cumulative Reward:  5.2860000000000005 \t eps:  0.26614679171807243\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.9579\n",
      "Episode Count:  1313 \t Cumulative Reward:  4.4590000000000005 \t eps:  0.2658806449263544\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0590\n",
      "Episode Count:  1314 \t Cumulative Reward:  -0.07500000000000104 \t eps:  0.26561476428142805\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.7027\n",
      "Episode Count:  1315 \t Cumulative Reward:  -0.017999999999999995 \t eps:  0.2653491495171466\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3095\n",
      "Episode Count:  1316 \t Cumulative Reward:  7.468000000000002 \t eps:  0.26508380036762946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3467\n",
      "Episode Count:  1317 \t Cumulative Reward:  13.04 \t eps:  0.26481871656726186\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3393\n",
      "Episode Count:  1318 \t Cumulative Reward:  8.932 \t eps:  0.2645538978506946\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1561\n",
      "Episode Count:  1319 \t Cumulative Reward:  19.621000000000002 \t eps:  0.2642893439528439\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2737\n",
      "Episode Count:  1320 \t Cumulative Reward:  6.401000000000003 \t eps:  0.2640250546088911\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1847\n",
      "Episode Count:  1321 \t Cumulative Reward:  9.279 \t eps:  0.2637610295542822\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0941\n",
      "Episode Count:  1322 \t Cumulative Reward:  11.389999999999999 \t eps:  0.26349726852472793\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3192\n",
      "Episode Count:  1323 \t Cumulative Reward:  8.555000000000001 \t eps:  0.26323377125620323\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0173\n",
      "Episode Count:  1324 \t Cumulative Reward:  21.147000000000006 \t eps:  0.26297053748494703\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3944\n",
      "Episode Count:  1325 \t Cumulative Reward:  6.207000000000005 \t eps:  0.26270756694746206\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1792\n",
      "Episode Count:  1326 \t Cumulative Reward:  5.525 \t eps:  0.2624448593805146\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5164\n",
      "Episode Count:  1327 \t Cumulative Reward:  11.105 \t eps:  0.26218241452113406\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0607\n",
      "Episode Count:  1328 \t Cumulative Reward:  15.656000000000002 \t eps:  0.2619202321066129\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2443\n",
      "Episode Count:  1329 \t Cumulative Reward:  7.528 \t eps:  0.2616583118745063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6533\n",
      "Episode Count:  1330 \t Cumulative Reward:  7.9590000000000005 \t eps:  0.26139665356263175\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1535\n",
      "Episode Count:  1331 \t Cumulative Reward:  11.657 \t eps:  0.26113525690906914\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3794\n",
      "Episode Count:  1332 \t Cumulative Reward:  1.7730000000000006 \t eps:  0.26087412165216006\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5690\n",
      "Episode Count:  1333 \t Cumulative Reward:  6.803000000000001 \t eps:  0.2606132475305079\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4653\n",
      "Episode Count:  1334 \t Cumulative Reward:  -0.1080000000000036 \t eps:  0.2603526342829774\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6250\n",
      "Episode Count:  1335 \t Cumulative Reward:  7.077 \t eps:  0.2600922816486944\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4419\n",
      "Episode Count:  1336 \t Cumulative Reward:  1.1580000000000008 \t eps:  0.2598321893670457\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4360\n",
      "Episode Count:  1337 \t Cumulative Reward:  -0.2730000000000007 \t eps:  0.2595723571776787\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.3240\n",
      "Episode Count:  1338 \t Cumulative Reward:  1.4450000000000012 \t eps:  0.259312784820501\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.0105\n",
      "Episode Count:  1339 \t Cumulative Reward:  -0.00900000000000179 \t eps:  0.2590534720356805\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.9279\n",
      "Episode Count:  1340 \t Cumulative Reward:  -0.05099999999999861 \t eps:  0.2587944185636448\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4063\n",
      "Episode Count:  1341 \t Cumulative Reward:  -0.019999999999998977 \t eps:  0.2585356241450812\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5445\n",
      "Episode Count:  1342 \t Cumulative Reward:  -0.18899999999999784 \t eps:  0.2582770885209361\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2868\n",
      "Episode Count:  1343 \t Cumulative Reward:  -0.10500000000000101 \t eps:  0.2580188114324152\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3790\n",
      "Episode Count:  1344 \t Cumulative Reward:  3.768000000000002 \t eps:  0.2577607926209828\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.0882\n",
      "Episode Count:  1345 \t Cumulative Reward:  -0.05099999999999745 \t eps:  0.2575030318283618\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3876\n",
      "Episode Count:  1346 \t Cumulative Reward:  -0.09599999999999925 \t eps:  0.25724552879653345\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4524\n",
      "Episode Count:  1347 \t Cumulative Reward:  -0.2460000000000013 \t eps:  0.25698828326773693\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4469\n",
      "Episode Count:  1348 \t Cumulative Reward:  -0.09599999999999642 \t eps:  0.25673129498446917\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.2434\n",
      "Episode Count:  1349 \t Cumulative Reward:  4.335999999999997 \t eps:  0.2564745636894847\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7673\n",
      "Episode Count:  1350 \t Cumulative Reward:  5.268999999999999 \t eps:  0.2562180891257952\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4821\n",
      "Episode Count:  1351 \t Cumulative Reward:  -0.20700000000000066 \t eps:  0.2559618710366694\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3601\n",
      "Episode Count:  1352 \t Cumulative Reward:  -0.09599999999999925 \t eps:  0.25570590916563274\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.7614\n",
      "Episode Count:  1353 \t Cumulative Reward:  10.412000000000003 \t eps:  0.2554502032564671\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4271\n",
      "Episode Count:  1354 \t Cumulative Reward:  -0.1350000000000014 \t eps:  0.25519475305321065\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1311\n",
      "Episode Count:  1355 \t Cumulative Reward:  17.69700000000001 \t eps:  0.25493955830015746\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4725\n",
      "Episode Count:  1356 \t Cumulative Reward:  8.028 \t eps:  0.2546846187418573\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.9949\n",
      "Episode Count:  1357 \t Cumulative Reward:  18.101999999999997 \t eps:  0.2544299341231155\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5726\n",
      "Episode Count:  1358 \t Cumulative Reward:  1.9939999999999982 \t eps:  0.2541755041889924\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5695\n",
      "Episode Count:  1359 \t Cumulative Reward:  -0.04200000000000613 \t eps:  0.2539213286848034\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3999\n",
      "Episode Count:  1360 \t Cumulative Reward:  -0.005999999999997186 \t eps:  0.2536674073561186\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7792\n",
      "Episode Count:  1361 \t Cumulative Reward:  16.374 \t eps:  0.2534137399487625\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5707\n",
      "Episode Count:  1362 \t Cumulative Reward:  18.489 \t eps:  0.25316032620881374\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1286\n",
      "Episode Count:  1363 \t Cumulative Reward:  40.265 \t eps:  0.2529071658826049\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8008\n",
      "Episode Count:  1364 \t Cumulative Reward:  8.620999999999999 \t eps:  0.2526542587167223\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4873\n",
      "Episode Count:  1365 \t Cumulative Reward:  12.541 \t eps:  0.2524016044580056\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.9475\n",
      "Episode Count:  1366 \t Cumulative Reward:  15.747999999999998 \t eps:  0.2521492028535476\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0393\n",
      "Episode Count:  1367 \t Cumulative Reward:  10.953 \t eps:  0.25189705365069404\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9341\n",
      "Episode Count:  1368 \t Cumulative Reward:  43.25700000000001 \t eps:  0.25164515659704334\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5165\n",
      "Episode Count:  1369 \t Cumulative Reward:  33.816 \t eps:  0.2513935114404463\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7342\n",
      "Episode Count:  1370 \t Cumulative Reward:  22.084000000000003 \t eps:  0.25114211792900587\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8334\n",
      "Episode Count:  1371 \t Cumulative Reward:  5.158 \t eps:  0.2508909758110769\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.1161\n",
      "Episode Count:  1372 \t Cumulative Reward:  15.789000000000003 \t eps:  0.2506400848352658\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.3711\n",
      "Episode Count:  1373 \t Cumulative Reward:  22.458 \t eps:  0.2503894447504305\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7169\n",
      "Episode Count:  1374 \t Cumulative Reward:  1.104000000000001 \t eps:  0.2501390553056801\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.4674\n",
      "Episode Count:  1375 \t Cumulative Reward:  24.727999999999998 \t eps:  0.2498889162503744\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7487\n",
      "Episode Count:  1376 \t Cumulative Reward:  7.293999999999999 \t eps:  0.24963902733412402\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7552\n",
      "Episode Count:  1377 \t Cumulative Reward:  12.489000000000003 \t eps:  0.2493893883067899\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6251\n",
      "Episode Count:  1378 \t Cumulative Reward:  27.596 \t eps:  0.2491399989184831\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6409\n",
      "Episode Count:  1379 \t Cumulative Reward:  54.479000000000006 \t eps:  0.2488908589195646\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9451\n",
      "Episode Count:  1380 \t Cumulative Reward:  56.88499999999999 \t eps:  0.24864196806064504\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6233\n",
      "Episode Count:  1381 \t Cumulative Reward:  23.255000000000003 \t eps:  0.24839332609258438\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.6217\n",
      "Episode Count:  1382 \t Cumulative Reward:  24.892 \t eps:  0.2481449327664918\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7421\n",
      "Episode Count:  1383 \t Cumulative Reward:  17.075 \t eps:  0.24789678783372532\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.8802\n",
      "Episode Count:  1384 \t Cumulative Reward:  18.037999999999997 \t eps:  0.2476488910458916\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7302\n",
      "Episode Count:  1385 \t Cumulative Reward:  14.669000000000002 \t eps:  0.2474012421548457\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8349\n",
      "Episode Count:  1386 \t Cumulative Reward:  25.01799999999999 \t eps:  0.24715384091269088\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9481\n",
      "Episode Count:  1387 \t Cumulative Reward:  15.561999999999998 \t eps:  0.24690668707177818\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7777\n",
      "Episode Count:  1388 \t Cumulative Reward:  14.931999999999999 \t eps:  0.2466597803847064\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.5134\n",
      "Episode Count:  1389 \t Cumulative Reward:  26.032999999999998 \t eps:  0.2464131206043217\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0881\n",
      "Episode Count:  1390 \t Cumulative Reward:  24.576 \t eps:  0.24616670748371736\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1416\n",
      "Episode Count:  1391 \t Cumulative Reward:  19.451 \t eps:  0.24592054077623363\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9516\n",
      "Episode Count:  1392 \t Cumulative Reward:  17.442999999999998 \t eps:  0.24567462023545739\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2723\n",
      "Episode Count:  1393 \t Cumulative Reward:  0.5260000000000019 \t eps:  0.24542894561522194\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7190\n",
      "Episode Count:  1394 \t Cumulative Reward:  -0.08700000000000818 \t eps:  0.24518351666960672\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9483\n",
      "Episode Count:  1395 \t Cumulative Reward:  11.846000000000002 \t eps:  0.24493833315293712\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1166\n",
      "Episode Count:  1396 \t Cumulative Reward:  14.699999999999996 \t eps:  0.24469339481978417\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7690\n",
      "Episode Count:  1397 \t Cumulative Reward:  -0.078 \t eps:  0.2444487014249644\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6076\n",
      "Episode Count:  1398 \t Cumulative Reward:  -4.160000000000003 \t eps:  0.24420425272353943\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0742\n",
      "Episode Count:  1399 \t Cumulative Reward:  16.648000000000003 \t eps:  0.2439600484708159\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.8130\n",
      "Episode Count:  1400 \t Cumulative Reward:  7.255 \t eps:  0.24371608842234507\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4421\n",
      "Episode Count:  1401 \t Cumulative Reward:  17.807 \t eps:  0.24347237233392272\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9085\n",
      "Episode Count:  1402 \t Cumulative Reward:  21.511 \t eps:  0.24322889996158878\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.3107\n",
      "Episode Count:  1403 \t Cumulative Reward:  16.284000000000002 \t eps:  0.2429856710616272\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.0119\n",
      "Episode Count:  1404 \t Cumulative Reward:  0.1889999999999982 \t eps:  0.24274268539056557\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0109\n",
      "Episode Count:  1405 \t Cumulative Reward:  -0.2809999999999977 \t eps:  0.242499942705175\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1211\n",
      "Episode Count:  1406 \t Cumulative Reward:  -0.05800000000000281 \t eps:  0.24225744276246983\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5315\n",
      "Episode Count:  1407 \t Cumulative Reward:  0.25800000000000306 \t eps:  0.24201518531970737\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9510\n",
      "Episode Count:  1408 \t Cumulative Reward:  5.787999999999998 \t eps:  0.24177317013438765\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2352\n",
      "Episode Count:  1409 \t Cumulative Reward:  5.903000000000001 \t eps:  0.24153139696425327\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7780\n",
      "Episode Count:  1410 \t Cumulative Reward:  -0.1050000000000046 \t eps:  0.241289865567289\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0423\n",
      "Episode Count:  1411 \t Cumulative Reward:  -0.14399999999999644 \t eps:  0.24104857570172172\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0307\n",
      "Episode Count:  1412 \t Cumulative Reward:  9.072000000000001 \t eps:  0.24080752712602\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1629\n",
      "Episode Count:  1413 \t Cumulative Reward:  -0.059999999999996424 \t eps:  0.24056671959889397\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.7822\n",
      "Episode Count:  1414 \t Cumulative Reward:  10.520000000000001 \t eps:  0.24032615287929507\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4905\n",
      "Episode Count:  1415 \t Cumulative Reward:  -0.032000000000005385 \t eps:  0.24008582672641576\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.4376\n",
      "Episode Count:  1416 \t Cumulative Reward:  -3.2650000000000015 \t eps:  0.23984574089968935\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0964\n",
      "Episode Count:  1417 \t Cumulative Reward:  12.974000000000002 \t eps:  0.23960589515878966\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1840\n",
      "Episode Count:  1418 \t Cumulative Reward:  24.635 \t eps:  0.23936628926363088\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.6047\n",
      "Episode Count:  1419 \t Cumulative Reward:  35.369 \t eps:  0.23912692297436725\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0095\n",
      "Episode Count:  1420 \t Cumulative Reward:  7.158999999999998 \t eps:  0.23888779605139288\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.0378\n",
      "Episode Count:  1421 \t Cumulative Reward:  21.935000000000002 \t eps:  0.2386489082553415\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0120\n",
      "Episode Count:  1422 \t Cumulative Reward:  0.33299999999999613 \t eps:  0.23841025934708615\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.0814\n",
      "Episode Count:  1423 \t Cumulative Reward:  6.996999999999999 \t eps:  0.23817184908773906\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1931\n",
      "Episode Count:  1424 \t Cumulative Reward:  62.174 \t eps:  0.23793367723865133\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 1.9587\n",
      "Episode Count:  1425 \t Cumulative Reward:  0.008999999999998731 \t eps:  0.23769574356141268\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1777\n",
      "Episode Count:  1426 \t Cumulative Reward:  -0.16799999999999923 \t eps:  0.23745804781785126\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.1467\n",
      "Episode Count:  1427 \t Cumulative Reward:  -0.045000000000004606 \t eps:  0.2372205897700334\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 1.7024\n",
      "Episode Count:  1428 \t Cumulative Reward:  8.966000000000001 \t eps:  0.2369833691802634\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.2765\n",
      "Episode Count:  1429 \t Cumulative Reward:  24.438000000000006 \t eps:  0.23674638581108312\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2423\n",
      "Episode Count:  1430 \t Cumulative Reward:  19.316 \t eps:  0.23650963942527203\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5503\n",
      "Episode Count:  1431 \t Cumulative Reward:  4.157000000000001 \t eps:  0.23627312978584677\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1801\n",
      "Episode Count:  1432 \t Cumulative Reward:  24.389999999999993 \t eps:  0.23603685665606092\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4139\n",
      "Episode Count:  1433 \t Cumulative Reward:  28.972 \t eps:  0.23580081979940484\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6677\n",
      "Episode Count:  1434 \t Cumulative Reward:  21.954 \t eps:  0.23556501897960544\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5455\n",
      "Episode Count:  1435 \t Cumulative Reward:  9.258999999999999 \t eps:  0.23532945396062582\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7021\n",
      "Episode Count:  1436 \t Cumulative Reward:  20.358999999999998 \t eps:  0.2350941245066652\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2495\n",
      "Episode Count:  1437 \t Cumulative Reward:  21.295 \t eps:  0.23485903038215852\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.1470\n",
      "Episode Count:  1438 \t Cumulative Reward:  21.161 \t eps:  0.23462417135177638\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8376\n",
      "Episode Count:  1439 \t Cumulative Reward:  25.177 \t eps:  0.2343895471804246\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5470\n",
      "Episode Count:  1440 \t Cumulative Reward:  3.265000000000001 \t eps:  0.23415515763324418\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4824\n",
      "Episode Count:  1441 \t Cumulative Reward:  4.244000000000003 \t eps:  0.23392100247561093\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5866\n",
      "Episode Count:  1442 \t Cumulative Reward:  -0.020999999999998974 \t eps:  0.23368708147313533\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4277\n",
      "Episode Count:  1443 \t Cumulative Reward:  -0.15899999999999748 \t eps:  0.23345339439166218\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4070\n",
      "Episode Count:  1444 \t Cumulative Reward:  -0.008999999999998974 \t eps:  0.23321994099727053\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3234\n",
      "Episode Count:  1445 \t Cumulative Reward:  -0.0389999999999954 \t eps:  0.23298672105627327\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3153\n",
      "Episode Count:  1446 \t Cumulative Reward:  -0.27599999999999775 \t eps:  0.232753734335217\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4879\n",
      "Episode Count:  1447 \t Cumulative Reward:  -0.1349999999999982 \t eps:  0.2325209806008818\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.5710\n",
      "Episode Count:  1448 \t Cumulative Reward:  96.332 \t eps:  0.2322884596202809\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3646\n",
      "Episode Count:  1449 \t Cumulative Reward:  15.673 \t eps:  0.23205617116066063\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.4286\n",
      "Episode Count:  1450 \t Cumulative Reward:  68.90500000000002 \t eps:  0.23182411498949998\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6624\n",
      "Episode Count:  1451 \t Cumulative Reward:  33.37 \t eps:  0.23159229087451047\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3941\n",
      "Episode Count:  1452 \t Cumulative Reward:  19.542999999999996 \t eps:  0.23136069858363595\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9077\n",
      "Episode Count:  1453 \t Cumulative Reward:  48.660000000000004 \t eps:  0.2311293378850523\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6213\n",
      "Episode Count:  1454 \t Cumulative Reward:  13.123999999999999 \t eps:  0.23089820854716725\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8534\n",
      "Episode Count:  1455 \t Cumulative Reward:  22.436 \t eps:  0.2306673103386201\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4713\n",
      "Episode Count:  1456 \t Cumulative Reward:  9.883 \t eps:  0.23043664302828146\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.3249\n",
      "Episode Count:  1457 \t Cumulative Reward:  22.167 \t eps:  0.2302062063852532\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.2841\n",
      "Episode Count:  1458 \t Cumulative Reward:  15.924000000000001 \t eps:  0.22997600017886793\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.5877\n",
      "Episode Count:  1459 \t Cumulative Reward:  13.986999999999995 \t eps:  0.22974602417868906\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7823\n",
      "Episode Count:  1460 \t Cumulative Reward:  26.842 \t eps:  0.22951627815451037\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8352\n",
      "Episode Count:  1461 \t Cumulative Reward:  19.773000000000003 \t eps:  0.22928676187635585\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6206\n",
      "Episode Count:  1462 \t Cumulative Reward:  12.639 \t eps:  0.2290574751144795\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0368\n",
      "Episode Count:  1463 \t Cumulative Reward:  21.223 \t eps:  0.228828417639365\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.8213\n",
      "Episode Count:  1464 \t Cumulative Reward:  22.675999999999995 \t eps:  0.22859958922172566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 2.7750\n",
      "Episode Count:  1465 \t Cumulative Reward:  23.734 \t eps:  0.22837098963250393\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4702\n",
      "Episode Count:  1466 \t Cumulative Reward:  14.472000000000003 \t eps:  0.22814261864287144\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8524\n",
      "Episode Count:  1467 \t Cumulative Reward:  21.738 \t eps:  0.22791447602422857\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5049\n",
      "Episode Count:  1468 \t Cumulative Reward:  24.168999999999993 \t eps:  0.22768656154820435\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1440\n",
      "Episode Count:  1469 \t Cumulative Reward:  7.598000000000002 \t eps:  0.22745887498665615\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8544\n",
      "Episode Count:  1470 \t Cumulative Reward:  18.988 \t eps:  0.2272314161116695\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9955\n",
      "Episode Count:  1471 \t Cumulative Reward:  24.963 \t eps:  0.22700418469555783\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1287\n",
      "Episode Count:  1472 \t Cumulative Reward:  4.551999999999998 \t eps:  0.22677718051086226\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7772\n",
      "Episode Count:  1473 \t Cumulative Reward:  38.835 \t eps:  0.2265504033303514\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8994\n",
      "Episode Count:  1474 \t Cumulative Reward:  28.072 \t eps:  0.22632385292702104\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2170\n",
      "Episode Count:  1475 \t Cumulative Reward:  16.85 \t eps:  0.226097529074094\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3942\n",
      "Episode Count:  1476 \t Cumulative Reward:  91.629 \t eps:  0.2258714315450199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1493\n",
      "Episode Count:  1477 \t Cumulative Reward:  34.471000000000004 \t eps:  0.2256455601134749\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0446\n",
      "Episode Count:  1478 \t Cumulative Reward:  32.935 \t eps:  0.2254199145533614\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0843\n",
      "Episode Count:  1479 \t Cumulative Reward:  41.800000000000004 \t eps:  0.22519449463880806\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.5252\n",
      "Episode Count:  1480 \t Cumulative Reward:  21.445 \t eps:  0.22496930014416924\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.4831\n",
      "Episode Count:  1481 \t Cumulative Reward:  19.724 \t eps:  0.22474433084402506\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9922\n",
      "Episode Count:  1482 \t Cumulative Reward:  12.449 \t eps:  0.22451958651318105\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2254\n",
      "Episode Count:  1483 \t Cumulative Reward:  17.052 \t eps:  0.22429506692666787\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.8939\n",
      "Episode Count:  1484 \t Cumulative Reward:  30.810999999999996 \t eps:  0.2240707718597412\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.2246\n",
      "Episode Count:  1485 \t Cumulative Reward:  15.514000000000003 \t eps:  0.22384670108788146\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.7653\n",
      "Episode Count:  1486 \t Cumulative Reward:  17.487 \t eps:  0.22362285438679358\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.0622\n",
      "Episode Count:  1487 \t Cumulative Reward:  28.964 \t eps:  0.2233992315324068\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.1500\n",
      "Episode Count:  1488 \t Cumulative Reward:  7.943999999999999 \t eps:  0.22317583230087437\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3344\n",
      "Episode Count:  1489 \t Cumulative Reward:  24.035 \t eps:  0.22295265646857348\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6134\n",
      "Episode Count:  1490 \t Cumulative Reward:  24.951000000000004 \t eps:  0.2227297038121049\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.3449\n",
      "Episode Count:  1491 \t Cumulative Reward:  16.753999999999998 \t eps:  0.22250697410829282\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 3.6635\n",
      "Episode Count:  1492 \t Cumulative Reward:  20.537000000000003 \t eps:  0.2222844671341845\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.5618\n",
      "Episode Count:  1493 \t Cumulative Reward:  3.086999999999997 \t eps:  0.22206218266705033\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8513\n",
      "Episode Count:  1494 \t Cumulative Reward:  56.894000000000005 \t eps:  0.22184012048438329\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.6126\n",
      "Episode Count:  1495 \t Cumulative Reward:  14.684999999999999 \t eps:  0.2216182803638989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.6581\n",
      "Episode Count:  1496 \t Cumulative Reward:  11.166 \t eps:  0.221396662083535\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 3.8037\n",
      "Episode Count:  1497 \t Cumulative Reward:  53.27300000000001 \t eps:  0.22117526542145147\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 2.9950\n",
      "Episode Count:  1498 \t Cumulative Reward:  41.21299999999999 \t eps:  0.22095409015603001\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 4.0553\n",
      "Episode Count:  1499 \t Cumulative Reward:  13.644000000000002 \t eps:  0.220733136065874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  ...],\n",
       " [-0.23900000000000055,\n",
       "  13.173,\n",
       "  2.8759999999999994,\n",
       "  7.936000000000002,\n",
       "  42.99,\n",
       "  18.188000000000002,\n",
       "  7.361,\n",
       "  23.512999999999998,\n",
       "  11.088,\n",
       "  10.642,\n",
       "  3.366000000000003,\n",
       "  0.27599999999999775,\n",
       "  23.984000000000012,\n",
       "  1.9989999999999983,\n",
       "  9.217,\n",
       "  26.711000000000002,\n",
       "  6.5980000000000025,\n",
       "  15.086000000000002,\n",
       "  2.9900000000000007,\n",
       "  23.967000000000002,\n",
       "  19.574,\n",
       "  18.74,\n",
       "  32.339,\n",
       "  20.881,\n",
       "  32.827000000000005,\n",
       "  6.651999999999997,\n",
       "  8.604000000000001,\n",
       "  11.163,\n",
       "  22.787,\n",
       "  12.493000000000004,\n",
       "  20.089,\n",
       "  7.895000000000001,\n",
       "  21.015000000000004,\n",
       "  25.336000000000002,\n",
       "  19.666,\n",
       "  12.768000000000002,\n",
       "  24.149999999999995,\n",
       "  3.4349999999999987,\n",
       "  12.124000000000002,\n",
       "  18.813,\n",
       "  11.188000000000002,\n",
       "  17.566000000000006,\n",
       "  5.3069999999999995,\n",
       "  14.963000000000003,\n",
       "  35.613,\n",
       "  18.159000000000006,\n",
       "  14.284999999999997,\n",
       "  18.941999999999997,\n",
       "  13.600999999999999,\n",
       "  8.254000000000001,\n",
       "  14.482999999999993,\n",
       "  26.148999999999997,\n",
       "  8.623000000000006,\n",
       "  15.310000000000002,\n",
       "  35.765,\n",
       "  0.29400000000000226,\n",
       "  59.888,\n",
       "  60.105000000000004,\n",
       "  0.19499999999999643,\n",
       "  29.219,\n",
       "  5.767999999999995,\n",
       "  14.223000000000003,\n",
       "  15.62,\n",
       "  8.962,\n",
       "  16.117,\n",
       "  12.920000000000002,\n",
       "  12.716000000000001,\n",
       "  6.708999999999999,\n",
       "  11.017,\n",
       "  7.860000000000001,\n",
       "  12.572000000000001,\n",
       "  20.928,\n",
       "  10.382,\n",
       "  4.571000000000003,\n",
       "  -0.0930000000000018,\n",
       "  -0.21599999999999886,\n",
       "  8.13,\n",
       "  0.5699999999999997,\n",
       "  24.181999999999995,\n",
       "  8.657,\n",
       "  15.995000000000003,\n",
       "  14.560999999999998,\n",
       "  10.277999999999999,\n",
       "  18.656000000000006,\n",
       "  12.746000000000004,\n",
       "  6.144000000000008,\n",
       "  9.514,\n",
       "  38.602999999999994,\n",
       "  24.044,\n",
       "  8.054,\n",
       "  0.5089999999999977,\n",
       "  65.019,\n",
       "  21.688000000000002,\n",
       "  34.287000000000006,\n",
       "  35.53999999999999,\n",
       "  10.176000000000002,\n",
       "  5.690999999999998,\n",
       "  0.49299999999999944,\n",
       "  8.095,\n",
       "  10.752,\n",
       "  13.176000000000002,\n",
       "  13.163000000000002,\n",
       "  5.384,\n",
       "  0.628000000000002,\n",
       "  4.001000000000005,\n",
       "  0.5300000000000015,\n",
       "  9.777000000000003,\n",
       "  6.435,\n",
       "  5.798999999999996,\n",
       "  19.153,\n",
       "  56.84300000000002,\n",
       "  18.130000000000003,\n",
       "  10.763000000000002,\n",
       "  14.497999999999998,\n",
       "  15.075000000000001,\n",
       "  10.583999999999998,\n",
       "  16.283,\n",
       "  19.982999999999997,\n",
       "  32.717999999999996,\n",
       "  78.59000000000002,\n",
       "  8.174000000000003,\n",
       "  8.951,\n",
       "  10.323000000000002,\n",
       "  5.030000000000001,\n",
       "  5.274,\n",
       "  19.776000000000003,\n",
       "  5.843000000000001,\n",
       "  -3.317000000000002,\n",
       "  13.846,\n",
       "  11.001000000000001,\n",
       "  0.2979999999999998,\n",
       "  61.38599999999999,\n",
       "  15.431,\n",
       "  13.723000000000006,\n",
       "  21.714,\n",
       "  17.590999999999998,\n",
       "  0.08000000000000174,\n",
       "  0.4410000000000008,\n",
       "  -25.249,\n",
       "  10.975999999999999,\n",
       "  17.040000000000003,\n",
       "  7.403000000000001,\n",
       "  39.399000000000015,\n",
       "  7.368999999999996,\n",
       "  14.665000000000003,\n",
       "  5.302000000000002,\n",
       "  19.665,\n",
       "  19.202,\n",
       "  31.907000000000004,\n",
       "  4.043999999999999,\n",
       "  10.112999999999998,\n",
       "  19.583000000000006,\n",
       "  15.478000000000002,\n",
       "  -0.027000000000001793,\n",
       "  21.706,\n",
       "  9.765000000000002,\n",
       "  8.829000000000002,\n",
       "  21.075,\n",
       "  22.395,\n",
       "  0.0030000000000022925,\n",
       "  0.3580000000000038,\n",
       "  12.141000000000002,\n",
       "  6.23,\n",
       "  6.579,\n",
       "  8.232000000000005,\n",
       "  94.70400000000001,\n",
       "  21.161000000000005,\n",
       "  16.916,\n",
       "  32.452999999999996,\n",
       "  9.397,\n",
       "  8.635,\n",
       "  5.929000000000002,\n",
       "  13.711999999999996,\n",
       "  6.731000000000001,\n",
       "  35.888,\n",
       "  8.271,\n",
       "  4.899000000000003,\n",
       "  7.518999999999988,\n",
       "  19.109,\n",
       "  6.434,\n",
       "  5.699000000000002,\n",
       "  29.545000000000005,\n",
       "  4.846999999999998,\n",
       "  6.837999999999999,\n",
       "  45.509,\n",
       "  16.083000000000002,\n",
       "  0.14800000000000152,\n",
       "  4.822000000000001,\n",
       "  32.434,\n",
       "  6.862999999999999,\n",
       "  4.551,\n",
       "  9.483,\n",
       "  17.680000000000003,\n",
       "  42.308,\n",
       "  25.358,\n",
       "  7.796999999999998,\n",
       "  18.149,\n",
       "  16.246,\n",
       "  12.367999999999999,\n",
       "  3.9799999999999995,\n",
       "  25.467000000000002,\n",
       "  52.07,\n",
       "  13.388000000000003,\n",
       "  28.518,\n",
       "  -5.394000000000002,\n",
       "  6.291000000000002,\n",
       "  32.119,\n",
       "  6.7020000000000035,\n",
       "  13.565,\n",
       "  11.234000000000004,\n",
       "  7.773999999999998,\n",
       "  14.839999999999996,\n",
       "  19.663,\n",
       "  17.502000000000002,\n",
       "  -0.18499999999999964,\n",
       "  39.38000000000001,\n",
       "  6.489999999999999,\n",
       "  7.467,\n",
       "  7.392000000000003,\n",
       "  3.272000000000001,\n",
       "  16.668000000000003,\n",
       "  15.970999999999998,\n",
       "  62.181,\n",
       "  15.754000000000001,\n",
       "  12.049,\n",
       "  7.253999999999999,\n",
       "  9.155,\n",
       "  20.817,\n",
       "  32.961000000000006,\n",
       "  19.441,\n",
       "  11.207000000000006,\n",
       "  5.049999999999999,\n",
       "  7.553000000000002,\n",
       "  11.226000000000003,\n",
       "  20.533,\n",
       "  21.387,\n",
       "  8.607999999999997,\n",
       "  7.0390000000000015,\n",
       "  17.874,\n",
       "  19.573999999999998,\n",
       "  13.737,\n",
       "  12.770999999999999,\n",
       "  14.055999999999997,\n",
       "  20.17,\n",
       "  18.896,\n",
       "  13.641000000000002,\n",
       "  17.352,\n",
       "  14.972000000000003,\n",
       "  18.618000000000002,\n",
       "  35.33400000000001,\n",
       "  18.196,\n",
       "  20.946999999999996,\n",
       "  8.218,\n",
       "  5.032000000000001,\n",
       "  12.256999999999998,\n",
       "  6.778999999999998,\n",
       "  14.279999999999998,\n",
       "  17.363,\n",
       "  47.17400000000001,\n",
       "  22.529999999999998,\n",
       "  15.175999999999997,\n",
       "  6.874999999999999,\n",
       "  23.171,\n",
       "  17.884999999999994,\n",
       "  7.996,\n",
       "  14.987,\n",
       "  23.639999999999997,\n",
       "  24.279,\n",
       "  16.108,\n",
       "  19.573999999999998,\n",
       "  57.713000000000015,\n",
       "  34.3,\n",
       "  12.870000000000001,\n",
       "  15.563999999999997,\n",
       "  17.346000000000004,\n",
       "  20.732,\n",
       "  29.261,\n",
       "  5.862,\n",
       "  -0.010999999999999503,\n",
       "  17.583000000000002,\n",
       "  14.668000000000001,\n",
       "  12.292999999999997,\n",
       "  47.633,\n",
       "  23.505,\n",
       "  9.223,\n",
       "  9.454999999999998,\n",
       "  15.672999999999996,\n",
       "  36.159,\n",
       "  13.466999999999997,\n",
       "  18.47,\n",
       "  6.824000000000001,\n",
       "  49.395,\n",
       "  26.933000000000003,\n",
       "  0.9459999999999964,\n",
       "  34.24500000000001,\n",
       "  8.873,\n",
       "  12.337,\n",
       "  9.228999999999996,\n",
       "  31.757,\n",
       "  6.255,\n",
       "  25.468,\n",
       "  7.042999999999999,\n",
       "  5.969999999999997,\n",
       "  7.464,\n",
       "  8.056,\n",
       "  32.41900000000002,\n",
       "  62.052,\n",
       "  37.129,\n",
       "  3.282000000000001,\n",
       "  33.76900000000002,\n",
       "  18.372000000000003,\n",
       "  14.940000000000005,\n",
       "  22.591,\n",
       "  21.900000000000006,\n",
       "  21.589,\n",
       "  17.05,\n",
       "  14.591000000000003,\n",
       "  9.64100000000001,\n",
       "  9.638,\n",
       "  7.390999999999998,\n",
       "  21.463,\n",
       "  13.481999999999996,\n",
       "  26.962999999999997,\n",
       "  36.309000000000005,\n",
       "  4.491000000000001,\n",
       "  17.431,\n",
       "  7.865,\n",
       "  12.756000000000002,\n",
       "  35.19,\n",
       "  38.726000000000006,\n",
       "  7.785,\n",
       "  21.802999999999997,\n",
       "  9.581000000000001,\n",
       "  -0.0869999999999982,\n",
       "  -0.060000000000002815,\n",
       "  -16.897,\n",
       "  8.561000000000003,\n",
       "  -0.10800000000000039,\n",
       "  44.33,\n",
       "  5.789000000000001,\n",
       "  7.562000000000003,\n",
       "  18.47,\n",
       "  8.438,\n",
       "  10.55,\n",
       "  -1.5929999999999955,\n",
       "  14.299000000000005,\n",
       "  14.145,\n",
       "  24.926,\n",
       "  19.31,\n",
       "  29.90400000000001,\n",
       "  38.20799999999999,\n",
       "  0.611999999999999,\n",
       "  6.671000000000004,\n",
       "  8.654000000000002,\n",
       "  22.867,\n",
       "  36.602,\n",
       "  11.874999999999998,\n",
       "  6.019,\n",
       "  19.354999999999997,\n",
       "  7.916000000000001,\n",
       "  14.218000000000002,\n",
       "  24.442000000000004,\n",
       "  8.866,\n",
       "  6.679,\n",
       "  4.705999999999999,\n",
       "  7.558000000000003,\n",
       "  9.035,\n",
       "  21.589,\n",
       "  6.881000000000001,\n",
       "  7.1190000000000015,\n",
       "  8.591,\n",
       "  3.7370000000000005,\n",
       "  15.217,\n",
       "  16.134999999999998,\n",
       "  18.929,\n",
       "  -0.10500000000000219,\n",
       "  12.328000000000003,\n",
       "  10.166999999999998,\n",
       "  7.382999999999995,\n",
       "  22.316,\n",
       "  12.210999999999993,\n",
       "  58.062,\n",
       "  16.905,\n",
       "  11.405,\n",
       "  20.014,\n",
       "  14.411999999999997,\n",
       "  27.008000000000006,\n",
       "  18.526000000000003,\n",
       "  20.714000000000006,\n",
       "  17.899999999999995,\n",
       "  15.672999999999998,\n",
       "  3.928999999999993,\n",
       "  36.730999999999995,\n",
       "  4.234999999999999,\n",
       "  -0.08700000000000105,\n",
       "  12.533000000000001,\n",
       "  7.976999999999999,\n",
       "  6.099,\n",
       "  3.392000000000001,\n",
       "  11.740999999999998,\n",
       "  23.756000000000004,\n",
       "  35.538,\n",
       "  24.924000000000007,\n",
       "  11.052,\n",
       "  34.391000000000005,\n",
       "  22.860999999999994,\n",
       "  7.606000000000001,\n",
       "  11.160999999999998,\n",
       "  35.577000000000005,\n",
       "  5.960000000000001,\n",
       "  11.166999999999996,\n",
       "  14.503000000000004,\n",
       "  6.562000000000001,\n",
       "  17.84,\n",
       "  12.904000000000002,\n",
       "  21.41200000000001,\n",
       "  6.472000000000002,\n",
       "  14.234000000000002,\n",
       "  15.288000000000002,\n",
       "  14.126000000000001,\n",
       "  14.744999999999997,\n",
       "  25.065,\n",
       "  13.813000000000004,\n",
       "  8.537,\n",
       "  16.386000000000003,\n",
       "  36.92500000000001,\n",
       "  11.462000000000002,\n",
       "  14.782,\n",
       "  18.704,\n",
       "  7.699000000000002,\n",
       "  15.483000000000002,\n",
       "  30.761,\n",
       "  16.622,\n",
       "  19.328,\n",
       "  6.494000000000002,\n",
       "  1.5979999999999994,\n",
       "  11.626000000000003,\n",
       "  7.739999999999999,\n",
       "  29.585,\n",
       "  29.206999999999997,\n",
       "  7.698999999999999,\n",
       "  8.499999999999998,\n",
       "  9.554999999999998,\n",
       "  6.076000000000005,\n",
       "  11.304000000000002,\n",
       "  13.665999999999999,\n",
       "  41.46900000000001,\n",
       "  -0.00900000000000089,\n",
       "  4.437000000000003,\n",
       "  1.0850000000000004,\n",
       "  1.0609999999999973,\n",
       "  0.8130000000000025,\n",
       "  -6.910000000000002,\n",
       "  -0.1169999999999986,\n",
       "  12.427999999999997,\n",
       "  3.347999999999999,\n",
       "  5.091,\n",
       "  6.570000000000004,\n",
       "  -0.036,\n",
       "  3.368000000000004,\n",
       "  16.552999999999997,\n",
       "  9.629999999999997,\n",
       "  16.222,\n",
       "  0.20599999999999638,\n",
       "  -0.2819999999999977,\n",
       "  11.092000000000002,\n",
       "  19.922,\n",
       "  22.339999999999996,\n",
       "  -0.19200000000000206,\n",
       "  12.092999999999998,\n",
       "  27.691000000000003,\n",
       "  15.527000000000005,\n",
       "  10.143000000000002,\n",
       "  8.737,\n",
       "  7.345000000000002,\n",
       "  6.032000000000001,\n",
       "  4.390999999999998,\n",
       "  6.932000000000002,\n",
       "  31.677999999999997,\n",
       "  12.605000000000006,\n",
       "  16.103999999999996,\n",
       "  26.698000000000004,\n",
       "  16.526,\n",
       "  23.091,\n",
       "  27.889999999999993,\n",
       "  19.757999999999996,\n",
       "  15.924000000000001,\n",
       "  17.796,\n",
       "  25.177999999999997,\n",
       "  28.334,\n",
       "  13.652,\n",
       "  34.481,\n",
       "  24.322,\n",
       "  22.410000000000004,\n",
       "  21.054000000000006,\n",
       "  13.015999999999998,\n",
       "  8.173999999999998,\n",
       "  33.748000000000005,\n",
       "  16.936000000000007,\n",
       "  8.058000000000003,\n",
       "  0.48400000000000154,\n",
       "  8.501,\n",
       "  53.224999999999994,\n",
       "  23.391,\n",
       "  25.563000000000006,\n",
       "  22.562000000000005,\n",
       "  2.1720000000000006,\n",
       "  12.885000000000005,\n",
       "  15.493000000000002,\n",
       "  35.577999999999996,\n",
       "  7.594,\n",
       "  17.65,\n",
       "  20.175,\n",
       "  25.208,\n",
       "  26.050000000000004,\n",
       "  21.956000000000003,\n",
       "  10.252999999999998,\n",
       "  -0.17400000000000118,\n",
       "  16.171,\n",
       "  20.067000000000007,\n",
       "  6.710000000000001,\n",
       "  10.767000000000005,\n",
       "  28.493000000000002,\n",
       "  13.854999999999997,\n",
       "  -0.18299999999999744,\n",
       "  31.096000000000004,\n",
       "  1.1220000000000003,\n",
       "  22.376000000000005,\n",
       "  19.326000000000004,\n",
       "  16.342,\n",
       "  23.355,\n",
       "  36.208,\n",
       "  21.643000000000004,\n",
       "  25.291999999999998,\n",
       "  17.994999999999997,\n",
       "  12.459000000000003,\n",
       "  14.847999999999999,\n",
       "  22.513999999999996,\n",
       "  19.77,\n",
       "  10.237,\n",
       "  13.921999999999997,\n",
       "  24.849000000000004,\n",
       "  61.139,\n",
       "  24.323000000000008,\n",
       "  24.16,\n",
       "  13.473000000000003,\n",
       "  20.543999999999993,\n",
       "  11.709000000000001,\n",
       "  11.463,\n",
       "  10.850000000000001,\n",
       "  36.29299999999999,\n",
       "  21.368,\n",
       "  28.868000000000002,\n",
       "  25.639,\n",
       "  18.320999999999994,\n",
       "  29.998,\n",
       "  14.258000000000003,\n",
       "  30.582,\n",
       "  9.767000000000001,\n",
       "  30.553999999999995,\n",
       "  21.019000000000002,\n",
       "  17.617,\n",
       "  6.894000000000002,\n",
       "  9.806000000000001,\n",
       "  9.515,\n",
       "  5.916000000000002,\n",
       "  21.849999999999998,\n",
       "  20.699999999999996,\n",
       "  9.219999999999997,\n",
       "  10.691,\n",
       "  12.463000000000003,\n",
       "  13.631000000000004,\n",
       "  14.596,\n",
       "  13.134000000000002,\n",
       "  10.615000000000006,\n",
       "  24.105999999999998,\n",
       "  31.39,\n",
       "  50.525000000000006,\n",
       "  20.008000000000003,\n",
       "  40.227999999999994,\n",
       "  22.928,\n",
       "  53.145,\n",
       "  16.483,\n",
       "  6.949000000000001,\n",
       "  31.860999999999994,\n",
       "  17.174999999999997,\n",
       "  43.359,\n",
       "  50.687999999999995,\n",
       "  9.776,\n",
       "  18.929000000000002,\n",
       "  8.020999999999999,\n",
       "  35.583,\n",
       "  13.014000000000001,\n",
       "  10.812,\n",
       "  10.586,\n",
       "  29.84,\n",
       "  40.332000000000015,\n",
       "  23.087000000000007,\n",
       "  33.734,\n",
       "  20.098999999999997,\n",
       "  25.57,\n",
       "  22.28,\n",
       "  18.659,\n",
       "  16.665,\n",
       "  12.262999999999998,\n",
       "  20.945000000000004,\n",
       "  10.268999999999997,\n",
       "  9.658,\n",
       "  11.945999999999998,\n",
       "  29.775000000000006,\n",
       "  19.982,\n",
       "  5.089999999999997,\n",
       "  16.192999999999998,\n",
       "  6.509999999999997,\n",
       "  3.6380000000000035,\n",
       "  7.261000000000002,\n",
       "  6.717999999999999,\n",
       "  20.697000000000003,\n",
       "  4.5760000000000005,\n",
       "  -0.15000000000000283,\n",
       "  5.166999999999999,\n",
       "  8.853,\n",
       "  16.095,\n",
       "  12.173000000000002,\n",
       "  7.336999999999999,\n",
       "  22.223000000000003,\n",
       "  5.096999999999998,\n",
       "  25.452,\n",
       "  10.292,\n",
       "  13.823000000000004,\n",
       "  25.816000000000003,\n",
       "  9.591999999999999,\n",
       "  9.401,\n",
       "  19.987000000000002,\n",
       "  15.476000000000004,\n",
       "  13.813,\n",
       "  15.896999999999998,\n",
       "  -0.04800000000000116,\n",
       "  -0.2819999999999989,\n",
       "  10.704000000000002,\n",
       "  12.898999999999997,\n",
       "  11.925,\n",
       "  21.308,\n",
       "  3.0660000000000016,\n",
       "  30.464999999999996,\n",
       "  21.859000000000005,\n",
       "  38.929,\n",
       "  9.721,\n",
       "  5.718000000000001,\n",
       "  -0.07800000000000358,\n",
       "  -1.0050000000000003,\n",
       "  -0.1200000000000036,\n",
       "  31.425,\n",
       "  33.431000000000004,\n",
       "  9.465999999999996,\n",
       "  16.592999999999996,\n",
       "  7.111,\n",
       "  2.2280000000000024,\n",
       "  -0.06899999999999859,\n",
       "  22.997,\n",
       "  16.336,\n",
       "  5.442000000000001,\n",
       "  5.982,\n",
       "  13.099999999999998,\n",
       "  21.864000000000004,\n",
       "  22.252000000000002,\n",
       "  13.364999999999998,\n",
       "  9.221999999999998,\n",
       "  18.932000000000002,\n",
       "  16.439000000000004,\n",
       "  7.483,\n",
       "  13.852000000000004,\n",
       "  14.017999999999997,\n",
       "  43.69199999999999,\n",
       "  16.657000000000004,\n",
       "  13.871000000000006,\n",
       "  13.992000000000003,\n",
       "  39.12500000000001,\n",
       "  19.516,\n",
       "  -0.08000000000000014,\n",
       "  8.531,\n",
       "  59.33399999999999,\n",
       "  19.697000000000003,\n",
       "  59.789,\n",
       "  20.641,\n",
       "  6.591999999999999,\n",
       "  32.257999999999996,\n",
       "  35.125000000000014,\n",
       "  11.059999999999999,\n",
       "  31.905000000000005,\n",
       "  14.880999999999998,\n",
       "  26.014999999999997,\n",
       "  32.282000000000004,\n",
       "  17.659,\n",
       "  41.85900000000001,\n",
       "  21.616999999999997,\n",
       "  3.2359999999999993,\n",
       "  0.09000000000000183,\n",
       "  -0.060000000000002815,\n",
       "  -15.165000000000001,\n",
       "  0.5750000000000015,\n",
       "  0.4499999999999938,\n",
       "  19.008,\n",
       "  25.56800000000001,\n",
       "  29.092,\n",
       "  23.028,\n",
       "  10.985000000000001,\n",
       "  31.925,\n",
       "  5.420999999999999,\n",
       "  25.648,\n",
       "  15.977999999999994,\n",
       "  33.77499999999999,\n",
       "  40.754,\n",
       "  25.105999999999998,\n",
       "  8.878000000000004,\n",
       "  9.026999999999997,\n",
       "  15.728,\n",
       "  6.008999999999999,\n",
       "  -0.1189999999999985,\n",
       "  11.524000000000001,\n",
       "  -0.20399999999999882,\n",
       "  16.768,\n",
       "  27.247,\n",
       "  14.044999999999995,\n",
       "  25.573,\n",
       "  7.6659999999999995,\n",
       "  10.004999999999999,\n",
       "  17.653,\n",
       "  24.452,\n",
       "  2.6810000000000027,\n",
       "  -0.04200000000000001,\n",
       "  17.809000000000005,\n",
       "  21.697000000000003,\n",
       "  15.443999999999999,\n",
       "  14.020999999999999,\n",
       "  30.875999999999998,\n",
       "  0.969,\n",
       "  -0.26399999999999924,\n",
       "  0.30800000000000055,\n",
       "  7.983000000000001,\n",
       "  10.339999999999998,\n",
       "  -0.17999999999999924,\n",
       "  -0.0389999999999954,\n",
       "  8.908000000000003,\n",
       "  5.006,\n",
       "  7.170000000000001,\n",
       "  10.499000000000004,\n",
       "  -0.024000000000003588,\n",
       "  -12.201000000000002,\n",
       "  8.496,\n",
       "  -0.29300000000000054,\n",
       "  -0.011000000000000003,\n",
       "  -0.07999999999999897,\n",
       "  -0.1140000000000036,\n",
       "  14.553999999999998,\n",
       "  17.188,\n",
       "  3.9729999999999994,\n",
       "  17.046999999999997,\n",
       "  15.29,\n",
       "  20.811999999999998,\n",
       "  22.463,\n",
       "  41.012,\n",
       "  23.029,\n",
       "  49.632000000000005,\n",
       "  17.964,\n",
       "  14.354,\n",
       "  41.949,\n",
       "  34.211000000000006,\n",
       "  21.79,\n",
       "  19.061,\n",
       "  18.248000000000008,\n",
       "  8.110000000000001,\n",
       "  37.52,\n",
       "  6.627999999999999,\n",
       "  10.15,\n",
       "  16.55,\n",
       "  26.714999999999996,\n",
       "  7.732000000000003,\n",
       "  18.488000000000003,\n",
       "  11.169,\n",
       "  30.73200000000001,\n",
       "  20.887,\n",
       "  16.674999999999997,\n",
       "  12.944,\n",
       "  14.392000000000001,\n",
       "  20.873,\n",
       "  27.684,\n",
       "  13.570000000000002,\n",
       "  22.064000000000004,\n",
       "  17.008,\n",
       "  10.387,\n",
       "  8.996999999999998,\n",
       "  4.889999999999999,\n",
       "  7.612000000000001,\n",
       "  8.532,\n",
       "  8.186000000000002,\n",
       "  30.03400000000001,\n",
       "  25.158,\n",
       "  21.354000000000003,\n",
       "  45.891999999999996,\n",
       "  27.697000000000003,\n",
       "  16.392,\n",
       "  15.006,\n",
       "  40.495,\n",
       "  14.407000000000002,\n",
       "  17.584,\n",
       "  11.068000000000001,\n",
       "  45.55200000000001,\n",
       "  9.621999999999996,\n",
       "  11.954999999999998,\n",
       "  14.276999999999997,\n",
       "  11.240999999999998,\n",
       "  16.692999999999998,\n",
       "  29.506000000000007,\n",
       "  19.048000000000002,\n",
       "  36.282,\n",
       "  22.281000000000002,\n",
       "  25.164,\n",
       "  15.557000000000006,\n",
       "  25.200999999999997,\n",
       "  15.136999999999997,\n",
       "  47.87600000000001,\n",
       "  34.57300000000001,\n",
       "  18.504999999999995,\n",
       "  20.590000000000007,\n",
       "  39.04600000000001,\n",
       "  22.202,\n",
       "  15.366999999999999,\n",
       "  26.138,\n",
       "  32.524,\n",
       "  37.232,\n",
       "  0.2670000000000189,\n",
       "  19.604999999999997,\n",
       "  15.547000000000002,\n",
       "  22.159,\n",
       "  11.101,\n",
       "  16.236,\n",
       "  18.096999999999994,\n",
       "  20.683999999999997,\n",
       "  9.756999999999998,\n",
       "  12.05,\n",
       "  17.857,\n",
       "  5.250000000000002,\n",
       "  1.952999999999997,\n",
       "  16.462,\n",
       "  13.393999999999998,\n",
       "  10.579999999999997,\n",
       "  14.446,\n",
       "  19.542,\n",
       "  12.200000000000003,\n",
       "  9.517,\n",
       "  21.251999999999995,\n",
       "  1.7890000000000001,\n",
       "  5.5329999999999995,\n",
       "  10.656,\n",
       "  7.362000000000001,\n",
       "  49.947,\n",
       "  16.837999999999997,\n",
       "  15.852,\n",
       "  5.394999999999999,\n",
       "  6.908,\n",
       "  13.864,\n",
       "  11.949000000000002,\n",
       "  7.966000000000002,\n",
       "  55.684,\n",
       "  6.563000000000001,\n",
       "  6.7559999999999985,\n",
       "  14.161999999999999,\n",
       "  22.761,\n",
       "  10.690000000000001,\n",
       "  28.458,\n",
       "  8.123000000000001,\n",
       "  16.712000000000007,\n",
       "  37.136,\n",
       "  21.406999999999996,\n",
       "  38.08800000000001,\n",
       "  21.832,\n",
       "  31.421,\n",
       "  14.323999999999998,\n",
       "  26.699,\n",
       "  15.716999999999999,\n",
       "  7.331999999999999,\n",
       "  13.113,\n",
       "  39.05499999999999,\n",
       "  19.030999999999995,\n",
       "  33.703,\n",
       "  10.959000000000001,\n",
       "  22.135,\n",
       "  0.3289999999999994,\n",
       "  12.97,\n",
       "  28.301000000000002,\n",
       "  8.948000000000002,\n",
       "  17.758999999999997,\n",
       "  39.164,\n",
       "  42.449000000000005,\n",
       "  55.35500000000001,\n",
       "  12.742999999999999,\n",
       "  6.219,\n",
       "  23.943,\n",
       "  10.98,\n",
       "  12.968999999999998,\n",
       "  11.292,\n",
       "  4.4990000000000006,\n",
       "  5.479000000000002,\n",
       "  10.734999999999998,\n",
       "  17.384,\n",
       "  8.245000000000003,\n",
       "  35.843,\n",
       "  8.885000000000005,\n",
       "  16.758,\n",
       "  15.904,\n",
       "  16.31,\n",
       "  23.673000000000002,\n",
       "  12.410999999999996,\n",
       "  9.968,\n",
       "  51.08700000000001,\n",
       "  13.586,\n",
       "  16.070999999999998,\n",
       "  6.733000000000001,\n",
       "  8.126000000000001,\n",
       "  13.096000000000004,\n",
       "  14.997000000000003,\n",
       "  17.582000000000004,\n",
       "  20.085000000000004,\n",
       "  15.939000000000004,\n",
       "  50.346000000000004,\n",
       "  9.330999999999998,\n",
       "  19.455,\n",
       "  7.897000000000004,\n",
       "  5.37300000000007,\n",
       "  6.369000000000001,\n",
       "  10.034,\n",
       "  10.95,\n",
       "  0.2790000000000029,\n",
       "  -0.11699999999999541,\n",
       "  9.200999999999997,\n",
       "  -0.14999999999999641,\n",
       "  3.997999999999999,\n",
       "  -0.02399999999999719,\n",
       "  4.047999999999999,\n",
       "  -0.045000000000004606,\n",
       "  6.123999999999999,\n",
       "  9.421999999999999,\n",
       "  -0.15599999999999925,\n",
       "  11.332,\n",
       "  6.497,\n",
       "  0.4099999999999984,\n",
       "  61.505,\n",
       "  4.397999999999997,\n",
       "  15.930000000000001,\n",
       "  20.938,\n",
       "  36.900999999999996,\n",
       "  38.393,\n",
       "  5.583,\n",
       "  8.727000000000002,\n",
       "  6.894999999999999,\n",
       "  7.850000000000003,\n",
       "  45.657,\n",
       "  23.767000000000003,\n",
       "  14.935000000000002,\n",
       "  32.171,\n",
       "  30.666999999999998,\n",
       "  0.8179999999999998,\n",
       "  -0.08099999999999898,\n",
       "  14.847999999999999,\n",
       "  29.330999999999996,\n",
       "  10.332,\n",
       "  18.555000000000003,\n",
       "  19.467,\n",
       "  31.090000000000003,\n",
       "  10.720999999999998,\n",
       "  29.657000000000007,\n",
       "  19.687,\n",
       "  33.323,\n",
       "  10.578,\n",
       "  22.558000000000003,\n",
       "  26.579,\n",
       "  8.777000000000005,\n",
       "  9.984000000000002,\n",
       "  48.364000000000004,\n",
       "  96.262,\n",
       "  22.321,\n",
       "  35.533,\n",
       "  42.998000000000005,\n",
       "  12.930000000000005,\n",
       "  61.315,\n",
       "  16.474,\n",
       "  24.120000000000005,\n",
       "  8.581999999999999,\n",
       "  19.525000000000002,\n",
       "  17.613999999999997,\n",
       "  13.873999999999997,\n",
       "  6.205000000000001,\n",
       "  7.0329999999999995,\n",
       "  16.673999999999996,\n",
       "  21.878000000000004,\n",
       "  10.545000000000002,\n",
       "  21.955999999999996,\n",
       "  14.594999999999999,\n",
       "  30.275999999999993,\n",
       "  ...],\n",
       " <__main__.NetworkTracker at 0x7fb7f784d410>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agent(contd = True,\n",
    "            verbose=True, \n",
    "            num_episodes=4000,\n",
    "            discount = 0.99, \n",
    "            batch_size = 512, \n",
    "            N = 100, # how often to clone the target policy\n",
    "            memory_size = 2048,\n",
    "            eps_decay_rate=0.999,\n",
    "            max_exp_rate=0.2, \n",
    "            min_exp_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# print(lmao[10][0][0].shape)\n",
    "# f = lmao[10][0][0].transpose((2, 0, 1))\n",
    "# f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
