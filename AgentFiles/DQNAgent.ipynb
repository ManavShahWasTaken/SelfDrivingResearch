{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z6tk-uX9bwY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import socket\n",
    "# import shutil\n",
    "# from skimage import io\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state_data):\n",
    "        self.state_data = np.asarray(state_data)\n",
    "  \n",
    "    def process_state(self):\n",
    "        pass\n",
    "  \n",
    "    def get_batch_tensor(self):\n",
    "        holder = np.asarray(self.state_data)\n",
    "        holder.reshape((1, ) + holder.shape)\n",
    "        return holder\n",
    "  \n",
    "    def get_individual_tensor(self):\n",
    "        return np.asarray(self.state_data)\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.state_data.shape\n",
    "  \n",
    "    def display(self):\n",
    "        print(self.state_data)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class Frame(State):\n",
    "    def __init__(self, state_data, crop_factor=None, destination_size=None, vert_cent=0.5):\n",
    "        State.__init__(self, state_data)\n",
    "#         self.state_data = self.process_state(crop_factor, vert_cent, destination_shape)\n",
    "        self.state_data = self.process_state([0.7, 1.0], 0.7, (128,64))\n",
    "  \n",
    "    def process_state(self, crop_factor, vert_cent, destination_shape):\n",
    "        \"\"\"\n",
    "        Does all the processing of the frame using the helper functions\n",
    "        \"\"\"\n",
    "        frame = self.crop_frame(self.state_data, crop_factor, vert_cent)\n",
    "        frame = self.normalise_frame(frame)\n",
    "        frame = self.gray_scale(frame) # cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        assert len(frame.shape) == 2\n",
    "        frame = self.downsample_frame(frame, destination_shape)\n",
    "        return frame\n",
    "\n",
    "  \n",
    "    def gray_scale(self, frame, gray_scale_factor=[0.3, 0.3, 0.3]):\n",
    "        frame = np.dot(frame, np.asarray(gray_scale_factor))\n",
    "        return frame\n",
    "\n",
    "    def normalise_frame(self, frame):\n",
    "        frame = frame.astype('float32') / 255.0\n",
    "        return frame\n",
    "  \n",
    "    def downsample_frame(self, frame, destination_shape):\n",
    "        \"\"\"\n",
    "        downsamples the frame. decreases the resolution\n",
    "        \"\"\"\n",
    "        frame = cv2.resize(np.asarray(frame), dsize=destination_shape, interpolation=cv2.INTER_CUBIC)\n",
    "        return frame\n",
    "  \n",
    "    def crop_frame(self, frame, crop_factor, vert_cent):\n",
    "        \"\"\"\n",
    "        input is the frame\n",
    "        output is the cropped frame\n",
    "        crop_factor is the ratio at which you want to crop the height and width(0.8, 0.8)\n",
    "        cent is the ratio at which the centre of the cropped frame should be\n",
    "        \"\"\"\n",
    "        if crop_factor is None:\n",
    "          return frame\n",
    "    \n",
    "        height_factor = int((crop_factor[0]*frame.shape[0]) // 2)\n",
    "        width_factor = int((crop_factor[1]*frame.shape[1]) // 2)\n",
    "        vert_cent = int(frame.shape[0]*vert_cent)\n",
    "        width_cent = int(frame.shape[1]*0.5)\n",
    "\n",
    "        frame = frame[vert_cent - height_factor: vert_cent + height_factor, \n",
    "                      width_cent - width_factor: width_cent + width_factor]\n",
    "        return frame\n",
    "\n",
    "# ------------------------------------\n",
    "class DataBuffer:\n",
    "    \"\"\"\n",
    "    Keeps track of n latest states \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, size=1):\n",
    "        self.buffer = []\n",
    "        self.size = size\n",
    "\n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        arr = np.array(self.buffer)\n",
    "        if self.size == 1 or in_batch:\n",
    "            return arr\n",
    "        else:\n",
    "            return arr.reshape((1, ) + arr.shape)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return np.asarray(self.current_state[0]).shape\n",
    "\n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(state)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class FrameBuffer(DataBuffer):\n",
    "    def __init__(self, size = 4):\n",
    "        DataBuffer.__init__(self, size=size)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.get_input_tensor().shape\n",
    "  \n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        temp = np.array(self.buffer)\n",
    "        return  temp.transpose((1, 2, 0))\n",
    "    \n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        # if buffer not initialised\n",
    "        if len(self.buffer) == 0:\n",
    "            self.buffer = [state]\n",
    "            return\n",
    "\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.buffer.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh-452ClVFpT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # initialise comms with the simulator here\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # initialise the socket\n",
    "        # connect to localhost, port 2345 \n",
    "        self.sock.bind((\"127.0.0.1\", 4444))\n",
    "        self.sock.listen(1)\n",
    "        print(\"Waiting to connect to Simulator...\")\n",
    "        self.clientsocket, _ = self.sock.accept() # connect to simulator [BLOCKING]\n",
    "        print(\"Connected to simulator!\")\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # initialising frame buffer\n",
    "        self.buffer_size = 4 # could change this \n",
    "        \n",
    "         # this is the FrameBuffer that keeps track of the latest frames.\n",
    "         #initialising the frame buffer by just giving it multiple copies of the same starting frame\n",
    "        # =========================================\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        self.current_buffer = None\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        \n",
    "        self.time_steps = 0\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        self.max_time_steps_per_episode = 500 #change this based on the enviorment\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # Create target directory if it doesn't exist\n",
    "       \n",
    "        parent_path = os.path.abspath(os.path.join(\"\", os.pardir))\n",
    "        self.final_path = parent_path + \"/simulation/Screenshots\"\n",
    "        if not os.path.exists(self.final_path):\n",
    "            os.mkdir(self.final_path)\n",
    "            print(\"Directory \" , self.final_path,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , self.final_path,  \" already exists\")\n",
    "            \n",
    "        \n",
    "        # reset actually initializes self.current_state, self.current_buffer etc.\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.action_space = ['as', 'ar', 'al', 'ds', 'dr', 'dl', 'bs', 'br', 'bl']\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        \"\"\"\n",
    "        returns the input shape for the input layer of the network\n",
    "        \"\"\"\n",
    "        return self.current_buffer.get_input_shape()\n",
    "    \n",
    "#     def get_state_shape(self):\n",
    "#         \"\"\"\n",
    "#         not to be confused with the input shape. This is the shape of individual state (the shape of an individual processed shape of the environment)\n",
    "#         \"\"\"\n",
    "#         return self.current_state\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        returns a random action to be taken. [steering, acceleration, brake]\n",
    "        \"\"\"\n",
    "        return random.choice(self.action_space)\n",
    "    \n",
    "    def get_action_at_index(self, index):\n",
    "        return self.action_space[index]\n",
    "    \n",
    "    def get_num_action_space(self):\n",
    "        \"\"\"\n",
    "        returns the number of permuations of valide actions. For Eg. [steer left, accelerate and no brake] is ONE action\n",
    "        [steer right, accelerate and brake] is invalid as we cannot accelerate and brake at the same time.\n",
    "        there are 9 possible actions I think?\n",
    "        \"\"\"\n",
    "        return len(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the environment. self.done denotes whether the episode is done i.e. the car has crashed or we have stopped it\n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "        self.time_steps = 0\n",
    "        \n",
    "        tup = self.step('reset') # initial step. Says don't do anything but send the first 4 frames and game info\n",
    "        self.current_state, _, self.done = tup\n",
    "        return self.current_state[0]\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        does the action and returns the reward for that action along with the next state\n",
    "\n",
    "        This function may get complicated as it has to interact with the car simulator throught sockets.\n",
    "        \"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if not self.is_done():\n",
    "            # if the episode has not ended\n",
    "            #=======================\n",
    "            \n",
    "            # send the action\n",
    "            self.send_message(action)\n",
    "            \n",
    "            # wait for results from that action\n",
    "            angle, distance, speed, self.done = self.get_game_stats() # blocking line\n",
    "            # print(\"5: info:{0}, {1}, {2}, {3}\".format(angle, distance, speed, self.done))\n",
    "            \n",
    "            # initialise buffer\n",
    "            self.current_buffer = FrameBuffer(size = self.buffer_size)\n",
    "            \n",
    "            # add images from path to current_buffer\n",
    "            for i in range(1, self.buffer_size + 1):\n",
    "                # each Frame object is then assigned to the FrameBuffer class in chronological order\n",
    "                path = self.final_path + '/scr{0}.png'.format(i)\n",
    "                self.current_buffer.assign_to_buffer(self.get_frame(path))\n",
    "            \n",
    "            buffer_tensor = self.current_buffer.get_input_tensor()\n",
    "            # ========================================\n",
    "            \n",
    "            # calculate reward\n",
    "            dist_delta = self.prev_dist - distance\n",
    "            self.prev_dist = distance\n",
    "            if abs(dist_delta) > 30:\n",
    "                dist_delta = 5 # if there's too big a negative jump in the distance, the car has passed a checkpoint.\n",
    "                # so, don't penalise it for that.\n",
    " \n",
    "            reward = (dist_delta * 0.7) + (speed * 0.3) - (abs(angle) * 0.1)\n",
    "            #=================\n",
    "            \n",
    "            # A buffer is a collection of consecutive frames that we feed to the NN. These frames are already processed.\n",
    "            \n",
    "            # the current state consists of all the information from the environment\n",
    "            self.current_state = (buffer_tensor, angle, distance, speed)\n",
    "            \n",
    "            # this returns the state of the environment after the action has been completed, the reward for the action and if the episode ended.\n",
    "            return self.current_state , reward, self.done\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def send_message(self, string):\n",
    "        self.clientsocket.sendall(string.encode())\n",
    "    \n",
    "    def receive_message(self):\n",
    "        data  = self.clientsocket.recv(256).decode()\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        returns if the episode is finished\n",
    "        \"\"\"\n",
    "        return self.done\n",
    "        \n",
    "    \n",
    "    def get_frame(self, path: str) -> Frame:\n",
    "        \"\"\"\n",
    "        communicates with the sim to get the latest state/frame. \n",
    "        returns a Frame object\n",
    "        Get image from path then convert to np array then make a frame object\n",
    "        \"\"\"\n",
    "        image = Image.open(path, 'r')\n",
    "        image.load()\n",
    "        np_data = np.asarray(image, dtype=\"float32\" )\n",
    "        return Frame(np_data)\n",
    "    \n",
    "    \n",
    "    # def delete_screenshots(self, folder_path: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This method deletes the four screenshots saved in folder_path, along with the entire folder.\n",
    "    #     Method must be called after all four screenshots are converted to Frame objects.\n",
    "    #     \"\"\"\n",
    "    #     shutil.rmtree(folder_path)\n",
    "    \n",
    "    \n",
    "    def get_current_state():\n",
    "        \"\"\"\n",
    "        get the last n frames from the simulator (they might be stored in a folder by the simulator)\n",
    "        and store them in a buffer and return them\n",
    "        \"\"\"\n",
    "        return self.current_buff\n",
    "\n",
    "    \n",
    "    def get_game_stats(self):\n",
    "        \"\"\"\n",
    "        returns a tuple of angle, distance from checkpoint and speed from the sim. Again requires comms with simulator.\n",
    "        \"\"\"\n",
    "        # wait for info to arrive\n",
    "        string = self.receive_message()\n",
    "        \n",
    "        # process string\n",
    "        value_list = string.split(\", \")\n",
    "        angle = float(value_list[0])\n",
    "        distance = float(value_list[1])\n",
    "        speed = float(value_list[2])\n",
    "        crashed_state = False\n",
    "        if value_list[3] == '1':\n",
    "            crashed_state = True\n",
    "        \n",
    "        # return tuple of values\n",
    "        return angle, distance, speed, crashed_state\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        in case we need to 'close' the environment\n",
    "        \"\"\"\n",
    "        self.sock.close()\n",
    "        self.clientsocket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing connection\n",
    "# import random\n",
    "# data = []\n",
    "# env = EnvironmentWrapper()\n",
    "# for i in range(10000):\n",
    "#    data.append(env.step(\"ar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u36eHVMV285q"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment, network, run_only=False, eps_decay_rate=0.9975,max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "        self.env = environment # this should be the environment wrapper class\n",
    "        \n",
    "        if not run_only:\n",
    "            self.exp_rate = max_exp_rate     # our network starts off with this exploration rate\n",
    "        else:\n",
    "            self.exp_rate = 0.0\n",
    "        \n",
    "        self.min_exp_rate = min_exp_rate  # have at least 0.01 exploration rate at all times\n",
    "        \n",
    "        self.decay_rate = eps_decay_rate   # decay the exploration rate at this rate\n",
    "        \n",
    "        self.time_step = 0     # keeps track of time steps\n",
    "        \n",
    "        self.network = network\n",
    "    \n",
    "    def take_action(self, current_state):\n",
    "        # Implement the epsilon greedy strategy \n",
    "        result = random.random()                      # get a random number from 0 to 1 with linear distribution\n",
    "        if result > self.get_exp_rate():              # if it falls over the explore rate, exploit\n",
    "            action = self.env.get_action_at_index(self.network.get_max_q_value_index(current_state))  # exploit\n",
    "            \n",
    "        else:                                         # if it falls under the explore rate, explore\n",
    "            action = self.env.get_random_action()          # explore (generate a random action from the environment class)\n",
    "            \n",
    "        self.increment_time_step()                    # increment time step as well as update the decay rate\n",
    "        next_state, reward, done = self.env.step(action)# finally, take the action and record the reward\n",
    "        \n",
    "        return current_state, self.env.action_space.index(action), reward, next_state[0], done  # return an experience Tuple\n",
    "        \n",
    "    \n",
    "    def reset_time_steps(self, i=0):\n",
    "        self.timesteps = i\n",
    "    \n",
    "    def increment_time_step(self):\n",
    "        self.time_step += 1\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.exp_rate > self.min_exp_rate:\n",
    "            self.exp_rate = self.exp_rate * self.decay_rate\n",
    "        else:\n",
    "            self.exp_rate = self.min_exp_rate\n",
    "    \n",
    "    def get_exp_rate(self):\n",
    "        return self.exp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MVpqLFJ83IXp",
    "outputId": "27566a6e-9187-4053-ce13-02fa2abb6cbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "network_name = 'DrivePolicy.h5'\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "class NetworkTracker:\n",
    "    \n",
    "    def __init__(self, environment, source=False): # pass in the environment which has input shape of the frame\n",
    "        if source:\n",
    "            self.model = models.load_model(network_name)\n",
    "        else:\n",
    "            self.model = self.define_model(environment)\n",
    "            self.model.save(network_name)\n",
    "        self.target_model = self.model\n",
    "              \n",
    "    def define_model(self, env):\n",
    "        model = models.Sequential() \n",
    "        model.add(layers.Conv2D(filters=10, \n",
    "                                kernel_size=(3,3), \n",
    "                                activation='relu', \n",
    "                                input_shape=env.get_input_shape())) # first layer takes input shape from the environment\n",
    "        \n",
    "        model.add(layers.MaxPool2D((3, 3)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=20, kernel_size = (3, 3), strides=2, activation='relu'))\n",
    "\n",
    "        model.add(layers.MaxPool2D(3, 3))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='softmax'))\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        \n",
    "        model.add(layers.Dense(env.get_num_action_space(), activation='linear'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                           loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def get_q_values_for_one(self, state):\n",
    "        \n",
    "        output_tensor = self.model.predict(state.reshape((1, ) + state.shape )) # the State class handles turning the state \n",
    "                                                                    # into an appropriate input tensor for a NN\n",
    "                                                                    # so you don't have to change it everywhere\n",
    "        return output_tensor[0]  # you want to convert the 2 dimensional output to 1 dimension to call argmax\n",
    "    \n",
    "    def get_max_q_value_index(self, state):\n",
    "        return np.argmax(self.get_q_values_for_one(state))\n",
    "    \n",
    "    def get_q_values_for_batch(self, states):\n",
    "        if states[0] is State:\n",
    "            states = np.asarray(states)\n",
    "        \n",
    "        f = self.model.predict(states)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def get_target_tensor(self, next_states):\n",
    "        if next_states[0] is State:\n",
    "            next_states = np.asarray([i.to_input_tensor(individual=False) for i in next_states])\n",
    "        output_tensor = self.target_model.predict(next_states)\n",
    "        \n",
    "        return output_tensor\n",
    "                  \n",
    "    def fit(self, states_batch, targets_batch):\n",
    "        self.model.fit(states_batch, targets_batch, verbose=1)\n",
    "        \n",
    "    def clone_policy(self):\n",
    "        self.model.save(network_name)\n",
    "        self.target_model = models.load_model(network_name)\n",
    "                  \n",
    "    def get_model_summary(self):\n",
    "        return self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.replay = []\n",
    "        self.limit = size\n",
    "        self.exp_count = 0\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.exp_count += 1\n",
    "        \n",
    "        if self.exp_count < self.limit:\n",
    "            self.replay.append(experience)  #append to experiences\n",
    "        else:\n",
    "            self.replay[self.exp_count%len(self.replay)] = experience  #wrap around if the memory capacity is reached\n",
    "        assert len(self.replay) <= self.limit\n",
    "        \n",
    "    def is_usable(self, batch_size):\n",
    "        return len(self.replay) >= batch_size\n",
    "    \n",
    "    def reset_replay_memory(self):\n",
    "        self.exp_count = 0\n",
    "        self.replay = []\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.replay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(sample):\n",
    "    states = np.asarray([i[0] for i in sample])\n",
    "    actions = np.asarray([i[1] for i in sample])\n",
    "    rewards = np.asarray([i[2] for i in sample])\n",
    "    next_states = np.asarray([i[3] for i in sample])\n",
    "    done_tensor = np.asarray([i[4] for i in sample])\n",
    "    return states, actions, rewards, next_states, done_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(states, actions, rewards, next_states, dones, net, gamma):\n",
    "    assert actions.ndim == 1\n",
    "    assert rewards.ndim == 1\n",
    "    assert dones.ndim == 1\n",
    "    assert len(actions) == len(rewards) == len(dones) == len(states) == len(next_states)\n",
    "    target_q_values = net.get_q_values_for_batch(states)\n",
    "    targets = rewards + gamma * (np.max(net.get_target_tensor(next_states), axis=1))\n",
    "    for i in range(len(targets)):\n",
    "        if dones[i]:\n",
    "            targets[i] = rewards[i]\n",
    "    \n",
    "    for index in range(len(target_q_values)):\n",
    "        \n",
    "        target_q_values[index][actions[index]] = targets[index]\n",
    "\n",
    "    return target_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_agent(verbose=False, num_episodes=1500,\n",
    "                discount = 0.95, batch_size = 64, N = 40, memory_size = 1024, \n",
    "                eps_decay_rate=0.9975, max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "    # get all the hyperparameters in one place!\n",
    "    \n",
    "    # stores all the total reward per episode\n",
    "    training_stats = []    \n",
    "    \n",
    "    # initialise your environment\n",
    "    env = EnvironmentWrapper()\n",
    "    \n",
    "    # initialise your policy and target networks\n",
    "    net = NetworkTracker(env)\n",
    "    \n",
    "    # initialise your agent that will follow the epsilon greedy strategy\n",
    "    agent = Agent(env, net, eps_decay_rate=eps_decay_rate, max_exp_rate=max_exp_rate,min_exp_rate=min_exp_rate )    \n",
    "    \n",
    "    # initialise experience replay memory\n",
    "    memory = Memory(memory_size)\n",
    "    \n",
    "    for episode_count in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        # uncomment if you want to start the environmet with a random move\n",
    "        # state = env.step(env.get_random_action())[0]\n",
    "        \n",
    "        # keeps track of the total reward that we got for this episode\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        stuck_counter = 0\n",
    "        # check if the environment is available to run\n",
    "        while not env.is_done(): # run the environment for one episode\n",
    "            current_state, action, reward, next_state, done = agent.take_action(state)\n",
    "            cumulative_reward += reward\n",
    "            experience = current_state, action, reward, next_state, done\n",
    "            state = next_state\n",
    "            memory.push(experience)\n",
    "            if abs(reward) < 0.3:\n",
    "                stuck_counter += 1\n",
    "                if stuck_counter > 10:\n",
    "                    break\n",
    "            else:\n",
    "                stuck_counter = 0\n",
    "                \n",
    "\n",
    "        \n",
    "        agent.update_epsilon()\n",
    "        if memory.is_usable(batch_size):\n",
    "\n",
    "                experience_batch = memory.sample(batch_size)\n",
    "                states, actions, rewards, next_states, done_tensor = extract_tensors(experience_batch) # unzips the tensors\n",
    "\n",
    "                target_batch = get_target_batch(states, actions, rewards, next_states, done_tensor, net, discount)\n",
    "\n",
    "\n",
    "                net.fit(states, target_batch)\n",
    "\n",
    "        if (episode_count + 1) % N == 0:\n",
    "            net.clone_policy()\n",
    "\n",
    "        training_stats.append(cumulative_reward)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Episode Count: \", episode_count, \"\\t Cumulative Reward: \", cumulative_reward, \"\\t eps: \", agent.exp_rate )\n",
    "\n",
    "\n",
    "    epochs = list(range(len(training_stats)))\n",
    "\n",
    "    plt.clf\n",
    "    plt.plot(epochs, training_stats, 'b', label='f')\n",
    "    env.close()\n",
    "    \n",
    "    return epochs, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = EnvironmentWrapper()\n",
    "# lmao = []\n",
    "# for i in range(2):\n",
    "#     env.reset()\n",
    "#     while not env.is_done():\n",
    "#         exp = env.step('as')\n",
    "#         lmao.append(exp)\n",
    "# env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to connect to Simulator...\n",
      "Connected to simulator!\n",
      "Directory  /Users/manavshah/GitHub/SelfDrivingResearch/simulation/Screenshots  already exists\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.0728\n",
      "Episode Count:  0 \t Cumulative Reward:  25.213999999999988 \t eps:  0.9997\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1.1591\n",
      "Episode Count:  1 \t Cumulative Reward:  59.46800000000001 \t eps:  0.9994000900000001\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3046\n",
      "Episode Count:  2 \t Cumulative Reward:  8.08 \t eps:  0.999100269973\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1.3691\n",
      "Episode Count:  3 \t Cumulative Reward:  35.784000000000006 \t eps:  0.9988005398920082\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.4460\n",
      "Episode Count:  4 \t Cumulative Reward:  20.56699999999999 \t eps:  0.9985008997300406\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6729\n",
      "Episode Count:  5 \t Cumulative Reward:  51.766000000000005 \t eps:  0.9982013494601216\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8179\n",
      "Episode Count:  6 \t Cumulative Reward:  31.814999999999998 \t eps:  0.9979018890552837\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.9728\n",
      "Episode Count:  7 \t Cumulative Reward:  44.54599999999999 \t eps:  0.9976025184885671\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.9484\n",
      "Episode Count:  8 \t Cumulative Reward:  48.26999999999999 \t eps:  0.9973032377330205\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8160\n",
      "Episode Count:  9 \t Cumulative Reward:  11.128000000000004 \t eps:  0.9970040467617006\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6921\n",
      "Episode Count:  10 \t Cumulative Reward:  86.96799999999995 \t eps:  0.9967049455476722\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3628\n",
      "Episode Count:  11 \t Cumulative Reward:  38.514 \t eps:  0.9964059340640079\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3641\n",
      "Episode Count:  12 \t Cumulative Reward:  30.342999999999993 \t eps:  0.9961070122837887\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6150\n",
      "Episode Count:  13 \t Cumulative Reward:  16.992999999999995 \t eps:  0.9958081801801036\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.4707\n",
      "Episode Count:  14 \t Cumulative Reward:  32.162 \t eps:  0.9955094377260496\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.4616\n",
      "Episode Count:  15 \t Cumulative Reward:  47.05500000000001 \t eps:  0.9952107848947318\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.5642\n",
      "Episode Count:  16 \t Cumulative Reward:  17.864 \t eps:  0.9949122216592634\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.9163\n",
      "Episode Count:  17 \t Cumulative Reward:  23.044 \t eps:  0.9946137479927657\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7088\n",
      "Episode Count:  18 \t Cumulative Reward:  37.82499999999999 \t eps:  0.9943153638683679\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.9882\n",
      "Episode Count:  19 \t Cumulative Reward:  38.16499999999999 \t eps:  0.9940170692592074\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.4948\n",
      "Episode Count:  20 \t Cumulative Reward:  43.625 \t eps:  0.9937188641384297\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.9667\n",
      "Episode Count:  21 \t Cumulative Reward:  73.30000000000001 \t eps:  0.9934207484791882\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7895\n",
      "Episode Count:  22 \t Cumulative Reward:  24.55 \t eps:  0.9931227222546445\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8362\n",
      "Episode Count:  23 \t Cumulative Reward:  21.300999999999995 \t eps:  0.9928247854379681\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1.1165\n",
      "Episode Count:  24 \t Cumulative Reward:  16.576999999999998 \t eps:  0.9925269380023368\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.9193\n",
      "Episode Count:  25 \t Cumulative Reward:  73.17600000000002 \t eps:  0.9922291799209362\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7789\n",
      "Episode Count:  26 \t Cumulative Reward:  32.757000000000005 \t eps:  0.9919315111669599\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7642\n",
      "Episode Count:  27 \t Cumulative Reward:  18.848 \t eps:  0.9916339317136098\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5724\n",
      "Episode Count:  28 \t Cumulative Reward:  21.538999999999994 \t eps:  0.9913364415340957\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5457\n",
      "Episode Count:  29 \t Cumulative Reward:  33.817 \t eps:  0.9910390406016355\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7214\n",
      "Episode Count:  30 \t Cumulative Reward:  24.000999999999994 \t eps:  0.990741728889455\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6329\n",
      "Episode Count:  31 \t Cumulative Reward:  20.994 \t eps:  0.9904445063707882\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7480\n",
      "Episode Count:  32 \t Cumulative Reward:  71.16999999999999 \t eps:  0.990147373018877\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1.2829\n",
      "Episode Count:  33 \t Cumulative Reward:  34.650000000000006 \t eps:  0.9898503288069713\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 1.1453\n",
      "Episode Count:  34 \t Cumulative Reward:  39.296000000000014 \t eps:  0.9895533737083293\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8915\n",
      "Episode Count:  35 \t Cumulative Reward:  31.982999999999997 \t eps:  0.9892565076962169\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8913\n",
      "Episode Count:  36 \t Cumulative Reward:  55.09399999999999 \t eps:  0.9889597307439081\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6161\n",
      "Episode Count:  37 \t Cumulative Reward:  39.038999999999994 \t eps:  0.988663042824685\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6955\n",
      "Episode Count:  38 \t Cumulative Reward:  15.480999999999995 \t eps:  0.9883664439118376\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3154\n",
      "Episode Count:  39 \t Cumulative Reward:  35.101000000000006 \t eps:  0.988069933978664\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5286\n",
      "Episode Count:  40 \t Cumulative Reward:  24.852999999999998 \t eps:  0.9877735129984705\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5108\n",
      "Episode Count:  41 \t Cumulative Reward:  21.089999999999996 \t eps:  0.9874771809445709\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1829\n",
      "Episode Count:  42 \t Cumulative Reward:  6.642000000000004 \t eps:  0.9871809377902876\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3014\n",
      "Episode Count:  43 \t Cumulative Reward:  20.746000000000002 \t eps:  0.9868847835089506\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4715\n",
      "Episode Count:  44 \t Cumulative Reward:  51.853 \t eps:  0.9865887180738979\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6920\n",
      "Episode Count:  45 \t Cumulative Reward:  30.384 \t eps:  0.9862927414584758\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3945\n",
      "Episode Count:  46 \t Cumulative Reward:  10.416 \t eps:  0.9859968536360383\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.9245\n",
      "Episode Count:  47 \t Cumulative Reward:  27.838000000000005 \t eps:  0.9857010545799475\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5154\n",
      "Episode Count:  48 \t Cumulative Reward:  19.424999999999997 \t eps:  0.9854053442635735\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.8093\n",
      "Episode Count:  49 \t Cumulative Reward:  29.729 \t eps:  0.9851097226602945\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Count:  50 \t Cumulative Reward:  15.436999999999998 \t eps:  0.9848141897434964\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6212\n",
      "Episode Count:  51 \t Cumulative Reward:  33.44799999999999 \t eps:  0.9845187454865735\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.7037\n",
      "Episode Count:  52 \t Cumulative Reward:  66.71399999999996 \t eps:  0.9842233898629276\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6199\n",
      "Episode Count:  53 \t Cumulative Reward:  23.744999999999997 \t eps:  0.9839281228459688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bf4ca6f2f2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0meps_decay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9997\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mmax_exp_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             min_exp_rate=0.05)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3365e0dcc282>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(verbose, num_episodes, discount, batch_size, N, memory_size, eps_decay_rate, max_exp_rate, min_exp_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# check if the environment is available to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# run the environment for one episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mcumulative_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mexperience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cb224484a666>\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, current_state)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_time_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# increment time step as well as update the decay rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# finally, take the action and record the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m  \u001b[0;31m# return an experience Tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42420b311c2f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# wait for results from that action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_game_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# blocking line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;31m# print(\"5: info:{0}, {1}, {2}, {3}\".format(angle, distance, speed, self.done))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42420b311c2f>\u001b[0m in \u001b[0;36mget_game_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# wait for info to arrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# process string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42420b311c2f>\u001b[0m in \u001b[0;36mreceive_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreceive_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mdata\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclientsocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(verbose=True, \n",
    "            num_episodes=100,\n",
    "            discount = 0.95, \n",
    "            batch_size = 32, \n",
    "            N = 40, # how often to clone the target policy\n",
    "            memory_size = 128,\n",
    "            eps_decay_rate=0.9997,\n",
    "            max_exp_rate=1.0, \n",
    "            min_exp_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(lmao[0][0][0].shape)\n",
    "f = lmao[0][0][0].transpose((2, 0, 1))\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
