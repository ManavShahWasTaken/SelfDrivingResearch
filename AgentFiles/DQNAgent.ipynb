{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z6tk-uX9bwY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import socket\n",
    "# import shutil\n",
    "# from skimage import io\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state_data):\n",
    "        self.state_data = np.asarray(state_data)\n",
    "  \n",
    "    def process_state(self):\n",
    "        pass\n",
    "  \n",
    "    def get_batch_tensor(self):\n",
    "        holder = np.asarray(self.state_data)\n",
    "        holder.reshape((1, ) + holder.shape)\n",
    "        return holder\n",
    "  \n",
    "    def get_individual_tensor(self):\n",
    "        return np.asarray(self.state_data)\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.state_data.shape\n",
    "  \n",
    "    def display(self):\n",
    "        print(self.state_data)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class Frame(State):\n",
    "    def __init__(self, state_data, crop_factor=None, destination_size=None, vert_cent=0.5):\n",
    "        State.__init__(self, state_data)\n",
    "#         self.state_data = self.process_state(crop_factor, vert_cent, destination_shape)\n",
    "        self.state_data = self.process_state([0.7, 1.0], 0.7, (128,64))\n",
    "  \n",
    "    def process_state(self, crop_factor, vert_cent, destination_shape):\n",
    "        \"\"\"\n",
    "        Does all the processing of the frame using the helper functions\n",
    "        \"\"\"\n",
    "        frame = self.crop_frame(self.state_data, crop_factor, vert_cent)\n",
    "        frame = self.normalise_frame(frame)\n",
    "        frame = self.gray_scale(frame) # cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        assert len(frame.shape) == 2\n",
    "        frame = self.downsample_frame(frame, destination_shape)\n",
    "        return frame\n",
    "\n",
    "  \n",
    "    def gray_scale(self, frame, gray_scale_factor=[0.3, 0.3, 0.3]):\n",
    "        frame = np.dot(frame, np.asarray(gray_scale_factor))\n",
    "        return frame\n",
    "\n",
    "    def normalise_frame(self, frame):\n",
    "        frame = frame.astype('float32') / 255.0\n",
    "        return frame\n",
    "  \n",
    "    def downsample_frame(self, frame, destination_shape):\n",
    "        \"\"\"\n",
    "        downsamples the frame. decreases the resolution\n",
    "        \"\"\"\n",
    "        frame = cv2.resize(np.asarray(frame), dsize=destination_shape, interpolation=cv2.INTER_CUBIC)\n",
    "        return frame\n",
    "  \n",
    "    def crop_frame(self, frame, crop_factor, vert_cent):\n",
    "        \"\"\"\n",
    "        input is the frame\n",
    "        output is the cropped frame\n",
    "        crop_factor is the ratio at which you want to crop the height and width(0.8, 0.8)\n",
    "        cent is the ratio at which the centre of the cropped frame should be\n",
    "        \"\"\"\n",
    "        if crop_factor is None:\n",
    "          return frame\n",
    "    \n",
    "        height_factor = int((crop_factor[0]*frame.shape[0]) // 2)\n",
    "        width_factor = int((crop_factor[1]*frame.shape[1]) // 2)\n",
    "        vert_cent = int(frame.shape[0]*vert_cent)\n",
    "        width_cent = int(frame.shape[1]*0.5)\n",
    "\n",
    "        frame = frame[vert_cent - height_factor: vert_cent + height_factor, \n",
    "                      width_cent - width_factor: width_cent + width_factor]\n",
    "        return frame\n",
    "\n",
    "# ------------------------------------\n",
    "class DataBuffer:\n",
    "    \"\"\"\n",
    "    Keeps track of n latest states \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, size=1):\n",
    "        self.buffer = []\n",
    "        self.size = size\n",
    "\n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        arr = np.array(self.buffer)\n",
    "        if self.size == 1 or in_batch:\n",
    "            return arr\n",
    "        else:\n",
    "            return arr.reshape((1, ) + arr.shape)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return np.asarray(self.current_state[0]).shape\n",
    "\n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(state)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class FrameBuffer(DataBuffer):\n",
    "    def __init__(self, size = 4):\n",
    "        DataBuffer.__init__(self, size=size)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.get_input_tensor().shape\n",
    "  \n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        temp = np.array(self.buffer)\n",
    "        return  temp.transpose((1, 2, 0))\n",
    "    \n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        # if buffer not initialised\n",
    "        if len(self.buffer) == 0:\n",
    "            self.buffer = [state]\n",
    "            return\n",
    "\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.buffer.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh-452ClVFpT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # initialise comms with the simulator here\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # initialise the socket\n",
    "        # connect to localhost, port 2345 \n",
    "        self.sock.bind((\"127.0.0.1\", 4444))\n",
    "        self.sock.listen(1)\n",
    "        print(\"Waiting to connect to Simulator...\")\n",
    "        self.clientsocket, _ = self.sock.accept() # connect to simulator [BLOCKING]\n",
    "        print(\"Connected to simulator!\")\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # initialising frame buffer\n",
    "        self.buffer_size = 4 # could change this \n",
    "        \n",
    "         # this is the FrameBuffer that keeps track of the latest frames.\n",
    "         #initialising the frame buffer by just giving it multiple copies of the same starting frame\n",
    "        # =========================================\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        self.current_buffer = None\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        \n",
    "        self.time_steps = 0\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        self.max_time_steps_per_episode = 500 #change this based on the enviorment\n",
    "        \n",
    "        # initialise buffer\n",
    "        self.current_buffer = FrameBuffer(size = self.buffer_size)\n",
    "        \n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # Create target directory if it doesn't exist\n",
    "       \n",
    "        self.parent_path = os.path.abspath(os.path.join(\"\", os.pardir))\n",
    "        self.final_path = self.parent_path + \"/simulation/Screenshots\"\n",
    "        if not os.path.exists(self.final_path):\n",
    "            os.mkdir(self.final_path)\n",
    "            print(\"Directory \" , self.final_path,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , self.final_path,  \" already exists\")\n",
    "\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # Save an initial scr1 image to Screenshots before initiation of simulator\n",
    "        \n",
    "        if not os.path.exists(self.final_path + \"/scr1.png\"):\n",
    "            scr1 = Image.open(self.parent_path + \"/simulation/scr1.png\", 'r')\n",
    "            scr1.save(self.final_path + \"/scr1.png\", \"PNG\")\n",
    "            print(\"scr1.png added to \" + self.final_path)\n",
    "        else:\n",
    "            print(\"scr1.png already exists in \" + self.final_path)\n",
    "    \n",
    "        # =========================================\n",
    "            \n",
    "        \n",
    "        # reset actually initializes self.current_state, self.current_buffer etc.\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.action_space = ['as', 'ar', 'al', 'ds', 'dr', 'dl', 'bs', 'br', 'bl']  \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_input_shape(self):\n",
    "        \"\"\"\n",
    "        returns the input shape for the input layer of the network\n",
    "        \"\"\"\n",
    "        return self.current_buffer.get_input_shape()\n",
    "    \n",
    "    \n",
    "#     def get_state_shape(self):\n",
    "#         \"\"\"\n",
    "#         not to be confused with the input shape. This is the shape of individual state (the shape of an individual processed shape of the environment)\n",
    "#         \"\"\"\n",
    "#         return self.current_state\n",
    "    \n",
    "    \n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        returns a random action to be taken. [steering, acceleration, brake]\n",
    "        \"\"\"\n",
    "        return random.choice(self.action_space)\n",
    "    \n",
    "    \n",
    "    def get_action_at_index(self, index):\n",
    "        return self.action_space[index]\n",
    "    \n",
    "    \n",
    "    def get_num_action_space(self):\n",
    "        \"\"\"\n",
    "        returns the number of permuations of valide actions. For Eg. [steer left, accelerate and no brake] is ONE action\n",
    "        [steer right, accelerate and brake] is invalid as we cannot accelerate and brake at the same time.\n",
    "        there are 9 possible actions I think?\n",
    "        \"\"\"\n",
    "        return len(self.action_space)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the environment. self.done denotes whether the episode is done i.e. the car has crashed or we have stopped it\n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "        self.time_steps = 0\n",
    "        \n",
    "        tup = self.step('reset') # initial step. Says don't do anything but send the first 4 frames and game info\n",
    "        self.current_state, _, self.done = tup\n",
    "        return self.current_state[0] # send only the frames\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        does the action and returns the reward for that action along with the next state\n",
    "\n",
    "        This function may get complicated as it has to interact with the car simulator throught sockets.\n",
    "        \"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if not self.is_done():\n",
    "            # if the episode has not ended\n",
    "            #=======================\n",
    "            \n",
    "            # send the action\n",
    "            print(\"sending action\")\n",
    "            self.send_message(action)\n",
    "            print(\"sent action\")\n",
    "            \n",
    "            # wait for results from that action\n",
    "            angle, distance, speed, self.done, frames_captured = self.get_game_stats() # blocking line\n",
    "            print(\"5: info:{0}, {1}, {2}, {3}, {4}\".format(angle, distance, speed, self.done, frames_captured))\n",
    "            \n",
    "            \n",
    "            # add images from path to current_buffer\n",
    "            for i in range(1, frames_captured + 1):\n",
    "                # each Frame object is then assigned to the FrameBuffer class in chronological order\n",
    "                path = self.final_path + '/scr{0}.png'.format(i)\n",
    "                self.current_buffer.assign_to_buffer(self.get_frame(path))\n",
    "            \n",
    "            buffer_tensor = self.current_buffer.get_input_tensor()\n",
    "            # ========================================\n",
    "            \n",
    "            # calculate reward\n",
    "            dist_delta = self.prev_dist - distance\n",
    "            self.prev_dist = distance\n",
    "            if abs(dist_delta) > 20:\n",
    "                dist_delta = 5 # if there's too big a negative jump in the distance, the car has passed a checkpoint.\n",
    "                # so, don't penalise it for that.\n",
    " \n",
    "            reward = (dist_delta * 0.85) - (abs(angle) * 0.15)\n",
    "            #=================\n",
    "            \n",
    "            # A buffer is a collection of consecutive frames that we feed to the NN. These frames are already processed.\n",
    "            \n",
    "            # the current state consists of all the information from the environment\n",
    "            self.current_state = (buffer_tensor, angle, distance, speed)\n",
    "            \n",
    "            # this returns the state of the environment after the action has been completed, the reward for the action and if the episode ended.\n",
    "            return self.current_state , reward, self.done\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def send_message(self, string):\n",
    "        self.clientsocket.sendall(string.encode())\n",
    "    \n",
    "    \n",
    "    def receive_message(self):\n",
    "        data  = self.clientsocket.recv(256).decode()\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        returns if the episode is finished\n",
    "        \"\"\"\n",
    "        return self.done\n",
    "        \n",
    "    \n",
    "    def get_frame(self, path: str) -> Frame:\n",
    "        \"\"\"\n",
    "        communicates with the sim to get the latest state/frame. \n",
    "        returns a Frame object\n",
    "        Get image from path then convert to np array then make a frame object\n",
    "        \"\"\"\n",
    "        image = Image.open(path, 'r')\n",
    "        image.load()\n",
    "        np_data = np.asarray(image, dtype=\"float32\" )\n",
    "        return Frame(np_data)\n",
    "    \n",
    "    \n",
    "    # def delete_screenshots(self, folder_path: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This method deletes the four screenshots saved in folder_path, along with the entire folder.\n",
    "    #     Method must be called after all four screenshots are converted to Frame objects.\n",
    "    #     \"\"\"\n",
    "    #     shutil.rmtree(folder_path)\n",
    "    \n",
    "    \n",
    "    def get_current_state():\n",
    "        \"\"\"\n",
    "        get the last n frames from the simulator (they might be stored in a folder by the simulator)\n",
    "        and store them in a buffer and return them\n",
    "        \"\"\"\n",
    "        return self.current_buff\n",
    "\n",
    "    \n",
    "    def get_game_stats(self):\n",
    "        \"\"\"\n",
    "        returns a tuple of angle, distance from checkpoint and speed from the sim. Again requires comms with simulator.\n",
    "        \"\"\"\n",
    "        # wait for info to arrive\n",
    "        string = self.receive_message()\n",
    "        \n",
    "        # process string\n",
    "        value_list = string.split(\", \")\n",
    "        angle = float(value_list[0])\n",
    "        distance = float(value_list[1])\n",
    "        speed = float(value_list[2])\n",
    "        crashed_state = False\n",
    "        if value_list[3] == '1':\n",
    "            crashed_state = True\n",
    "        frames_captured = int(value_list[4])\n",
    "        # return tuple of values\n",
    "        return angle, distance, speed, crashed_state, frames_captured\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        in case we need to 'close' the environment\n",
    "        \"\"\"\n",
    "        self.sock.close()\n",
    "        self.clientsocket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing connection\n",
    "# import random\n",
    "# data = []\n",
    "# env = EnvironmentWrapper()\n",
    "# for i in range(10000):\n",
    "#    data.append(env.step(\"ar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u36eHVMV285q"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment, network, run_only=False, eps_decay_rate=0.9975,max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "        self.env = environment # this should be the environment wrapper class\n",
    "        \n",
    "        if not run_only:\n",
    "            self.exp_rate = max_exp_rate     # our network starts off with this exploration rate\n",
    "        else:\n",
    "            self.exp_rate = 0.0\n",
    "        \n",
    "        self.min_exp_rate = min_exp_rate  # have at least 0.01 exploration rate at all times\n",
    "        \n",
    "        self.decay_rate = eps_decay_rate   # decay the exploration rate at this rate\n",
    "        \n",
    "        self.time_step = 0     # keeps track of time steps\n",
    "        \n",
    "        self.network = network\n",
    "    \n",
    "    def take_action(self, current_state):\n",
    "        # Implement the epsilon greedy strategy \n",
    "        result = random.random()                      # get a random number from 0 to 1 with linear distribution\n",
    "        if result > self.get_exp_rate():              # if it falls over the explore rate, exploit\n",
    "            # Get the action with the maximum q-value\n",
    "            action = self.env.get_action_at_index(self.network.get_max_q_value_index(current_state))  # exploit\n",
    "        else:                                         # if it falls under the explore rate, explore\n",
    "            action = self.env.get_random_action()          # explore (generate a random action from the environment class)\n",
    "            \n",
    "        self.increment_time_step()                    # increment time step as well as update the decay rate\n",
    "        next_state, reward, done = self.env.step(action)# finally, take the action and record the reward\n",
    "        \n",
    "        return current_state, self.env.action_space.index(action), reward, next_state[0], done  # return an experience Tuple\n",
    "        \n",
    "    \n",
    "    def reset_time_steps(self, i=0):\n",
    "        self.timesteps = i\n",
    "    \n",
    "    def increment_time_step(self):\n",
    "        self.time_step += 1\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.exp_rate > self.min_exp_rate:\n",
    "            self.exp_rate = self.exp_rate * self.decay_rate\n",
    "        else:\n",
    "            self.exp_rate = self.min_exp_rate\n",
    "    \n",
    "    def get_exp_rate(self):\n",
    "        return self.exp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MVpqLFJ83IXp",
    "outputId": "27566a6e-9187-4053-ce13-02fa2abb6cbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "network_name = 'DrivePolicy.h5'\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "class NetworkTracker:\n",
    "    \n",
    "    def __init__(self, environment, source=False): # pass in the environment which has input shape of the frame\n",
    "        if source:\n",
    "            self.model = models.load_model(network_name)\n",
    "        else:\n",
    "            self.model = self.define_model(environment)\n",
    "            self.model.save(network_name)\n",
    "        self.target_model = self.model\n",
    "              \n",
    "    def define_model(self, env):\n",
    "        model = models.Sequential() \n",
    "        model.add(layers.Conv2D(filters=10, \n",
    "                                kernel_size=(3,3), \n",
    "                                activation='relu', \n",
    "                                input_shape=env.get_input_shape())) # first layer takes input shape from the environment\n",
    "        \n",
    "        model.add(layers.MaxPool2D((3, 3)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=20, kernel_size = (3, 3), strides=2, activation='relu'))\n",
    "\n",
    "        model.add(layers.MaxPool2D(3, 3))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='softmax'))\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        \n",
    "        model.add(layers.Dense(env.get_num_action_space(), activation='linear'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                           loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def get_q_values_for_one(self, state):\n",
    "        \n",
    "        output_tensor = self.model.predict(state.reshape((1, ) + state.shape )) # the State class handles turning the state \n",
    "                                                                    # into an appropriate input tensor for a NN\n",
    "                                                                    # so you don't have to change it everywhere\n",
    "        return output_tensor[0]  # you want to convert the 2 dimensional output to 1 dimension to call argmax\n",
    "    \n",
    "    def get_max_q_value_index(self, state):\n",
    "        return np.argmax(self.get_q_values_for_one(state))\n",
    "    \n",
    "    def get_q_values_for_batch(self, states):\n",
    "        if states[0] is State:\n",
    "            states = np.asarray(states)\n",
    "        \n",
    "        f = self.model.predict(states)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def get_target_tensor(self, next_states):\n",
    "        \"\"\"\n",
    "        Return the output from the next states in the experience tuple.\n",
    "        \"\"\"\n",
    "        if isinstance(next_states[0], DataBuffer): # if you have a list of buffers, convert them to numpy tensors\n",
    "            next_states = np.asarray([i.get_input_tensor(in_batch=True) for i in next_states])\n",
    "        \n",
    "        output_tensor = self.target_model.predict(next_states)\n",
    "        \n",
    "        return output_tensor\n",
    "                  \n",
    "    def fit(self, states_batch, targets_batch):\n",
    "        \"\"\"\n",
    "        Fit the states with the target batch\n",
    "        \"\"\"\n",
    "        self.model.fit(states_batch, targets_batch, verbose=1)\n",
    "        \n",
    "    def clone_policy(self):\n",
    "        \"\"\"\n",
    "        Clone the target policy\n",
    "        \"\"\"\n",
    "        self.model.save(network_name)\n",
    "        self.target_model = models.load_model(network_name)\n",
    "                  \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        Return a summary of the defined model\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def save_policy(self):\n",
    "        self.model.save(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.replay = []\n",
    "        self.limit = size\n",
    "        self.exp_count = 0\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.exp_count += 1\n",
    "        \n",
    "        if self.exp_count < self.limit:\n",
    "            self.replay.append(experience)  #append to experiences\n",
    "        else:\n",
    "            self.replay[self.exp_count%len(self.replay)] = experience  #wrap around if the memory capacity is reached\n",
    "        assert len(self.replay) <= self.limit\n",
    "        \n",
    "    def is_usable(self, batch_size):\n",
    "        return len(self.replay) >= batch_size\n",
    "    \n",
    "    def reset_replay_memory(self):\n",
    "        self.exp_count = 0\n",
    "        self.replay = []\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.replay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(sample):\n",
    "    states = np.asarray([i[0] for i in sample])\n",
    "    actions = np.asarray([i[1] for i in sample])\n",
    "    rewards = np.asarray([i[2] for i in sample])\n",
    "    next_states = np.asarray([i[3] for i in sample])\n",
    "    done_tensor = np.asarray([i[4] for i in sample])\n",
    "    return states, actions, rewards, next_states, done_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(states, actions, rewards, next_states, dones, net, gamma):\n",
    "    assert actions.ndim == 1\n",
    "    assert rewards.ndim == 1\n",
    "    assert dones.ndim == 1\n",
    "    assert len(actions) == len(rewards) == len(dones) == len(states) == len(next_states)\n",
    "    target_q_values = net.get_q_values_for_batch(states) # get the q values from the current states\n",
    "    targets = rewards + gamma * (np.max(net.get_target_tensor(next_states), axis=1)) # get the target values for the state action pairs.\n",
    "    for i in range(len(targets)): # change the targets for state which ended an episode\n",
    "        if dones[i]:\n",
    "            targets[i] = rewards[i]\n",
    "    \n",
    "    for index in range(len(target_q_values)):\n",
    "        target_q_values[index][actions[index]] = targets[index] # assign the targets to the corresponding state action pairs\n",
    "\n",
    "    return target_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_agent(verbose=False, num_episodes=1500,\n",
    "                discount = 0.95, batch_size = 64, N = 40, memory_size = 1024, \n",
    "                eps_decay_rate=0.9975, max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "    # get all the hyperparameters in one place!\n",
    "    \n",
    "    # stores all the total reward per episode\n",
    "    training_stats = []    \n",
    "    \n",
    "    # initialise your environment\n",
    "    env = EnvironmentWrapper()\n",
    "    \n",
    "    # initialise your policy and target networks\n",
    "    net = NetworkTracker(env)\n",
    "    \n",
    "    # initialise your agent that will follow the epsilon greedy strategy\n",
    "    agent = Agent(env, net, eps_decay_rate=eps_decay_rate, max_exp_rate=max_exp_rate,min_exp_rate=min_exp_rate )    \n",
    "    \n",
    "    # initialise experience replay memory\n",
    "    memory = Memory(memory_size)\n",
    "    \n",
    "    for episode_count in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        # uncomment if you want to start the environmet with a random move\n",
    "        # state = env.step(env.get_random_action())[0]\n",
    "        \n",
    "        # keeps track of the total reward that we got for this episode\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        stuck_counter = 0\n",
    "        # check if the environment is available to run\n",
    "        while not env.is_done(): # run the environment for one episode\n",
    "            current_state, action, reward, next_state, done = agent.take_action(state) # let the agent take an action for one time step\n",
    "            cumulative_reward += reward \n",
    "            experience = current_state, action, reward, next_state, done # experience tuple \n",
    "            state = next_state # update the current state\n",
    "            memory.push(experience) # push the experience in memory\n",
    "            # check if the car is stuck when the reward isn't changing by much\n",
    "            if abs(reward) < 0.3:\n",
    "                stuck_counter += 1\n",
    "                if stuck_counter > 10:\n",
    "                    break\n",
    "            else:\n",
    "                stuck_counter = 0\n",
    "        \n",
    "        agent.update_epsilon() # update the exploration rate of the agent after each episode\n",
    "        \n",
    "        if memory.is_usable(batch_size):\n",
    "                experience_batch = memory.sample(batch_size) # sample randomly from memory\n",
    "                states, actions, rewards, next_states, done_tensor = extract_tensors(experience_batch) # unzips the tensors\n",
    "\n",
    "                target_batch = get_target_batch(states, actions, rewards, next_states, done_tensor, net, discount) # get a batch of target values to fit against\n",
    "                \n",
    "                net.fit(states, target_batch) # fit the network\n",
    "\n",
    "        if (episode_count + 1) % N == 0:\n",
    "            net.clone_policy()  # clone the target policy every N episodes.\n",
    "\n",
    "        training_stats.append(cumulative_reward)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Episode Count: \", episode_count, \"\\t Cumulative Reward: \", cumulative_reward, \"\\t eps: \", agent.exp_rate )\n",
    "\n",
    "\n",
    "    epochs = list(range(len(training_stats)))\n",
    "\n",
    "    plt.clf\n",
    "    plt.plot(epochs, training_stats, 'b', label='f') # plot the training curve after training \n",
    "    env.close()\n",
    "    \n",
    "    return epochs, training_stats, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to connect to Simulator...\n",
      "Connected to simulator!\n",
      "Directory  C:\\Users\\Sohaib Saqib\\Documents\\GitHub\\SelfDrivingResearch/simulation/Screenshots  already exists\n",
      "scr1.png already exists in C:\\Users\\Sohaib Saqib\\Documents\\GitHub\\SelfDrivingResearch/simulation/Screenshots\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 17.29, 4.12, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 29.01, 2.35, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 29.1, 4.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:2.14, 0.0, 0.0, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 14.74, 2.75, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.25, 14.82, 4.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 15.01, 6.67, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 29.02, 1.18, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 29.07, 3.33, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.14, 29.21, 5.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.17, 28.95, 0.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.25, 28.48, 2.53, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.36, 27.44, 4.79, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.5, 25.95, 7.07, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.66, 24.35, 9.05, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.87, 22.58, 11.0, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.16, 21.06, 12.72, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.51, 19.87, 14.41, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 29.04, 1.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 29.1, 3.53, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.14, 29.24, 5.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 28.74, 2.17, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 28.06, 4.41, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.28, 26.83, 6.77, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.37, 25.31, 8.82, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.5, 23.26, 11.01, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.67, 20.93, 13.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.9, 18.69, 15.0, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.17, 16.85, 16.24, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 29.02, 0.78, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 29.07, 2.75, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 29.17, 4.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 28.88, 1.64, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.2, 28.09, 4.39, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 26.59, 7.22, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 24.8, 9.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.52, 22.62, 11.67, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.7, 20.21, 13.75, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.93, 17.97, 15.44, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.18, 16.02, 16.59, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 34.06, 1.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 34.09, 3.73, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 34.19, 6.08, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 33.74, 2.51, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 32.79, 5.24, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.49, 31.44, 7.67, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.64, 29.53, 10.18, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.82, 27.54, 12.33, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.07, 25.43, 14.43, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.34, 23.53, 15.37, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 19.87, 0.98, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.19, 19.86, 3.33, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 19.89, 5.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.23, 19.77, 1.91, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 18.96, 4.43, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.36, 17.74, 6.82, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.46, 15.92, 9.36, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.62, 13.67, 11.8, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.85, 11.47, 13.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.24, 9.7, 15.91, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.65, 9.2, 16.85, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 38.31, 1.18, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 38.39, 3.33, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 38.55, 5.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.26, 38.13, 1.78, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.33, 37.44, 4.03, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.42, 36.33, 6.3, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.53, 34.81, 8.52, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.66, 32.95, 10.67, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.86, 30.73, 12.9, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.09, 28.77, 14.5, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.27, 27.16, 14.75, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.06, 59.31, 1.37, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 59.38, 3.33, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 59.5, 5.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 59.25, 1.46, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 58.59, 3.63, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 57.41, 6.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 55.97, 8.12, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 54.35, 9.9, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 52.19, 11.88, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.26, 49.66, 13.85, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.3, 47.08, 15.6, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.34, 43.32, 17.02, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.38, 39.51, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.42, 35.73, 17.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.48, 32.02, 17.04, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.54, 28.4, 17.17, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.55, 25.63, 16.39, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.03, 20.69, 0.78, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.03, 20.64, 2.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.04, 20.58, 5.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 20.76, 1.35, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 20.11, 3.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 19.03, 5.95, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 17.46, 8.34, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 15.38, 10.65, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.2, 12.56, 13.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 9.52, 15.33, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.47, 6.2, 16.96, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 17.86, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.5, 14.15, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.66, 11.01, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.95, 8.45, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.41, 7.14, 17.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.98, 7.06, 16.93, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 35.74, 1.96, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 35.64, 4.12, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.19, 35.5, 6.28, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.19, 35.68, 1.91, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 35.16, 3.44, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 34.16, 5.58, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.05, 32.84, 7.6, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 31.11, 9.59, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.14, 29.22, 11.35, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 26.76, 13.24, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.37, 24.01, 14.99, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.48, 21.39, 16.37, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.57, 18.79, 17.0, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.69, 16.03, 17.02, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.86, 13.2, 16.68, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.06, 59.31, 1.96, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 59.38, 3.73, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 59.52, 5.69, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 59.07, 1.8, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 58.42, 4.3, False, 4\n",
      "sending action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent action\n",
      "5: info:0.13, 57.33, 6.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 55.8, 8.7, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 54.07, 10.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.23, 51.27, 13.02, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.27, 48.52, 15.01, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.31, 45.44, 16.79, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.33, 42.24, 17.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.36, 39.36, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 36.18, 17.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.43, 33.06, 17.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.48, 29.93, 16.99, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.53, 26.66, 17.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.54, 22.88, 16.53, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.2, 14.72, 0.39, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 14.75, 2.55, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 14.81, 4.32, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.28, 14.97, 6.27, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 14.78, 1.65, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.37, 14.04, 4.08, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.47, 12.93, 6.41, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.59, 11.74, 8.29, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.76, 10.37, 10.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.01, 9.0, 11.83, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.36, 7.96, 13.46, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.85, 7.89, 14.73, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.03, 20.69, 0.59, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.03, 20.64, 2.75, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.04, 20.58, 4.9, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 20.77, 0.96, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 20.17, 3.39, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 19.14, 5.79, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 17.59, 8.2, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 15.54, 10.52, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.2, 13.01, 12.79, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 9.74, 15.21, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.47, 6.44, 16.97, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.4, 18.13, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.5, 14.75, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.68, 11.34, 17.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.96, 8.83, 17.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.4, 7.53, 17.05, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.93, 7.2, 16.89, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 20.63, 0.59, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 20.62, 3.34, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 20.66, 5.88, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 20.69, 1.95, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 19.95, 4.48, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.17, 18.82, 6.82, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 17.25, 8.98, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.48, 15.12, 11.16, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.71, 12.97, 12.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.02, 10.98, 14.43, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.38, 9.65, 14.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.76, 9.47, 15.28, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 20.64, 0.59, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 20.64, 2.55, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 20.68, 5.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.17, 20.76, 3.91, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 20.37, 3.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 19.49, 5.66, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 18.13, 8.01, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 16.33, 10.19, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.35, 14.38, 12.02, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.53, 11.96, 13.9, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.81, 9.29, 15.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.16, 7.81, 16.54, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.59, 7.09, 16.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.02, 32.59, 0.78, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.04, 32.64, 3.34, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 32.74, 5.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 32.7, 2.36, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.11, 32.04, 3.72, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.2, 31.04, 6.16, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.31, 29.6, 8.38, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.45, 27.78, 10.45, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.62, 25.69, 12.37, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.83, 23.54, 14.06, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.05, 21.86, 15.01, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 19.86, 0.78, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 19.81, 3.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.2, 19.8, 5.49, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 19.79, 2.02, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 18.88, 4.99, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.29, 17.51, 7.66, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.37, 15.44, 10.37, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.49, 12.86, 12.92, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.68, 10.24, 15.15, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.03, 7.67, 16.96, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.59, 6.74, 17.06, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:2.1, 7.5, 16.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.01, 90.71, 0.78, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.02, 90.75, 2.75, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.05, 90.81, 4.51, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.03, 90.68, 1.45, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.06, 90.14, 3.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 89.29, 5.24, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 88.03, 7.26, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.16, 86.37, 9.26, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 84.54, 11.04, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.26, 82.16, 13.01, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.33, 79.45, 14.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.37, 76.44, 16.69, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 73.0, 17.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.41, 69.55, 17.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.44, 66.43, 17.07, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.46, 63.17, 17.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.47, 59.9, 16.47, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.49, 57.15, 16.8, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.03, 15.45, 1.18, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.05, 15.39, 3.53, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 15.36, 5.88, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 15.4, 2.23, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.14, 14.51, 4.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.17, 13.24, 6.93, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 11.48, 9.25, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.3, 9.04, 11.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.47, 6.41, 13.93, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.52, 22.65, 16.26, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.62, 19.55, 17.08, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.77, 16.43, 17.12, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.97, 13.87, 17.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.19, 11.58, 16.4, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.08, 29.02, 0.98, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 29.07, 2.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 29.2, 5.3, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 28.81, 1.92, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 28.05, 4.62, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 26.9, 6.9, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.32, 25.15, 9.26, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.42, 22.96, 11.52, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.56, 20.43, 13.69, False, 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending action\n",
      "sent action\n",
      "5: info:0.75, 17.77, 15.68, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.95, 15.15, 16.88, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 31.39, 0.39, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 31.41, 2.55, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 31.48, 4.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.16, 31.36, 1.4, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.27, 30.7, 3.63, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.4, 29.54, 6.05, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.6, 27.59, 8.81, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.89, 25.08, 11.58, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.27, 23.15, 13.74, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 31.39, 0.2, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 31.41, 2.35, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 31.48, 4.32, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.17, 31.5, 2.31, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 30.8, 3.48, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.34, 29.74, 6.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.46, 28.19, 8.55, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.63, 26.06, 11.03, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.84, 23.92, 13.11, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.12, 21.91, 15.03, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.4, 20.51, 16.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 19.87, 1.57, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.19, 19.86, 3.92, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 19.92, 6.28, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.25, 19.62, 2.46, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.31, 18.7, 5.01, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 17.38, 7.39, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.52, 15.27, 10.1, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.78, 12.37, 13.09, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.11, 10.25, 15.3, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.56, 9.51, 16.21, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 43.7, 0.59, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 43.73, 2.35, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.09, 43.82, 4.32, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.13, 43.97, 6.08, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 43.38, 2.44, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.15, 42.7, 4.74, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.19, 41.38, 7.28, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 39.53, 9.65, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.32, 36.98, 12.14, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.39, 34.46, 14.13, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.48, 31.68, 16.03, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.53, 29.06, 16.97, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.6, 26.12, 16.94, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.67, 23.52, 17.04, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.49, 0.0, 0.0, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.24, 25.2, 2.16, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.23, 25.11, 4.12, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.22, 24.99, 6.08, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.25, 25.33, 2.15, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.18, 24.86, 3.47, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.12, 23.94, 5.71, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.1, 22.59, 7.9, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.16, 20.61, 10.17, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.28, 18.4, 12.12, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.42, 16.15, 13.73, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.6, 13.79, 15.15, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.82, 11.64, 16.16, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.06, 9.82, 16.79, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.97, 0.0, 0.0, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.21, 38.35, 2.55, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.23, 38.52, 5.3, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.28, 37.96, 2.5, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.36, 37.03, 5.06, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.45, 35.72, 7.35, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.57, 34.02, 9.56, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.72, 32.02, 11.7, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.93, 29.75, 13.87, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:1.17, 27.42, 14.82, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 125.46, 1.18, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 125.43, 0.6, False, 4\n",
      "sending action\n",
      "sent action\n",
      "5: info:0.07, 124.92, 2.86, False, 4\n",
      "sending action\n",
      "sent action\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-776f8e932d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mlmao\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ffac84fee8d6>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# wait for results from that action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes_captured\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_game_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# blocking line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"5: info:{0}, {1}, {2}, {3}, {4}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes_captured\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ffac84fee8d6>\u001b[0m in \u001b[0;36mget_game_stats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;31m# process string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mvalue_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\", \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mangle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mspeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "env = EnvironmentWrapper()\n",
    "lmao = []\n",
    "for i in range(2):\n",
    "    env.reset()\n",
    "    while not env.is_done():\n",
    "        exp = env.step('as')\n",
    "        lmao.append(exp)\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_agent(verbose=True, \n",
    "#             num_episodes=100,\n",
    "#             discount = 0.95, \n",
    "#             batch_size = 32, \n",
    "#             N = 40, # how often to clone the target policy\n",
    "#             memory_size = 128,\n",
    "#             eps_decay_rate=0.9997,\n",
    "#             max_exp_rate=1.0, \n",
    "#             min_exp_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(lmao[1][0][0].shape)\n",
    "f = lmao[0][0][0].transpose((2, 0, 1))\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lmao[1][0][0].transpose((2, 0, 1))\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lmao[2][0][0].transpose((2, 0, 1))\n",
    "f.shape\n",
    "\n",
    "plt.imshow(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lmao[3][0][0].transpose((2, 0, 1))\n",
    "f.shape\n",
    "\n",
    "plt.imshow(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lmao[4][0][0].transpose((2, 0, 1))\n",
    "f.shape\n",
    "\n",
    "plt.imshow(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(f[3])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
