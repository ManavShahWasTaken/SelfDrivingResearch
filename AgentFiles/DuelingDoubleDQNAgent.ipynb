{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z6tk-uX9bwY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import socket\n",
    "# import shutil\n",
    "# from skimage import io\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state_data):\n",
    "        self.state_data = np.asarray(state_data)\n",
    "  \n",
    "    def process_state(self):\n",
    "        pass\n",
    "  \n",
    "    def get_batch_tensor(self):\n",
    "        holder = np.asarray(self.state_data)\n",
    "        holder.reshape((1, ) + holder.shape)\n",
    "        return holder\n",
    "  \n",
    "    def get_individual_tensor(self):\n",
    "        return np.asarray(self.state_data)\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.state_data.shape\n",
    "  \n",
    "    def display(self):\n",
    "        print(self.state_data)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class Frame(State):\n",
    "    def __init__(self, state_data, crop_factor=None, destination_size=None, vert_cent=0.5):\n",
    "        State.__init__(self, state_data)\n",
    "#         self.state_data = self.process_state(crop_factor, vert_cent, destination_shape)\n",
    "        self.state_data = self.process_state([0.7, 1.0], 0.7, (128,64))\n",
    "  \n",
    "    def process_state(self, crop_factor, vert_cent, destination_shape):\n",
    "        \"\"\"\n",
    "        Does all the processing of the frame using the helper functions\n",
    "        \"\"\"\n",
    "        frame = self.crop_frame(self.state_data, crop_factor, vert_cent)\n",
    "        frame = self.normalise_frame(frame)\n",
    "        frame = self.gray_scale(frame) # cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        assert len(frame.shape) == 2\n",
    "        frame = self.downsample_frame(frame, destination_shape)\n",
    "        return frame\n",
    "\n",
    "  \n",
    "    def gray_scale(self, frame, gray_scale_factor=[0.3, 0.3, 0.3]):\n",
    "        frame = np.dot(frame, np.asarray(gray_scale_factor))\n",
    "        return frame\n",
    "\n",
    "    def normalise_frame(self, frame):\n",
    "        frame = frame.astype('float32') / 255.0\n",
    "        return frame\n",
    "  \n",
    "    def downsample_frame(self, frame, destination_shape):\n",
    "        \"\"\"\n",
    "        downsamples the frame. decreases the resolution\n",
    "        \"\"\"\n",
    "        frame = cv2.resize(np.asarray(frame), dsize=destination_shape, interpolation=cv2.INTER_CUBIC)\n",
    "        return frame\n",
    "  \n",
    "    def crop_frame(self, frame, crop_factor, vert_cent):\n",
    "        \"\"\"\n",
    "        input is the frame\n",
    "        output is the cropped frame\n",
    "        crop_factor is the ratio at which you want to crop the height and width(0.8, 0.8)\n",
    "        cent is the ratio at which the centre of the cropped frame should be\n",
    "        \"\"\"\n",
    "        if crop_factor is None:\n",
    "          return frame\n",
    "    \n",
    "        height_factor = int((crop_factor[0]*frame.shape[0]) // 2)\n",
    "        width_factor = int((crop_factor[1]*frame.shape[1]) // 2)\n",
    "        vert_cent = int(frame.shape[0]*vert_cent)\n",
    "        width_cent = int(frame.shape[1]*0.5)\n",
    "\n",
    "        frame = frame[vert_cent - height_factor: vert_cent + height_factor, \n",
    "                      width_cent - width_factor: width_cent + width_factor]\n",
    "        return frame\n",
    "\n",
    "# ------------------------------------\n",
    "class DataBuffer:\n",
    "    \"\"\"\n",
    "    Keeps track of n latest states \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, size=1):\n",
    "        self.buffer = []\n",
    "        self.size = size\n",
    "\n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        arr = np.array(self.buffer)\n",
    "        if self.size == 1 or in_batch:\n",
    "            return arr\n",
    "        else:\n",
    "            return arr.reshape((1, ) + arr.shape)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return np.asarray(self.current_state[0]).shape\n",
    "\n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(state)\n",
    "        \n",
    "# ------------------------------------\n",
    "\n",
    "class FrameBuffer(DataBuffer):\n",
    "    def __init__(self, size = 4):\n",
    "        DataBuffer.__init__(self, size=size)\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.get_input_tensor().shape\n",
    "  \n",
    "    def get_input_tensor(self, in_batch=True):\n",
    "        temp = np.array(self.buffer)\n",
    "        return  temp.transpose((1, 2, 0))\n",
    "    \n",
    "    def assign_to_buffer(self, state):\n",
    "        if isinstance(state, State):\n",
    "            state = state.get_individual_tensor()\n",
    "        # if buffer not initialised\n",
    "        if len(self.buffer) == 0:\n",
    "            self.buffer = [state]\n",
    "            return\n",
    "\n",
    "        if len(self.buffer) >= self.size:\n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.buffer.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh-452ClVFpT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # initialise comms with the simulator here\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # initialise the socket\n",
    "        # connect to localhost, port 2345 \n",
    "        self.sock.bind((\"127.0.0.1\", 4444))\n",
    "        self.sock.listen(1)\n",
    "        print(\"Waiting to connect to Simulator...\")\n",
    "        self.clientsocket, _ = self.sock.accept() # connect to simulator [BLOCKING]\n",
    "        print(\"Connected to simulator!\")\n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # initialising frame buffer\n",
    "        self.buffer_size = 4 # could change this \n",
    "        \n",
    "         # this is the FrameBuffer that keeps track of the latest frames.\n",
    "         #initialising the frame buffer by just giving it multiple copies of the same starting frame\n",
    "        # =========================================\n",
    "        \n",
    "        self.current_state = None\n",
    "        \n",
    "        self.current_buffer = None\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        \n",
    "        self.time_steps = 0\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        self.max_time_steps_per_episode = 500 #change this based on the enviorment\n",
    "        \n",
    "        # initialise buffer\n",
    "        self.current_buffer = FrameBuffer(size = self.buffer_size)\n",
    "        \n",
    "        # =========================================\n",
    "        \n",
    "        \n",
    "        # Create target directory if it doesn't exist\n",
    "       \n",
    "        parent_path = os.path.abspath(os.path.join(\"\", os.pardir))\n",
    "        self.final_path = parent_path + \"/simulation/Screenshots\"\n",
    "        if not os.path.exists(self.final_path):\n",
    "            os.mkdir(self.final_path)\n",
    "            print(\"Directory \" , self.final_path,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , self.final_path,  \" already exists\")\n",
    "            \n",
    "        \n",
    "        # Save scr1 to Screenshots if it doesn't already exist\n",
    "        if not os.path.exists(self.final_path + '/scr1.png'):\n",
    "            scr1 = Image.open(self.parent_path + '/simulation/scr1.png', 'r')\n",
    "            scr1.save(self.final_path + \"/scr1.png\", \"PNG\")\n",
    "        \n",
    "        # reset actually initializes self.current_state, self.current_buffer etc.\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.action_space = ['as', 'ar', 'al', 'ds', 'dr', 'dl', 'bs', 'br', 'bl']\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        \"\"\"\n",
    "        returns the input shape for the input layer of the network\n",
    "        \"\"\"\n",
    "        return self.current_buffer.get_input_shape()\n",
    "    \n",
    "#     def get_state_shape(self):\n",
    "#         \"\"\"\n",
    "#         not to be confused with the input shape. This is the shape of individual state (the shape of an individual processed shape of the environment)\n",
    "#         \"\"\"\n",
    "#         return self.current_state\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        \"\"\"\n",
    "        returns a random action to be taken. [steering, acceleration, brake]\n",
    "        \"\"\"\n",
    "        return random.choice(self.action_space)\n",
    "    \n",
    "    def get_action_at_index(self, index):\n",
    "        return self.action_space[index]\n",
    "    \n",
    "    def get_num_action_space(self):\n",
    "        \"\"\"\n",
    "        returns the number of permuations of valide actions. For Eg. [steer left, accelerate and no brake] is ONE action\n",
    "        [steer right, accelerate and brake] is invalid as we cannot accelerate and brake at the same time.\n",
    "        there are 9 possible actions I think?\n",
    "        \"\"\"\n",
    "        return len(self.action_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the environment. self.done denotes whether the episode is done i.e. the car has crashed or we have stopped it\n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "        self.time_steps = 0\n",
    "        \n",
    "        tup = self.step('reset') # initial step. Says don't do anything but send the first 4 frames and game info\n",
    "        self.current_state, _, self.done = tup\n",
    "        return self.current_state[0] # send only the frames\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        does the action and returns the reward for that action along with the next state\n",
    "\n",
    "        This function may get complicated as it has to interact with the car simulator throught sockets.\n",
    "        \"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if not self.is_done():\n",
    "            # if the episode has not ended\n",
    "            #=======================\n",
    "            \n",
    "            # send the action\n",
    "            self.send_message(action)\n",
    "            \n",
    "            # wait for results from that action\n",
    "            angle, distance, speed, self.done, frames_captured = self.get_game_stats() # blocking line\n",
    "            # print(\"5: info:{0}, {1}, {2}, {3}, {4}\".format(angle, distance, speed, self.done, frames_captured))\n",
    "            \n",
    "            \n",
    "            # add images from path to current_buffer\n",
    "            for i in range(1, frames_captured + 1):\n",
    "                # each Frame object is then assigned to the FrameBuffer class in chronological order\n",
    "                path = self.final_path + '/scr{0}.png'.format(i)\n",
    "                self.current_buffer.assign_to_buffer(self.get_frame(path))\n",
    "            \n",
    "            buffer_tensor = self.current_buffer.get_input_tensor()\n",
    "            # ========================================\n",
    "            \n",
    "            # calculate reward\n",
    "            dist_delta = self.prev_dist - distance\n",
    "            self.prev_dist = distance\n",
    "            if abs(dist_delta) > 10:\n",
    "                dist_delta = 5 # if there's too big a negative jump in the distance, the car has passed a checkpoint.\n",
    "                # so, don't penalise it for that.\n",
    " \n",
    "            reward = (dist_delta * 0.9) - (abs(angle) * 0.1)\n",
    "            #=================\n",
    "            \n",
    "            # A buffer is a collection of consecutive frames that we feed to the NN. These frames are already processed.\n",
    "            \n",
    "            # the current state consists of all the information from the environment\n",
    "            self.current_state = (buffer_tensor, angle, distance, speed)\n",
    "            \n",
    "            # this returns the state of the environment after the action has been completed, the reward for the action and if the episode ended.\n",
    "            return self.current_state , reward, self.done\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def send_message(self, string):\n",
    "        self.clientsocket.sendall(string.encode())\n",
    "    \n",
    "    def receive_message(self):\n",
    "        data  = self.clientsocket.recv(256).decode()\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        returns if the episode is finished\n",
    "        \"\"\"\n",
    "        return self.done\n",
    "        \n",
    "    \n",
    "    def get_frame(self, path: str) -> Frame:\n",
    "        \"\"\"\n",
    "        communicates with the sim to get the latest state/frame. \n",
    "        returns a Frame object\n",
    "        Get image from path then convert to np array then make a frame object\n",
    "        \"\"\"\n",
    "        image = Image.open(path, 'r')\n",
    "        image.load()\n",
    "        np_data = np.asarray(image, dtype=\"float32\" )\n",
    "        return Frame(np_data)\n",
    "    \n",
    "    \n",
    "    # def delete_screenshots(self, folder_path: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     This method deletes the four screenshots saved in folder_path, along with the entire folder.\n",
    "    #     Method must be called after all four screenshots are converted to Frame objects.\n",
    "    #     \"\"\"\n",
    "    #     shutil.rmtree(folder_path)\n",
    "    \n",
    "    \n",
    "    def get_current_state():\n",
    "        \"\"\"\n",
    "        get the last n frames from the simulator (they might be stored in a folder by the simulator)\n",
    "        and store them in a buffer and return them\n",
    "        \"\"\"\n",
    "        return self.current_buff\n",
    "\n",
    "    \n",
    "    def get_game_stats(self):\n",
    "        \"\"\"\n",
    "        returns a tuple of angle, distance from checkpoint and speed from the sim. Again requires comms with simulator.\n",
    "        \"\"\"\n",
    "        # wait for info to arrive\n",
    "        string = self.receive_message()\n",
    "        \n",
    "        # process string\n",
    "        value_list = string.split(\", \")\n",
    "        angle = float(value_list[0])\n",
    "        distance = float(value_list[1])\n",
    "        speed = float(value_list[2])\n",
    "        crashed_state = False\n",
    "        if value_list[3] == '1':\n",
    "            crashed_state = True\n",
    "        frames_captured = int(value_list[4])\n",
    "        # return tuple of values\n",
    "        return angle, distance, speed, crashed_state, frames_captured\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        in case we need to 'close' the environment\n",
    "        \"\"\"\n",
    "        self.sock.close()\n",
    "        self.clientsocket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u36eHVMV285q"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment, network, run_only=False, eps_decay_rate=0.9975,max_exp_rate=1.0, min_exp_rate=0.05):\n",
    "        self.env = environment # this should be the environment wrapper class\n",
    "        \n",
    "        if not run_only:\n",
    "            self.exp_rate = max_exp_rate     # our network starts off with this exploration rate\n",
    "        else:\n",
    "            self.exp_rate = 0.0\n",
    "            self.min_exp_rate = 0.0\n",
    "        \n",
    "        self.min_exp_rate = min_exp_rate  # have at least 0.01 exploration rate at all times\n",
    "        \n",
    "        self.decay_rate = eps_decay_rate   # decay the exploration rate at this rate\n",
    "        \n",
    "        self.time_step = 0     # keeps track of time steps\n",
    "        \n",
    "        self.network = network\n",
    "    \n",
    "    def take_action(self, current_state):\n",
    "        # Implement the epsilon greedy strategy \n",
    "        result = random.random()                      # get a random number from 0 to 1 with linear distribution\n",
    "        if result > self.get_exp_rate():              # if it falls over the explore rate, exploit\n",
    "            # Get the action with the maximum q-value\n",
    "            action = self.env.get_action_at_index(\n",
    "                self.network.get_max_q_value_index(current_state, which_net = 'online', batch=False))  # exploit\n",
    "        else:                                         # if it falls under the explore rate, explore\n",
    "            action = self.env.get_random_action()          # explore (generate a random action from the environment class)\n",
    "            \n",
    "        self.increment_time_step()                    # increment time step as well as update the decay rate\n",
    "        next_state, reward, done = self.env.step(action)# finally, take the action and record the reward\n",
    "        \n",
    "        return current_state, self.env.action_space.index(action), reward, next_state[0], done  # return an experience Tuple\n",
    "        \n",
    "    \n",
    "    def reset_time_steps(self, i=0):\n",
    "        self.timesteps = i\n",
    "    \n",
    "    def increment_time_step(self):\n",
    "        self.time_step += 1\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.exp_rate > self.min_exp_rate:\n",
    "            self.exp_rate = self.exp_rate * self.decay_rate\n",
    "        else:\n",
    "            self.exp_rate = self.min_exp_rate\n",
    "    \n",
    "    def get_exp_rate(self):\n",
    "        return self.exp_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MVpqLFJ83IXp",
    "outputId": "27566a6e-9187-4053-ce13-02fa2abb6cbe"
   },
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "class NetworkTracker:\n",
    "    \n",
    "    def __init__(self, environment, source=True, verbose = True, network_name = None): # pass in the environment which has input shape of the frame\n",
    "        self.network_name = network_name\n",
    "        if source:\n",
    "            self.model = models.load_model(self.network_name)\n",
    "        else:\n",
    "            self.model = self.define_model(environment)\n",
    "        \n",
    "        self.target_model = None\n",
    "        self.clone_policy()\n",
    "        \n",
    "        self.verbose = 0\n",
    "        if verbose:\n",
    "            self.verbose = 1\n",
    "              \n",
    "    def define_model(self, env):\n",
    "        \n",
    "        inp_layer = layers.Input(env.get_input_shape())\n",
    "        \n",
    "        conv_1 = layers.Conv2D(filters=10, \n",
    "                                kernel_size=(4,4), \n",
    "                                activation='relu')(inp_layer) # first layer takes input shape from the environment\n",
    "        \n",
    "        maxpool_1 = layers.MaxPool2D((3, 3))(conv_1)\n",
    "        \n",
    "        conv_2 = layers.Conv2D(filters=20, kernel_size = (3, 3), strides=2, activation='relu')(maxpool_1)\n",
    "\n",
    "        maxpool_2 = layers.MaxPool2D(3, 3)(conv_2)\n",
    "        \n",
    "        flatten = layers.Flatten()(maxpool_2)\n",
    "        \n",
    "        adv_dense_1 = layers.Dense(64, activation = 'relu')(flatten)\n",
    "        \n",
    "        adv_out = layers.Dense(env.get_num_action_space(), activation='linear', name = 'advantage') (adv_dense_1)\n",
    "        \n",
    "        val_dense_1 = layers.Dense(32, activation = 'relu')(flatten)\n",
    "        \n",
    "        val_out = layers.Dense(1, activation='linear', name = 'value') (val_dense_1)\n",
    "        \n",
    "        def q_out(x):\n",
    "            value = x[0]\n",
    "            advantage = x[1]\n",
    "            return value + advantage\n",
    "        \n",
    "        q_output = layers.Lambda(q_out)([val_out, adv_out])\n",
    "        \n",
    "        model = models.Model(inp_layer, q_output, name='dueling_dqn')\n",
    "        \n",
    "        model.compile(optimizer=Adam(lr=0.0012), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def get_q_values(self, states, which_net = 'online', batch = False):\n",
    "        \"\"\"\n",
    "        get q values for specified states.\n",
    "        INPUTS:\n",
    "        1. states: the input states\n",
    "        2. which_net: which net you want the prediction from\n",
    "        3. batch: true if there are multiple states, false if there is only one state\n",
    "        \"\"\"\n",
    "        if isinstance(states[0], DataBuffer): # if you have a list of buffers, convert them to numpy tensors\n",
    "            states = np.asarray([i.get_input_tensor(in_batch=True) for i in states])\n",
    "        \n",
    "        if (not batch) and (not states.shape[0] == 1):\n",
    "            states = np.expand_dims(states, axis=0)\n",
    "        \n",
    "        output_tensor = None\n",
    "        \n",
    "        if which_net == 'online':\n",
    "            output_tensor = self.model.predict(states) \n",
    "        elif which_net == 'target':\n",
    "            output_tensor = self.target_model.predict(states) \n",
    "        \n",
    "        \n",
    "        if not batch:\n",
    "            return output_tensor[0]  # you want to convert the 2 dimensional output to 1 dimension to call argmax\n",
    "        else:\n",
    "            return output_tensor\n",
    "    \n",
    "    def get_max_q_value_index(self, states, which_net = 'online', batch=False):\n",
    "        return np.argmax(self.get_q_values(states, which_net=which_net, batch = batch), axis=int(batch))\n",
    "    \n",
    "    def fit(self, states_batch, targets_batch):\n",
    "        \"\"\"\n",
    "        Fit the states with the target batch\n",
    "        \"\"\"\n",
    "        self.model.fit(states_batch, targets_batch, verbose=1)\n",
    "        \n",
    "    def clone_policy(self):\n",
    "        \"\"\"\n",
    "        Clone the target policy\n",
    "        \"\"\"\n",
    "        self.model.save(self.network_name)\n",
    "        self.target_model = models.load_model(self.network_name)\n",
    "                  \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        Return a summary of the defined model\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def save_policy(self):\n",
    "        self.model.save(self.network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "class Memory:\n",
    "    def __init__(self, size, save_path, mem_source = None):\n",
    "        self.save_path = save_path\n",
    "        self.replay = []\n",
    "        self.limit = size\n",
    "        self.exp_count = 0\n",
    "        if mem_source is not None:\n",
    "            # pick up memory from the specified progess folder\n",
    "            with open(mem_source + '/memory.pkl', 'rb') as file:\n",
    "                self.replay = pickle.load(file)[:self.limit] # limit the list size based on the memory limit you specify\n",
    "            \n",
    "            self.exp_count = len(self.replay)\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.exp_count += 1\n",
    "        \n",
    "        if self.exp_count < self.limit:\n",
    "            self.replay.append(experience)  #append to experiences\n",
    "        else:\n",
    "            self.replay[self.exp_count%len(self.replay)] = experience  #wrap around if the memory capacity is reached\n",
    "        assert len(self.replay) <= self.limit\n",
    "        \n",
    "    def is_usable(self, batch_size):\n",
    "        return len(self.replay) >= batch_size\n",
    "    \n",
    "    def reset_replay_memory(self):\n",
    "        self.exp_count = 0\n",
    "        self.replay = []\n",
    "        \n",
    "    def save_to_disk(self):\n",
    "        # dump memory in the \n",
    "        with open(self.save_path + '/memory.pkl', 'wb') as file:\n",
    "            pickle.dump(self.replay, file)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.replay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(sample):\n",
    "    \"\"\"\n",
    "    takes a sample of experiences and converts them into individual tensors.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    done_tensor = []\n",
    "    for experience in sample:\n",
    "        states.append(experience[0])\n",
    "        actions.append(experience[1])\n",
    "        rewards.append(experience[2])\n",
    "        next_states.append(experience[3])\n",
    "        done_tensor.append(experience[4])\n",
    "    \n",
    "    return np.asarray(states), np.asarray(actions), np.asarray(rewards), np.asarray(next_states), np.asarray(done_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_batch(states, actions, rewards, next_states, dones, net, gamma):\n",
    "    assert actions.ndim == 1\n",
    "    assert rewards.ndim == 1\n",
    "    assert dones.ndim == 1\n",
    "    assert len(actions) == len(rewards) == len(dones) == len(states) == len(next_states)\n",
    "    \n",
    "    target_q_values = net.get_q_values(states, which_net = 'online', batch = True) # get the q values from the current states\n",
    "    \n",
    "    which_acts = net.get_max_q_value_index(next_states, which_net= 'online', batch = True) # this gives you a list of action indices\n",
    "    \n",
    "    q_estimates = net.get_q_values(next_states, which_net = 'target', batch = True) # get the q estimates from the target net for the next states\n",
    "    \n",
    "    for index in range(len(target_q_values)):\n",
    "        # index indexes the batch axis\n",
    "        q_estimate = q_estimates[index]\n",
    "        decoupled_action = which_acts[index]\n",
    "        reward = rewards[index]\n",
    "        action_taken = actions[index]\n",
    "        ended = dones[index]\n",
    "        \n",
    "        # bellman equation related target value calculation for DDQNs\n",
    "        if not ended:\n",
    "            prev_target = q_estimate[decoupled_action]\n",
    "        else:\n",
    "            prev_target = 0\n",
    "        \n",
    "        target = reward + gamma * prev_target\n",
    "        \n",
    "        target_q_values[index][action_taken] = target # assign the target to the corresponding (state, action) pair\n",
    "\n",
    "    return target_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tempfile import TemporaryFile\n",
    "quit = False\n",
    "import pickle \n",
    "def train_agent(contd=True, network_name = None, save_path = 'training_progress', contd_path = None, verbose=True, num_episodes=1500,\n",
    "                discount = 0.95, batch_size = 64, N = 40, memory_size = 1024, \n",
    "                eps_decay_rate=0.9975, max_exp_rate=1.0, min_exp_rate=0.05, max_reward = 1000 ):\n",
    "    # get all the hyperparameters in one place!\n",
    "            \n",
    "     \n",
    "    # initialise your environment\n",
    "    env = EnvironmentWrapper()\n",
    "    \n",
    "    # initialise your policy and target networks inside net\n",
    "    net = NetworkTracker(env, source=contd, verbose=verbose, network_name=network_name)\n",
    "    print(net.get_model_summary())\n",
    "    \n",
    "    # initialise your agent that will follow the epsilon greedy strategy\n",
    "    agent = Agent(env, net, eps_decay_rate=eps_decay_rate, max_exp_rate=max_exp_rate,min_exp_rate=min_exp_rate )    \n",
    "    \n",
    "    # initialise experience replay memory\n",
    "    memory = Memory(memory_size, save_path = save_path, mem_source = contd_path)\n",
    "    \n",
    "    # stores all the total reward per episode\n",
    "    \n",
    "    training_stats = []\n",
    "    epochs = []\n",
    "    starting_episode = 0\n",
    "    \n",
    "    sum_over_ten = 0\n",
    "    moving_avg = []\n",
    "    avg_epochs = []\n",
    "    \n",
    "    validations = []\n",
    "    validation_epochs = []\n",
    "    if contd:\n",
    "        with open(save_path + '/training_stats.pkl', 'rb') as file:\n",
    "            epochs, training_stats = pickle.load(file)\n",
    "#             counter_temp = 0\n",
    "#             temp_sum = 0\n",
    "#             t = 0\n",
    "#             for stat in training_stats:\n",
    "#                 t += 1\n",
    "#                 counter_temp += 1\n",
    "#                 temp_sum += stat\n",
    "#                 if counter_temp%10 == 0:\n",
    "#                     counter_temp = 0\n",
    "#                     avg = temp_sum / 10.0\n",
    "#                     moving_avg.append(avg)\n",
    "#                     avg_epochs.append(t)\n",
    "#                     temp_sum = 0\n",
    "        \n",
    "        with open(save_path + '/moving_average.pkl', 'rb') as file:\n",
    "            avg_epochs, moving_avg = pickle.load(file)          \n",
    "        \n",
    "        with open(save_path + '/episode_count.pkl', 'rb') as file:\n",
    "            starting_episode = pickle.load(file)\n",
    "            \n",
    "    # graph display init code\n",
    "    %matplotlib notebook\n",
    "    plt.rcParams['animation.html'] = 'jshtml'\n",
    "    fig = plt.figure()\n",
    "    subplot = fig.add_subplot(111)\n",
    "    \n",
    "    for episode_count in range(starting_episode, num_episodes):\n",
    "        # uncomment if you want to start the environmet with a random move\n",
    "        # state = env.step(env.get_random_action())[0]\n",
    "        valid_episode = False\n",
    "        \n",
    "        \n",
    "        # check if the environment is available to run\n",
    "        while not valid_episode:\n",
    "            # keeps track of how many steps has been since the reward hasn't moved\n",
    "            stuck_counter = 0\n",
    "\n",
    "            # keeps track of the total reward that we got for this episode\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            # keeps track of steps in the episode.\n",
    "            counter = 0\n",
    "\n",
    "            # reset environement and record the initial state before every episode\n",
    "            state = env.reset()\n",
    "\n",
    "            # store the experiences in a temporary tuple so that we only add them to memory if it was a valid episode.\n",
    "            temp_exp = []\n",
    "            stuck = False\n",
    "            # ==============================\n",
    "            while not env.is_done() and not stuck and cumulative_reward < max_reward: # run the environment for one episode\n",
    "                counter += 1\n",
    "                current_state, action, reward, next_state, done = agent.take_action(state) # let the agent take an action for one time step\n",
    "                \n",
    "                cumulative_reward += reward # add the reward to total rewards.\n",
    "                \n",
    "                # check if the car is stuck when the reward isn't changing by much\n",
    "                if abs(reward) < 0.5:\n",
    "                    stuck_counter += 1\n",
    "                    if stuck_counter > 7:\n",
    "                        done = True\n",
    "                        stuck = True\n",
    "                else:\n",
    "                    stuck_counter = 0\n",
    "                experience = current_state, action, reward, next_state, done # experience tuple \n",
    "                state = next_state # update the current state\n",
    "                 # push the experience in memory\n",
    "                temp_exp.append(experience)\n",
    "            # ==============================\n",
    "            \n",
    "            if counter > 5:\n",
    "                valid_episode = True\n",
    "                for i in range(len(temp_exp)):\n",
    "                    # temp_exp[i][-1] = cumulative_reward\n",
    "                    memory.push(temp_exp[i])\n",
    "                sum_over_ten += cumulative_reward\n",
    "\n",
    "        agent.update_epsilon() # update the exploration rate of the agent after each episode\n",
    "\n",
    "        if memory.is_usable(batch_size):\n",
    "                experience_batch = memory.sample(batch_size) # sample randomly from memory\n",
    "                states, actions, rewards, next_states, done_tensor = extract_tensors(experience_batch) # unzips the tensors\n",
    "\n",
    "                target_batch = get_target_batch(states, actions, rewards, next_states, done_tensor, net, discount) # get a batch of target values to fit against\n",
    "\n",
    "                net.fit(states, target_batch) # fit the network\n",
    "\n",
    "        # append the training stats\n",
    "        training_stats.append(cumulative_reward)\n",
    "        epochs.append(episode_count)\n",
    "\n",
    "        # clone the target policy every N episodes.\n",
    "        if (episode_count + 1) % N == 0:\n",
    "            net.clone_policy()\n",
    "        \n",
    "        if  (episode_count) % (N*2) == 0:\n",
    "            performance = evaluate_agent(runs = 5, model_name = network_name, env = env)\n",
    "            validations.append(performance)\n",
    "            validation_epochs.append(epochs[-1])\n",
    "            with open(save_path + '/validation.pkl', 'wb') as file:\n",
    "                pickle.dump((validation_epochs, validations), file)\n",
    "            \n",
    "\n",
    "        # update the training chart every 10 episodes\n",
    "        if (episode_count + 1) % 10 == 0:\n",
    "            avg = sum_over_ten / 10.0\n",
    "            sum_over_ten = 0\n",
    "            moving_avg.append(avg)\n",
    "            avg_epochs.append(epochs[-1])\n",
    "            subplot.plot(epochs, training_stats, color='b')\n",
    "            subplot.plot(avg_epochs, moving_avg, color = 'r')\n",
    "            subplot.plot(validation_epochs, validations, color = 'g')\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            # periodically save training progress.\n",
    "            memory.save_to_disk()\n",
    "            with open(save_path + '/training_stats.pkl', 'wb') as file:\n",
    "                pickle.dump((epochs, training_stats), file)\n",
    "            \n",
    "            with open(save_path + '/moving_average.pkl', 'wb') as file:\n",
    "                pickle.dump((avg_epochs, moving_avg), file)\n",
    "            \n",
    "            with open(save_path + '/episode_count.pkl', 'wb') as file:\n",
    "                pickle.dump(episode_count, file)\n",
    "            \n",
    "            with open(save_path + '/epsilon.pkl', 'wb') as file:\n",
    "                pickle.dump(agent.exp_rate, file)\n",
    "            \n",
    "\n",
    "        # if specified, print stats.\n",
    "        if verbose:\n",
    "            print(\"Episode Count: \", episode_count, \"\\t Cumulative Reward: \", round(cumulative_reward, 2), \"\\t eps: \", round(agent.exp_rate, 3) )\n",
    "\n",
    "    \n",
    "    memory.save_to_disk()\n",
    "    net.save_policy()\n",
    "    env.close()\n",
    "    \n",
    "    return epochs, training_stats, net\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(runs, model_name, env = None):\n",
    "    if env is None:\n",
    "        env = EnvironmentWrapper()\n",
    "    net = NetworkTracker(env, source=True, verbose=False, network_name=model_name)\n",
    "    agent = Agent(env, net, run_only = True)\n",
    "    \n",
    "    perf_sum = 0\n",
    "    for run in range(runs):\n",
    "        valid_episode = False\n",
    "        cumulative_reward = 0\n",
    "        time_steps = 0\n",
    "        while not valid_episode:\n",
    "            state = env.reset()\n",
    "            cumulative_reward = 0\n",
    "            time_steps = 0\n",
    "            stuck_counter = 0\n",
    "            stuck = False\n",
    "            while not env.is_done() and not stuck:\n",
    "                _ , action, reward, next_state, done = agent.take_action(state)\n",
    "                state = next_state\n",
    "                time_steps += 1\n",
    "                cumulative_reward += reward \n",
    "                if abs(reward) < 0.5:\n",
    "                    stuck_counter += 1\n",
    "                    if stuck_counter > 7:\n",
    "                        done = True\n",
    "                        stuck = True\n",
    "                else:\n",
    "                    stuck_counter = 0\n",
    "            \n",
    "            if time_steps > 3:\n",
    "                valid_episode = True\n",
    "                cumulative_reward = round(cumulative_reward, 2)\n",
    "        perf_sum += cumulative_reward\n",
    "        print('run {0}: cumulative_reward: {1}, ran for: {2} timesteps'.format(run, round(cumulative_reward, 2), time_steps))\n",
    "    \n",
    "    avg_perf = float(perf_sum/runs)\n",
    "    \n",
    "    print('average performance: ', avg_perf)\n",
    "#     env.close()\n",
    "    return avg_perf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to connect to Simulator...\n"
     ]
    }
   ],
   "source": [
    "history = train_agent(contd = True,\n",
    "            verbose = True, \n",
    "            save_path = 'training_progress',\n",
    "            contd_path = 'training_progress', \n",
    "            network_name = 'DuelingDdqn.h5',\n",
    "            num_episodes = 1000,\n",
    "            discount = 0.99, \n",
    "            batch_size = 512, \n",
    "            N = 100, # how often to clone the target policy\n",
    "            memory_size = 4096,\n",
    "            eps_decay_rate = 0.999,\n",
    "            max_exp_rate = 0.2, \n",
    "            min_exp_rate = 0.1,\n",
    "            max_reward = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_agent(runs = 10, model_name = 'DuelingDdqn.h5', env = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
